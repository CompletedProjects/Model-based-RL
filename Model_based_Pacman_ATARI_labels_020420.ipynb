{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Model_based_Pacman_ATARI_labels_020420.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PpGt1rdvsLFT"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN/xT21OVPOOfdWGUHQ0hnx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/Model_based_Pacman_ATARI_labels_020420.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "90be23f4-d3d2-41c0-c356-70a1d7b08233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/mila-iqia/atari-representation-learning.git\n",
            "  Cloning git://github.com/mila-iqia/atari-representation-learning.git to /tmp/pip-req-build-lgscm3ua\n",
            "  Running command git clone -q git://github.com/mila-iqia/atari-representation-learning.git /tmp/pip-req-build-lgscm3ua\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->atariari==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: atariari\n",
            "  Building wheel for atariari (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atariari: filename=atariari-0.0.1-cp36-none-any.whl size=46584 sha256=9a3da5866687fe6509571906cbe7a2019fd5a416ae7a0503bddfc788b8bf8c86\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5am0rzfv/wheels/3d/69/51/5e436e5ae566c5b4dec5c53e65396d516459877a42a11d7aa4\n",
            "Successfully built atariari\n",
            "Installing collected packages: atariari\n",
            "Successfully installed atariari-0.0.1\n",
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-skgadwlx\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-skgadwlx\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->a2c-ppo-acktr==0.0.1) (46.0.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=009ec331b8fe4dfb313ef41a87901be851cbd2974aa0b963c747a319ae92bb52\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iu5fmnwx/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-y__btutj\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-y__btutj\n",
            "Collecting gym<0.16.0,>=0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/01/8771e8f914a627022296dab694092a11a7d417b6c8364f0a44a8debca734/gym-0.15.7.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.38.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines, gym\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=0edde6fd33780245acf23c8004fe016aff1843731f69f432014b4d6514823a74\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fnoctv2i/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-cp36-none-any.whl size=1648840 sha256=6e9af4ce096e4d1cd115c88bb125439b2e5d399ed395dc62aeb0488b4d89516d\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/60/6a/f9c27ae133abaf5a5687ed2fa8ed19627d7fac5d843a27572b\n",
            "Successfully built baselines gym\n",
            "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gym, baselines\n",
            "  Found existing installation: gym 0.17.1\n",
            "    Uninstalling gym-0.17.1:\n",
            "      Successfully uninstalled gym-0.17.1\n",
            "Successfully installed baselines-0.1.6 gym-0.15.7\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/18/ef5215832f523c29f6e0c19a5b87e0dd90fe40fb48ba38362f961be14e4f/wandb-0.8.31-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 18.7MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 24.0MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.0MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.1MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.1)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hCollecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: gql, watchdog, subprocess32, graphql-core, pathtools\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=7eecb2b4d8640bbd94a4ca20c1ade60549454ff8f6eaa6c073ebd35a1f1b2eb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=e49162536fa9ee672598d2e0de2551eb97f686d65d5e7a6deb6df50a2720cdd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=8ceb5cf4777c26878daa0bf7dd33c21c838aff4b6450fbd58c3a7156a665d271\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=b069308a0f2177a159d48ff5b113d498c7860f5012a3e9a2e1803922eb4f914f\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=438b3596d8d9a45c5c19c414c241aa8ee4313777e522af8912d3d73d2a010497\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built gql watchdog subprocess32 graphql-core pathtools\n",
            "Installing collected packages: graphql-core, gql, smmap, gitdb, GitPython, sentry-sdk, pathtools, watchdog, docker-pycreds, configparser, subprocess32, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.0 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.2 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.1 subprocess32-3.5.4 wandb-0.8.31 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZ9gMrxp5vU",
        "colab_type": "code",
        "outputId": "4fc347b5-674a-4e00-bea7-632bc06b210b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI287_Yjxn3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, BatchSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from atariari.benchmark.envs import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from atariari.benchmark.wrapper import AtariARIWrapper\n",
        "\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Imported required for the Model-based RL\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "2348e1c4-d9d3-4fe0-ae80-825901713e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login #############"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gdFb0bqHHN",
        "colab_type": "code",
        "outputId": "2f6d0ba2-864a-4127-8775-bc6041a464ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "wandb.init(entity=\"nerdk312\", project=\"ATARI_LABELS_MPC\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/5t7drc4e\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/5t7drc4e</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/5t7drc4e"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePMZh0WfleFC",
        "colab_type": "text"
      },
      "source": [
        "# Model-based RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NobLqSMozxSa",
        "colab_type": "code",
        "outputId": "bf10084c-a93f-4c48-8717-ffb09605eb30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env = AtariARIWrapper(gym.make('MsPacmanNoFrameskip-v4'))\n",
        "obs = env.reset()\n",
        "original_info = env.labels() # Nawid - This only contains the labels\n",
        "print(original_info)\n",
        "i = 0\n",
        "t = 0\n",
        "while True:\n",
        "    _,_,done,info = env.step(env.action_space.sample())\n",
        "    info_labels = info['labels']\n",
        "    t += 1\n",
        "    if not original_info['num_lives'] == info_labels['num_lives']:\n",
        "        print('change in num lives')\n",
        "        original_info = info_labels\n",
        "        i = 0\n",
        "        print('t value',t)\n",
        "\n",
        "    #print('info',info['labels']) # Nawid - Need to specify the labels\n",
        "    if original_info == info_labels:\n",
        "        i += 1\n",
        "        print(i)\n",
        "\n",
        "    if done:\n",
        "        print('overall time',t)\n",
        "        break\n",
        "\n",
        "'''\n",
        "next_ale_lives = np.expand_dims(np.array([i['ale.lives'] for i in infos]),1) # Nawid - Extracts the information related to the number of lives\n",
        "dones =  ale_lives - next_ale_lives # Nawid -  Finds the change in lives after a timestep\n",
        "dones[dones< 1] = 0 # Nawid - Makes it so that the situation where it resets does not have a change in 3 for the values\n",
        "ale_lives = next_ale_lives # Nawid- Update the ale lives\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 0, 'player_score': 0, 'num_lives': 2}\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "change in num lives\n",
            "t value 1354\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "change in num lives\n",
            "t value 2090\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "overall time 2401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nnext_ale_lives = np.expand_dims(np.array([i['ale.lives'] for i in infos]),1) # Nawid - Extracts the information related to the number of lives\\ndones =  ale_lives - next_ale_lives # Nawid -  Finds the change in lives after a timestep\\ndones[dones< 1] = 0 # Nawid - Makes it so that the situation where it resets does not have a change in 3 for the values\\nale_lives = next_ale_lives # Nawid- Update the ale lives\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nukbbTgoFK47",
        "colab_type": "code",
        "outputId": "c275b980-af86-4950-9cce-e64f94fd2af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env = AtariARIWrapper(gym.make('MsPacmanNoFrameskip-v4'))\n",
        "\n",
        "#print(original_info)\n",
        "i = 0\n",
        "for k in range(2):\n",
        "    obs = env.reset()\n",
        "    original_info = env.labels() # Nawid - This only contains the labels\n",
        "    while True:\n",
        "        _,_,done,info = env.step(env.action_space.sample())\n",
        "        info_labels = info['labels']\n",
        "    \n",
        "        if not original_info['num_lives'] == info_labels['num_lives']:\n",
        "            \n",
        "            print('change in num lives')\n",
        "            print('info_labels',info_labels)\n",
        "            original_info = info_labels\n",
        "            i = 0\n",
        "\n",
        "        #print('info',info['labels']) # Nawid - Need to specify the labels\n",
        "        if original_info == info_labels:\n",
        "            i += 1\n",
        "            print(i)\n",
        "\n",
        "        if done:\n",
        "            print('episode is done')\n",
        "            i = 0\n",
        "            break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "change in num lives\n",
            "info_labels {'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 6, 'player_score': 96, 'num_lives': 1}\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "change in num lives\n",
            "info_labels {'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 8, 'player_score': 128, 'num_lives': 0}\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "episode is done\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "change in num lives\n",
            "info_labels {'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 8, 'player_score': 128, 'num_lives': 1}\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "change in num lives\n",
            "info_labels {'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 13, 'player_score': 48, 'num_lives': 0}\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "episode is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnrvnbMcP6Xz",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9wLqaPmqqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'MsPacmanNoFrameskip-v4'\n",
        "\n",
        "#feature_size = all_defaults['feature_size'] # Nawid- Dimensionality of the representation\n",
        "# workers = 8 # Nawid - Choosing the number of workers for the network\n",
        "\n",
        "# Main loop hyperp\n",
        "AGGR_ITER = 10\n",
        "STEPS_PER_AGGR = 500\n",
        "\n",
        "# Random MB hyperp\n",
        "NUM_RAND_TRAJECTORIES = 2000\n",
        "\n",
        "# 'cuda' or 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Supervised Model Hyperp\n",
        "ENV_LEARNING_RATE = 1e-4\n",
        "REW_LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1024\n",
        "TRAIN_ITER_MODEL =  55\n",
        "\n",
        "# Controller Hyperp\n",
        "HORIZON_LENGTH = 5\n",
        "NUM_ACTIONS_SEQUENCES = 200\n",
        "\n",
        "observation_channels = 1\n",
        "action_dim = 1\n",
        "n_actions = 5 #  9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "reward_dim = 1\n",
        "\n",
        "# Time and date information\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n",
        "\n",
        "# Making the network deterministic - https://pytorch.org/docs/stable/notes/randomness.html\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKVfcEFEz9-t",
        "colab_type": "text"
      },
      "source": [
        "# Model setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaBzmIUDnqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping_loss(object):\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, verbose=False, wandb=None, name=\"\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = 1e11 # Nawid - Set a very high initial best loss\n",
        "        self.name = name\n",
        "        self.wandb = wandb\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score >= self.best_score: # Nawid - Inverse signs to take into minimising loss instead of maximising accuracy\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping for {self.name} counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(f'{self.name} has stopped')\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f'Validation loss decreased/improved for {self.name}  ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "\n",
        "        save_dir = self.wandb.run.dir\n",
        "        torch.save(model.state_dict(), save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.wandb.save(save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJvx1bTGSmLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(i):\n",
        "    a = np.zeros(n_actions, 'uint8')\n",
        "    a[i] = 1\n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGPEetVzTtOb",
        "colab": {}
      },
      "source": [
        "class NNDynamicsModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the next state, given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, obs_output_dim):\n",
        "        super(NNDynamicsModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, obs_output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "class NNRewardModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the reward given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, reward_output_dim):\n",
        "        super(NNRewardModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, reward_output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "def model_MSEloss(y_pred,y_truth, device):\n",
        "    '''\n",
        "    Compute the MSE (Mean Squared Error)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))\n",
        "\n",
        "def model_CEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the CEloss\n",
        "    '''\n",
        "    y_truth = torch.Tensor(np.array(y_truth)).to(device)\n",
        "    return F.cross_entropy(y_pred, y_truth)\n",
        "\n",
        "def model_BCEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the BCE (Binary cross entropy)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.binary_cross_entropy(y_pred.view(-1).float(), y_truth.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJgI07xT7eo",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzpOMJFWs9MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data_collection():\n",
        "    def __init__(self,ENV_NAME,state_mode = 'ATARIARI', encoder = None):\n",
        "        self.ENV_NAME = ENV_NAME\n",
        "        self.state_mode = state_mode\n",
        "        self.encoder = encoder\n",
        "        self.repeated_initial = True # Nawid - set repeated initial as true initially\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def gather_random_trajectories(self,num_traj):\n",
        "\n",
        "        dataset_random = []\n",
        "        #Env name could either be the RAM case or the generic case\n",
        "        env = AtariARIWrapper(gym.make(self.ENV_NAME)) \n",
        "        #i = 0\n",
        "        for n in range(num_traj):\n",
        "            # Initial set up\n",
        "            obs = env.reset()\n",
        "            self.repeated_initial = True # Nawid- Used to represent the initial state\n",
        "            initial_info_labels = env.labels()\n",
        "            info_labels = env.labels() # Nawid -  Used to get the current state\n",
        "            #print('trajectory number',n)\n",
        "            \n",
        "            while True:\n",
        "                # Choosing action and env step\n",
        "                sampled_action = np.random.randint(0,n_actions)\n",
        "                sampled_action_one_hot = one_hot(sampled_action)\n",
        "                next_obs, reward, done, next_info = env.step(sampled_action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                # self.repeated initial is set to true at first and it is is turned to true in the reward_collection class when done is true\n",
        "\n",
        "                if self.repeated_initial: # If the initial state is repeating\n",
        "                    if initial_info_labels == next_info_labels: # Current state is still the same as the initial state\n",
        "                        pass\n",
        "                    else:\n",
        "                        # New state achieved, so save data\n",
        "                        self.repeated_initial = False\n",
        "                    \n",
        "                        state, next_state = self.state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                        reward, pacman_done = self.reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                        dataset_random.append([state, next_state,reward,pacman_done,sampled_action_one_hot])\n",
        "                    \n",
        "                        obs = next_obs\n",
        "                        info_labels = next_info_labels\n",
        "\n",
        "                else:\n",
        "                    # Save data as this is when the initial state and the next state are not identical\n",
        "                    state, next_state = self.state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                    reward, pacman_done = self.reward_collection(reward, done, info_labels, next_info_labels) # Nawid - Changed name to pacman done as the variable done is used to exit the loop\n",
        "                    dataset_random.append([state, next_state,reward,pacman_done,sampled_action_one_hot])\n",
        "                \n",
        "                    obs = next_obs\n",
        "                    info_labels = next_info_labels\n",
        "                    ''' Used to test whether the done behaviour was correct\n",
        "                    if pacman_done:\n",
        "                        i +=1 \n",
        "                        print('pacman done', i)\n",
        "                    '''\n",
        "\n",
        "                if done:\n",
        "                    break \n",
        "        '''               \n",
        "        #dataset_random = np.array(dataset_random)\n",
        "        first_dataset = np.array(dataset_random[0])\n",
        "        print(first_dataset[0])\n",
        "        print('zero',dataset_random[0])\n",
        "        print('One',dataset_random[1])\n",
        "        print('Two',dataset_random[2])\n",
        "        print('Three',dataset_random[3])\n",
        "        print('Four',dataset_random[4])\n",
        "        #print(dataset_random.shape)\n",
        "        '''\n",
        "        return dataset_random\n",
        "    \n",
        "\n",
        "    def state_collection(self,obs, next_obs, info_labels, next_info_labels):\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            state = []\n",
        "            next_state = []\n",
        "            i = 0 \n",
        "            for key in info_labels :\n",
        "                if i < 14: # Nawid - Only the first 14 info are crucial I believe\n",
        "                    state.append(info_labels[key])\n",
        "                    next_state.append(next_info_labels[key])\n",
        "                    i +=1\n",
        "                else:\n",
        "                    state = np.array(state)\n",
        "                    next_state = np.array(next_state)\n",
        "                    return state, next_state\n",
        "    \n",
        "        elif self.state_mode == 'STDIM': # Encoding is the state\n",
        "            assert self.encoder is not None\n",
        "            with torch.no_grad():\n",
        "                self.encoder.eval()\n",
        "                state = self.encoder(obs.float().to(self.device) / 255)\n",
        "                next_state = self.encoder(next_obs.float().to(self.device) / 255)\n",
        "                return state.cpu().detach().numpy(), next_state.cpu().detach().numpy()\n",
        "\n",
        "        else: # Image observations are the state or the RAM labels\n",
        "            return obs, next_obs\n",
        "\n",
        "    def reward_collection(self,reward,done,info_labels,next_info_labels):\n",
        "        # checks whether there has been a change in lives- change done to 1, otherwise the done should be fine regardless\n",
        "        if not info_labels['num_lives'] == next_info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "            #print('change in lives')\n",
        "            done = 1 \n",
        "            self.repeated_initial = True\n",
        "        return reward, done\n",
        "\n",
        "    def unison_shuffled_copies(self,*args): # Nawid- Randomises all the different values, using *args to use a variable number of parameters so i can shuffle many different values at once\n",
        "        p = np.random.permutation(len(args[0]))\n",
        "        shuffled = [i[p] for i in args]\n",
        "        return shuffled\n",
        "    \n",
        "    def collate_data(self,random_dataset, rl_dataset):\n",
        "        if len(rl_dataset) > 0:\n",
        "                \n",
        "            random_dataset = np.array(random_dataset)\n",
        "            rl_dataset = np.array(rl_dataset)\n",
        "            print('random_dataset', random_dataset.shape)\n",
        "            print('rl dataset', rl_dataset.shape)\n",
        "        \n",
        "            d_concat = np.concatenate([random_dataset, rl_dataset], axis=0)\n",
        "            print('d_concat',d_concat.shape)        \n",
        "        else:\n",
        "            d_concat = np.array(random_dataset)\n",
        "        \n",
        "        num_examples_added = len(d_concat)\n",
        "\n",
        "        # Split the dataset into train(80%) and test(20%)\n",
        "        D_train = d_concat[:int(-num_examples_added*1/5)]\n",
        "        D_valid = d_concat[int(-num_examples_added*1/5):]\n",
        "\n",
        "        print(\"len(D):\", len(d_concat), 'len(Dtrain)', len(D_train))\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        sff = np.arange(len(D_train))\n",
        "        np.random.shuffle(sff)\n",
        "        D_train = D_train[sff]\n",
        "\n",
        "        # Create the input and output for the train\n",
        "        X_train_obs = np.array([obs for obs,_,_,_,_ in D_train]) # Takes obs and action\n",
        "        X_train_act = np.array([act for _,_,_,_,act in D_train])\n",
        "\n",
        "        # Env output\n",
        "        y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "        y_env_train = y_env_train - np.array([obs for obs,_,_,_,_ in D_train]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        # Reward's output\n",
        "        y_rew_train = np.array([[D] for _,_,_,D,_ in D_train])\n",
        "    \n",
        "        # Next state output\n",
        "        X_val_obs = np.array([obs for obs,_,_,_,_ in D_valid]) # Takes obs and action\n",
        "        X_val_act = np.array([act for _,_,_,_,act in D_valid])\n",
        "\n",
        "        y_env_val = np.array([no for _,no,_,_,_ in D_valid])\n",
        "        y_env_val = y_env_val - np.array([obs for obs,_,_,_,_ in D_valid]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        # Reward output\n",
        "        y_rew_val = np.array([[D] for _,_,_,D,_ in D_valid])\n",
        "    \n",
        "        env_train_data, env_val_data = (X_train_obs, X_train_act, y_env_train), (X_val_obs, X_val_act, y_env_val)\n",
        "        rew_train_data, rew_val_data = (X_train_obs, X_train_act, y_rew_train), (X_val_obs, X_val_act, y_rew_val)\n",
        "\n",
        "        return env_train_data, env_val_data, rew_train_data, rew_val_data \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "data_collector = Data_collection('MsPacmanNoFrameskip-v4')\n",
        "random_data = data_collector.gather_random_trajectories(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7jzgjUs3fx7",
        "colab_type": "code",
        "outputId": "c60ee51c-8384-4fc6-e8d3-4bf575992961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def state_collection(obs, next_obs, info_labels, next_info_labels,mode='ATARIARI', encoder = None):\n",
        "    if mode == 'ATARIARI':\n",
        "        state = []\n",
        "        next_state = []\n",
        "        i = 0 \n",
        "        for key in info_labels :\n",
        "            if i < 14: # Nawid - Only the first 14 info are crucial I believe\n",
        "                state.append(info_labels[key])\n",
        "                next_state.append(info_labels[key])\n",
        "                i +=1\n",
        "            else:\n",
        "                state = np.array(state)\n",
        "                next_state = np.array(next_state)\n",
        "                return state, next_state\n",
        "    \n",
        "    elif mode == 'STDIM': # Encoding is the state\n",
        "        assert encoder is not None\n",
        "        with torch.no_grad():\n",
        "            encoder.eval()\n",
        "            states = encoder(obs.float().to(device) / 255)\n",
        "            next_states = encoder(next_obs.float().to(device) / 255)\n",
        "            return states.cpu().detach().numpy(), next_states.cpu().detach().numpy()\n",
        "\n",
        "    states.cpu().detach().numpy()\n",
        "\n",
        "    else: # Image observations are the state or the RAM labels\n",
        "        return obs, next_obs\n",
        "\n",
        "'''\n",
        "env = AtariARIWrapper(gym.make('MsPacmanNoFrameskip-v4')) \n",
        "info_labels = env.labels()\n",
        "#print(info_labels)\n",
        "state = []\n",
        "print(info_labels)\n",
        "for key in info_labels:\n",
        "    state.append(info_labels[key])\n",
        "\n",
        "state = np.array(state)\n",
        "compact_state = state[0:14]\n",
        "print(compact_state)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nenv = AtariARIWrapper(gym.make('MsPacmanNoFrameskip-v4')) \\ninfo_labels = env.labels()\\n#print(info_labels)\\nstate = []\\nprint(info_labels)\\nfor key in info_labels:\\n    state.append(info_labels[key])\\n\\nstate = np.array(state)\\ncompact_state = state[0:14]\\nprint(compact_state)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmxqcOky0Yhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_random_trajectories(num_traj,ENV_NAME,mode = 'ATARIARI', encoder = None):\n",
        "    '''\n",
        "    states_list = []\n",
        "    next_states_list = []\n",
        "    rewards_list = []\n",
        "    dones_list = []\n",
        "    actions_list = []\n",
        "    '''\n",
        "    dataset_random = []\n",
        "    #Env name could either be the RAM case or the generic case\n",
        "    env = AtariARIWrapper(gym.make(ENV_NAME)) \n",
        "\n",
        "    for n in range(num_traj):\n",
        "        obs = env.reset()\n",
        "        repeated_initial = True # Nawid- Used to represent the initial state\n",
        "        initial_info_labels = env.labels()\n",
        "        info_labels = env.labels() # Nawid -  Used to get the current state\n",
        "        while True:\n",
        "            sampled_action = np.random.randint(0,n_actions)\n",
        "            sampled_action_one_hot = one_hot(sampled_action)\n",
        "            next_obs, reward, done, next_info = env.step(sampled_action)\n",
        "            next_info_labels = next_info['labels']\n",
        "\n",
        "            if repeated_initial: # If the initial state is repeating\n",
        "                if initial_info_labels == next_info_labels: # Current state is still the same as the initial state\n",
        "                    pass\n",
        "                else:\n",
        "                    repeated_initial = False\n",
        "                    \n",
        "                    state, next_state = state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                    reward, done = reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                    dataset_random.append([state, next_state,reward,done,sampled_action_one_hot])\n",
        "                    \n",
        "                    obs = next_obs\n",
        "                    info_labels = next_info_labels\n",
        "\n",
        "                    # Go into a new state and from the previous state and the current state as this is a transition between the states\n",
        "\n",
        "            else:\n",
        "                # Save data as this is when the initial state and the next state are not identical\n",
        "                state, next_state = state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                reward, done = reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                dataset_random.append([state, next_state,reward,done,sampled_action_one_hot])\n",
        "                \n",
        "                obs = next_obs\n",
        "                info_labels = next_info_labels\n",
        "\n",
        "                if not initial_info_labels['num_lives'] == info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "                    print('change in lives')\n",
        "                    repeated_initial = True\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQkLJkpaWWxN",
        "colab_type": "code",
        "outputId": "aff31269-d627-486c-c3f9-1ae3c6cc0894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def gather_random_trajectories(steps,ENV_NAME,encoder):\n",
        "    \n",
        "    states_list = []\n",
        "    next_states_list = []\n",
        "    rewards_list = []\n",
        "    dones_list = []\n",
        "    actions_list = []\n",
        "    #dataset_random = []\n",
        "    \n",
        "    envs = make_vec_envs(ENV_NAME,1,8,downsample=False)\n",
        "    obs = envs.reset()\n",
        "    ale_lives = np.full((workers,1),3) \n",
        "    for i in range(steps//8):\n",
        "        #actions = torch.tensor(np.array([np.random.randint(1, envs.action_space.n) for _ in range(workers)])).unsqueeze(1)\n",
        "        actions = torch.tensor(np.array([np.random.randint(1, n_actions) for _ in range(workers)])).unsqueeze(1) # Nawid - Choosing actions on a smaller action space\n",
        "        \n",
        "        next_obs, rewards, _, infos = envs.step(actions)\n",
        "\n",
        "        next_ale_lives = np.expand_dims(np.array([i['ale.lives'] for i in infos]),1) # Nawid - Extracts the information related to the number of lives\n",
        "        dones =  ale_lives - next_ale_lives # Nawid -  Finds the change in lives after a timestep\n",
        "        dones[dones< 1] = 0 # Nawid - Makes it so that the situation where it resets does not have a change in 3 for the values\n",
        "        ale_lives = next_ale_lives # Nawid- Update the ale lives\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            encoder.eval()\n",
        "            states = encoder(obs.float().to(device) / 255)\n",
        "            next_states = encoder(next_obs.float().to(device) / 255)\n",
        "\n",
        "        sampled_action_one_hot = np.array([one_hot(action) for action in actions]) # Nawid - Looks at each action in the vectorised environment and obtains the different\n",
        "        \n",
        "        states_list.append(states.cpu().detach().numpy())\n",
        "        next_states_list.append(next_states.cpu().detach().numpy())\n",
        "        rewards_list.append(rewards.cpu().numpy())\n",
        "        dones_list.append(dones)\n",
        "        # dones_list.append(np.expand_dims(dones,1))\n",
        "        actions_list.append(sampled_action_one_hot)\n",
        "        \n",
        "        #dataset_random.append([states.cpu().detach().numpy(), next_states.cpu().detach().numpy(), rewards.cpu().numpy(), np.expand_dims(dones,1), sampled_action_one_hot])\n",
        "    envs.close()\n",
        "    encoder.train()\n",
        "    \n",
        "    states_list = np.concatenate(states_list)\n",
        "    next_states_list = np.concatenate(next_states_list)\n",
        "    rewards_list = np.concatenate(rewards_list)\n",
        "    dones_list = np.concatenate(dones_list)\n",
        "    actions_list =  np.concatenate(actions_list)\n",
        "    return states_list, next_states_list, rewards_list, dones_list, actions_list\n",
        "    \n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = ImpalaCNN(1,all_defaults)\n",
        "encoder.to(device)\n",
        "states_list,next_states_list, rewards_list, dones_list, actions_list = gather_random_trajectories(10000,'MsPacmanNoFrameskip-v4',encoder)\n",
        "#random_dataset = gather_random_trajectories(1000,'MsPacmanNoFrameskip-v4',encoder)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards list (10000, 1)\n",
            "dones list (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1IHuflHNJDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unison_shuffled_copies(*args): # Nawid- Randomises all the different values, using *args to use a variable number of parameters so i can shuffle many different values at once\n",
        "    p = np.random.permutation(len(args[0]))\n",
        "    shuffled = [i[p] for i in args]\n",
        "    return shuffled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBHnnR5yKyyR",
        "colab_type": "code",
        "outputId": "80d69320-665a-4943-b9b4-e53240b76d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def collate_data(random_dataset, rl_dataset, num_examples_added):\n",
        "    if len(rl_dataset)>0:\n",
        "        rand_states, rand_next_states, rand_rewards, rand_dones, rand_actions = random_dataset\n",
        "        rl_states, rl_next_states, rl_rewards,rl_dones,rl_actions = rl_dataset\n",
        "        states = np.concatenate(rand_states,rl_states)\n",
        "        next_states = np.concatenate(rand_next_states,rl_next_states) \n",
        "        rewards = np.concatenate(rand_rewards,rl_rewards)\n",
        "        dones = np.concatenate(rand_dones,rl_dones)\n",
        "        actions = np.concatenate(rand_actions,rl_actions)\n",
        "    else:\n",
        "        states, next_states, rewards, dones, actions_onehot = random_dataset\n",
        "\n",
        "    # Separate out into training and validation sets\n",
        "    X_train_states, X_val_states = states[:int(-num_examples_added*1/5)], states[int(-num_examples_added*1/5):]\n",
        "    X_train_act, X_val_act = actions_onehot[:int(-num_examples_added*1/5)], actions_onehot[int(-num_examples_added*1/5):]\n",
        "    \n",
        "    y_env_train, y_env_val = next_states[:int(-num_examples_added*1/5)], next_states[int(-num_examples_added*1/5):]\n",
        "    y_env_train, y_env_val = y_env_train - X_train_states, y_env_val - X_val_states\n",
        "    \n",
        "    y_rew_train, y_rew_val = dones[:int(-num_examples_added*1/5)], dones[int(-num_examples_added*1/5):]\n",
        "    \n",
        "    # Shuffles all the training data in the same way\n",
        "    X_train_states, X_train_act, y_env_train, y_rew_train = unison_shuffled_copies(X_train_states, X_train_act, y_env_train,y_rew_train)   \n",
        "\n",
        "    env_train_data, env_val_data = (X_train_states, X_train_act, y_env_train), (X_val_states, X_val_act, y_env_val)\n",
        "    rew_train_data, rew_val_data = (X_train_states, X_train_act, y_rew_train), (X_val_states, X_val_act, y_rew_val)\n",
        "\n",
        "    return env_train_data, env_val_data, rew_train_data, rew_val_data\n",
        "\n",
        "'''\n",
        "random_dataset = gather_random_trajectories(1000,'MsPacmanNoFrameskip-v4',encoder)\n",
        "num_examples = len(states_list)\n",
        "rl_dataset = []\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(random_dataset,rl_dataset,num_examples)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nrandom_dataset = gather_random_trajectories(1000,'MsPacmanNoFrameskip-v4',encoder)\\nnum_examples = len(states_list)\\nrl_dataset = []\\nenv_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(random_dataset,rl_dataset,num_examples)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCHWWzZ_f5GV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalise(train_data, val_data, scaler = None): # Nawid - Used to normalise each dimension individually\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        train_data = scaler.fit_transform(train_data)\n",
        "    else: \n",
        "        train_data = scaler.transform(train_data)\n",
        "        \n",
        "    val_data = scaler.transform(val_data)\n",
        "    return train_data, val_data, scaler\n",
        "\n",
        "def normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data,X_env_obs_scaler = None,y_env_scaler=None,X_rew_obs_scaler= None):\n",
        "    # Unpack data\n",
        "    (X_env_train_obs, X_env_train_act, y_env_train), (X_env_val_obs, X_env_val_act, y_env_val) = env_train_data, env_val_data\n",
        "    (X_rew_train_obs, X_rew_train_act, y_rew_train), (X_rew_val_obs, X_rew_val_act, y_rew_val) = rew_train_data, rew_val_data\n",
        "    \n",
        "    # Normalise training and validation data\n",
        "    X_env_train_obs,X_env_val_obs, X_env_obs_scaler =  normalise(X_env_train_obs, X_env_val_obs, X_env_obs_scaler)\n",
        "    y_env_train, y_env_val, y_env_scaler = normalise(y_env_train, y_env_val, y_env_scaler)\n",
        "    X_rew_train_obs, X_rew_val_obs, X_rew_obs_scaler = normalise(X_rew_train_obs, X_rew_val_obs, X_rew_obs_scaler)\n",
        "    \n",
        "    # Concatentates the normalised states with the one hot vector for the actions\n",
        "    X_env_train = np.concatenate((X_env_train_obs,X_env_train_act),axis=1)\n",
        "    X_env_val = np.concatenate((X_env_val_obs,X_env_val_act),axis=1)\n",
        "    X_rew_train = np.concatenate((X_rew_train_obs,X_rew_train_act),axis=1)\n",
        "    X_rew_val = np.concatenate((X_rew_val_obs,X_rew_val_act),axis=1)\n",
        "\n",
        "    # Pack data tuples\n",
        "    env_train_data, env_val_data = (X_env_train, y_env_train),(X_env_val, y_env_val) \n",
        "    rew_train_data, rew_val_data = (X_rew_train, y_rew_train),(X_rew_val, y_rew_val)\n",
        "\n",
        "    return env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_obs_scaler, y_env_scaler, X_rew_obs_scaler\n",
        "\n",
        "#normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ahvG8lhSVp",
        "colab_type": "text"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3T5LHY6hRW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_data,val_data,model,batch_size, max_model_iter, optimizer, device,early_stopper,desired_model='Env_model'):\n",
        "    ''' \n",
        "    General function to train either of the two models\n",
        "    '''\n",
        "    # Unpack data\n",
        "    (X_train, y_train), (X_val, y_val) =  train_data, val_data\n",
        "    losses_env = []\n",
        "\n",
        "    # Choose loss function based on what type of model is training\n",
        "    if desired_model =='Env_model': # Nawid -  Decides which loss function to use\n",
        "        loss_function = model_MSEloss\n",
        "    else:\n",
        "        loss_function = model_BCEloss\n",
        "\n",
        "    # go through max_model iter supervised iterations\n",
        "    for it in tqdm(range(max_model_iter)):\n",
        "        # create mini batches of size batch_size\n",
        "        for mb in range(0,len(X_train), batch_size): # Nawid- Batch size is the step size\n",
        "            if len(X_train) > mb + BATCH_SIZE:\n",
        "                X_mb = X_train[mb:mb+BATCH_SIZE]\n",
        "                y_mb = y_train[mb:mb+BATCH_SIZE]\n",
        "                #X_mb += np.random.normal(loc = 0, scale = 0.001, size= X_mb.shape)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # forward pass of model to compute the output\n",
        "                pred_mb = model(torch.tensor(X_mb).to(device))\n",
        "                \n",
        "                # compute the loss\n",
        "                loss = loss_function(pred_mb,y_mb,device) #Nawid-  Uses Mse loss if dynamics model or uses BCE loss if reward/done model\n",
        "                wandb.log({'{} Training loss'.format(desired_model):loss.cpu().detach().numpy()})\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                # optimization step\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Nawid - Calculate the validation loss after each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_val = model(torch.tensor(X_val).to(device))\n",
        "            val_loss = loss_function(pred_val, y_val,device)\n",
        "            wandb.log({'{} Validation loss'.format(desired_model):val_loss})\n",
        "        \n",
        "\n",
        "        # Checks whether to early stop after each epoch\n",
        "        early_stopper(val_loss,model)\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQGgpOpiAlT",
        "colab_type": "text"
      },
      "source": [
        "# Controller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtZyr8TUdghZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_model_based_control_oracle_state(rew_model, real_obs, num_sequences, horizon_length, sample_action, norm,device):\n",
        "    '''\n",
        "    Use a random-sampling shooting method generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "    '''\n",
        "    oracle_env = gym.make('MsPacman')\n",
        "    best_reward = -1e9\n",
        "    best_next_action = []\n",
        "\n",
        "    (env_input_scaler, env_output_scaler, rew_input_scaler) =norm\n",
        "    m_obs = np.array([real_obs for _ in range(num_sequences)])\n",
        "\n",
        "    # array that contains the rewards for all the sequence\n",
        "    unroll_rewards = np.zeros((num_sequences,1))\n",
        "    first_sampled_actions = []\n",
        "\n",
        "    rew_model.eval()\n",
        "\n",
        "    # Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon length' times\n",
        "    ## i.e roll a given number of trajectories in a single batch (to increase speed)\n",
        "    \n",
        "    for t in range(horizon_length):\n",
        "        # sample actions for each sequence\n",
        "        sampled_actions = np.array([np.random.randint(1, n_actions) for _ in range(num_sequences)]) # Nawid - Choosing actions on a smaller action space\n",
        "        #sampled_actions = np.array([sample_action() for _ in range(num_sequences)])\n",
        "        sampled_actions_one_hot = np.array([one_hot(action) for action in sampled_actions])\n",
        "\n",
        "        if isinstance(env_model,NNDynamicsModel): # Nawid-  If the env model is a neural net\n",
        "            #print('using dynamics model')\n",
        "            m_obs_env_scaled = env_input_scaler.transform(m_obs)\n",
        "            env_model_input = np.concatenate([m_obs_env_scaled, sampled_actions_one_hot], axis = 1)\n",
        "            # compute the next state for each sequence\n",
        "            pred_obs = env_model(torch.tensor(env_model_input).to(device))\n",
        "\n",
        "            # inverse scaler transformation\n",
        "            pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
        "            # add previous observation\n",
        "            next_obs = pred_obs + m_obs\n",
        "        else:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268AcQH1h-24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_model_based_control(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, norm,device):\n",
        "    '''\n",
        "    Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "    '''\n",
        "    best_reward = -1e9\n",
        "    best_next_action = []\n",
        "    \n",
        "    (env_input_scaler, env_output_scaler, rew_input_scaler) =norm\n",
        "    m_obs = np.array([real_obs for _ in range(num_sequences)])\n",
        "    \n",
        "    # array that contains the rewards for all the sequence\n",
        "    unroll_rewards = np.zeros((num_sequences, 1)) \n",
        "    first_sampled_actions = []\n",
        "\n",
        "    env_model.eval()\n",
        "    rew_model.eval()\n",
        "\n",
        "    # Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon length' times\n",
        "    ## i.e roll a given number of trajectories in a single batch (to increase speed)\n",
        "    \n",
        "    for t in range(horizon_length):\n",
        "        # sample actions for each sequence\n",
        "        sampled_actions = np.array([sample_action() for _ in range(num_sequences)])\n",
        "        sampled_actions_one_hot = np.array([one_hot(action) for action in sampled_actions])\n",
        "\n",
        "\n",
        "        if isinstance(env_model,NNDynamicsModel): # Nawid-  If the env model is a neural net\n",
        "            #print('using dynamics model')\n",
        "            m_obs_env_scaled = env_input_scaler.transform(m_obs)\n",
        "            env_model_input = np.concatenate([m_obs_env_scaled, sampled_actions_one_hot], axis = 1)\n",
        "            # compute the next state for each sequence\n",
        "            pred_obs = env_model(torch.tensor(env_model_input).to(device))\n",
        "\n",
        "            # inverse scaler transformation\n",
        "            pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
        "            # add previous observation\n",
        "            next_obs = pred_obs + m_obs\n",
        "        else:\n",
        "            #print('using oracle state')            \n",
        "            next_obs = env_model.predict_states(m_obs,sampled_actions) # Nawid - Able to obtain the next state directly rather than predicting a change in states\n",
        "\n",
        "        if isinstance(rew_model, NNRewardModel):\n",
        "            m_obs_rew_scaled = rew_input_scaler.transform(m_obs)\n",
        "            rew_model_input = np.concatenate([m_obs_rew_scaled, sampled_actions_one_hot], axis = 1)\n",
        "            pred_rew = rew_model(torch.tensor(rew_model_input).to(device)) # Nawid -  I believe I do not need to rescale for a True of false situation\n",
        "            unroll_rewards +=  (1 - pred_rew.cpu().detach().numpy())\n",
        "        else:\n",
        "            #print('using oracle reward')\n",
        "            pred_rew = rew_model.predict_reward(m_obs, sampled_actions,next_obs)\n",
        "            unroll_rewards += pred_rew\n",
        "\n",
        "        m_obs = next_obs # Nawid - Update the state after calculating the new state and calculating the rewards\n",
        "    \n",
        "        if t ==0:\n",
        "            first_sampled_actions = sampled_actions\n",
        "        \n",
        "    env_model.train()\n",
        "    rew_model.train()\n",
        "\n",
        "    # Best the position of the sequence with the higher reward\n",
        "    arg_best_reward = np.argmax(unroll_rewards)\n",
        "    best_sum_reward = unroll_rewards[arg_best_reward].squeeze()\n",
        "    # take the first action of this sequence\n",
        "    best_action = first_sampled_actions[arg_best_reward]\n",
        "    #best_action =  np.squeeze(best_action)\n",
        "    return best_action, best_sum_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKm_FKWGnPep",
        "colab_type": "text"
      },
      "source": [
        "# Saving config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5P-E36KnFoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = wandb.config\n",
        "config.batch_size = BATCH_SIZE          \n",
        "config.horizon_length = HORIZON_LENGTH\n",
        "config.num_action_seq = NUM_ACTIONS_SEQUENCES\n",
        "config.train_model_iter = TRAIN_ITER_MODEL \n",
        "config.num_rand_trajectories = NUM_RAND_TRAJECTORIES\n",
        "config.aggr_iter = AGGR_ITER\n",
        "config.steps_per_aggr = STEPS_PER_AGGR\n",
        "config.env_lr = ENV_LEARNING_RATE\n",
        "config.rew_lr = REW_LEARNING_RATE\n",
        "config.no_actions = n_actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIBf1pknbf_",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcpZM7X0MQtc",
        "colab_type": "text"
      },
      "source": [
        "Dynamics model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqE-9CLYK6jF",
        "colab_type": "code",
        "outputId": "e1201452-eabb-4faf-9408-b915bd48d326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "\n",
        "encoder = NatureCNN(observation_channels,all_defaults)\n",
        "encoder.load_state_dict(torch.load('/content/MsPacmanNoFrameskip-v4_55.pt'))\n",
        "encoder.to(device)\n",
        "encoder.eval()\n",
        "rand_dataset = gather_random_trajectories(10000,'MsPacmanNoFrameskip-v4',encoder)\n",
        "rl_dataset = []\n",
        "num_examples_added = len(rand_dataset)\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(rand_dataset,rl_dataset,num_examples_added)\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "norm = (X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "\n",
        "env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "wandb.watch(env_model, log=\"all\")\n",
        "env_model_name = 'Env_model'+ '_' + date_time\n",
        "early_stopping_env = EarlyStopping(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_env.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▎         | 2/55 [00:00<00:18,  2.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy increased for Env_model_27_13.53.44  (0.000000 --> 1.118334).  Saving model ...\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 1 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 4/55 [00:00<00:11,  4.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_27_13.53.44 counter: 2 out of 5\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 3 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 5/55 [00:01<00:10,  4.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_27_13.53.44 counter: 4 out of 5\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 5 out of 5\n",
            "Env_model_27_13.53.44 has stopped\n",
            "Early stopping\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6TG9BDMMTS5",
        "colab_type": "text"
      },
      "source": [
        "Reward model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGP9vBlbnMQ9",
        "colab_type": "code",
        "outputId": "78040184-3d95-4815-c719-e05061a4bc2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Instantiate reward model\n",
        "env = gym.make('MsPacmanNoFrameskip-v4')\n",
        "encoder = ImpalaCNN(observation_channels,all_defaults)\n",
        "encoder.load_state_dict(torch.load('/content/MsPacmanNoFrameskip-v4_95.pt'))\n",
        "encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "rew_model = NNRewardModel(n_actions + feature_size,reward_dim).to(device)\n",
        "rew_optimizer = torch.optim.Adam(rew_model.parameters(), REW_LEARNING_RATE)\n",
        "wandb.watch(rew_model, log=\"all\")\n",
        "desired_model = 'Reward_model'\n",
        "model_name = desired_model +'_'+ date_time\n",
        "early_stopping_rew = EarlyStopping(patience=7, verbose=True, wandb=wandb, name=model_name)\n",
        "\n",
        "game_reward = 0\n",
        "rand_dataset = gather_random_trajectories(10000,'MsPacmanNoFrameskip-v4',encoder)\n",
        "print('Does it work past here')\n",
        "rl_dataset = []\n",
        "num_examples_added = len(rand_dataset)\n",
        "\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(rand_dataset, rl_dataset, num_examples_added)\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "print('Is data normalised?')\n",
        "#env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(rand_dataset,rl_dataset,num_examples_added)\n",
        "\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_reward.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    #norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "    print('Does the model train')\n",
        "    train_model(norm_rew_train_data, norm_rew_val_data, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL,rew_optimizer,device,early_stopping_rew)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rewards list (10000, 1)\n",
            "dones list (10000, 1)\n",
            "Does it work past here\n",
            "Is data normalised?\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 3/55 [00:00<00:29,  1.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy increased for Reward_model_2_9.26.46  (0.000000 --> 0.230905).  Saving model ...\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 1 out of 7\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 2 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 5/55 [00:01<00:21,  2.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 3 out of 7\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 4 out of 7\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 5 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 7/55 [00:01<00:09,  4.91it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 6 out of 7\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 7 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 8 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 9 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 10 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 11 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 12 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 13 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 14 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 15 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 16 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpGt1rdvsLFT",
        "colab_type": "text"
      },
      "source": [
        "# OLD CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGXOh2oVWT8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_random_trajectories(num_traj,ENV_NAME,mode = 'ATARIARI', encoder = None):\n",
        "    '''\n",
        "    states_list = []\n",
        "    next_states_list = []\n",
        "    rewards_list = []\n",
        "    dones_list = []\n",
        "    actions_list = []\n",
        "    '''\n",
        "    dataset_random = []\n",
        "    #Env name could either be the RAM case or the generic case\n",
        "    env = AtariARIWrapper(gym.make(ENV_NAME)) \n",
        "\n",
        "    for n in range(num_traj):\n",
        "        obs = env.reset()\n",
        "        repeated_initial = True # Nawid- Used to represent the initial state\n",
        "        initial_info_labels = env.labels()\n",
        "        info_labels = env.labels() # Nawid -  Used to get the current state\n",
        "        while True:\n",
        "            sampled_action = np.random.randint(0,n_actions)\n",
        "            sampled_action_one_hot = one_hot(sampled_action)\n",
        "            next_obs, reward, done, next_info = env.step(sampled_action)\n",
        "            next_info_labels = next_info['labels']\n",
        "\n",
        "            if repeated_initial: # If the initial state is repeating\n",
        "                if initial_info_labels == next_info_labels: # Current state is still the same as the initial state\n",
        "                    pass\n",
        "                else:\n",
        "                    repeated_initial = False\n",
        "                    \n",
        "                    state, next_state = state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                    reward, done = reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                    dataset_random.append([state, next_state,reward,done,sampled_action_one_hot])\n",
        "                    \n",
        "                    obs = next_obs\n",
        "                    info_labels = next_info_labels\n",
        "\n",
        "                    # Go into a new state and from the previous state and the current state as this is a transition between the states\n",
        "\n",
        "            else:\n",
        "                # Save data as this is when the initial state and the next state are not identical\n",
        "                state, next_state = state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                reward, done = reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                dataset_random.append([state, next_state,reward,done,sampled_action_one_hot])\n",
        "                \n",
        "                obs = next_obs\n",
        "                info_labels = next_info_labels\n",
        "\n",
        "                if not initial_info_labels['num_lives'] == info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "                    print('change in lives')\n",
        "                    repeated_initial = True\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWnL4S9xsedw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def state_collection(obs, next_obs, info_labels, next_info_labels,mode='ATARIARI', encoder = None):\n",
        "    if mode == 'ATARIARI':\n",
        "        state = []\n",
        "        next_state = []\n",
        "        i = 0 \n",
        "        for key in info_labels :\n",
        "            if i < 14: # Nawid - Only the first 14 info are crucial I believe\n",
        "                state.append(info_labels[key])\n",
        "                next_state.append(info_labels[key])\n",
        "                i +=1\n",
        "            else:\n",
        "                state = np.array(state)\n",
        "                next_state = np.array(next_state)\n",
        "                return state, next_state\n",
        "    \n",
        "    elif mode == 'STDIM': # Encoding is the state\n",
        "        assert encoder is not None\n",
        "        with torch.no_grad():\n",
        "            encoder.eval()\n",
        "            states = encoder(obs.float().to(device) / 255)\n",
        "            next_states = encoder(next_obs.float().to(device) / 255)\n",
        "            return states, next_states\n",
        "\n",
        "    else: # Image observations are the state or the RAM labels\n",
        "        return obs, next_obs\n",
        "\n",
        "'''\n",
        "env = AtariARIWrapper(gym.make('MsPacmanNoFrameskip-v4')) \n",
        "info_labels = env.labels()\n",
        "#print(info_labels)\n",
        "state = []\n",
        "print(info_labels)\n",
        "for key in info_labels:\n",
        "    state.append(info_labels[key])\n",
        "\n",
        "state = np.array(state)\n",
        "compact_state = state[0:14]\n",
        "print(compact_state)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}