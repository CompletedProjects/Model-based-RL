{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_based_STDIM_V2_090320.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMfyJrbcWh39+6YbyC+trw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/Model_based_STDIM_V2_090320.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "ffee927b-2ebd-4007-8192-fa8a008ec398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/mila-iqia/atari-representation-learning.git\n",
            "  Cloning git://github.com/mila-iqia/atari-representation-learning.git to /tmp/pip-req-build-89ga4jat\n",
            "  Running command git clone -q git://github.com/mila-iqia/atari-representation-learning.git /tmp/pip-req-build-89ga4jat\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (0.15.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->atariari==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: atariari\n",
            "  Building wheel for atariari (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atariari: filename=atariari-0.0.1-cp36-none-any.whl size=46584 sha256=261abdbc46ecd8bbb5efe895345397b7d30ff11a09c322063fe7094556615827\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ckhaz94b/wheels/3d/69/51/5e436e5ae566c5b4dec5c53e65396d516459877a42a11d7aa4\n",
            "Successfully built atariari\n",
            "Installing collected packages: atariari\n",
            "Successfully installed atariari-0.0.1\n",
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-5dce9ssj\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-5dce9ssj\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.15.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.1.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.10)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->a2c-ppo-acktr==0.0.1) (45.2.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=f976dc8fee9caa79f55b6de2623618ed53226c68e19f409854a55e220c09ba82\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k8y03gzw/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-sp_sex02\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-sp_sex02\n",
            "Requirement already satisfied: gym<0.16.0,>=0.15.4 in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.15.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.28.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.4.10)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.17.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=99c27805d5e9d275d2c83114cdaa3a2a681e5f5e7b5d2e21bfe8fd5f35cc1248\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zvsokg4f/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "Successfully built baselines\n",
            "Installing collected packages: baselines\n",
            "Successfully installed baselines-0.1.6\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/05/a0bf45b2f4909c3ffb1729deb19355a067a8cf8d56eebd3159d702321b68/wandb-0.8.29-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 25.6MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/e6/058c2dc4723b3647a8cf61385c61f284bfbd0d0657bc5623c22e9ef45a3c/sentry_sdk-0.14.2-py2.py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.2MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.0MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hCollecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: gql, watchdog, subprocess32, graphql-core, pathtools\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=0d91167cd3586aa11f5b7f6bf7385bbabf91d76af032ab63e07a5968850f3fb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=364fa7823e75733b6a172b874170a1d5528e0338cbee9175aa24e7753995bc81\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=cf24fae1f38190ac323d2f03f08cd1275265eaa6435c350bc93c5ca13cedc79e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=178cf81bc9743c1d4f8e11a4f1e3c729f1a70f3e014143a3a0b76377dfac7f34\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=b8a1efb6e3679b52397fcae32ac5957b7a48f22f9c274c9591ab711e0e7c27b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built gql watchdog subprocess32 graphql-core pathtools\n",
            "Installing collected packages: configparser, smmap, gitdb, GitPython, graphql-core, gql, shortuuid, pathtools, watchdog, sentry-sdk, subprocess32, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.0 configparser-4.0.2 docker-pycreds-0.4.0 gitdb-4.0.2 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.2 shortuuid-1.0.1 smmap-3.0.1 subprocess32-3.5.4 wandb-0.8.29 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZ9gMrxp5vU",
        "colab_type": "code",
        "outputId": "d3dba14f-88ee-4471-b3b6-ec20c6728e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNZIoA0Fp9Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, BatchSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from atariari.methods.utils import calculate_accuracy, Cutout, EarlyStopping\n",
        "from atariari.methods.trainer import Trainer\n",
        "from atariari.benchmark.episodes import get_episodes\n",
        "from atariari.benchmark.envs import *\n",
        "from atariari.methods.utils import get_argparser\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Imported required for the Model-based RL\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "70410511-5896-4de9-b93a-accd6cf1051f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!wandb login ####################"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gdFb0bqHHN",
        "colab_type": "code",
        "outputId": "055cc8d7-39d9-4ebe-c244-b989b1b63477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "parser = get_argparser()\n",
        "args = parser.parse_args(\"\")\n",
        "all_defaults = {}\n",
        "for key in vars(args):\n",
        "    all_defaults[key] = parser.get_default(key)\n",
        "\n",
        "all_defaults['env_name'] = 'MsPacmanNoFrameskip-v4'\n",
        "\n",
        "wandb.init(entity=\"nerdk312\", project=\"STDIM\",config=all_defaults)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/STDIM\" target=\"_blank\">https://app.wandb.ai/nerdk312/STDIM</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/STDIM/runs/2jxg9nvq\" target=\"_blank\">https://app.wandb.ai/nerdk312/STDIM/runs/2jxg9nvq</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/nerdk312/STDIM/runs/2jxg9nvq"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G5adriWecft",
        "colab_type": "code",
        "outputId": "39d9886c-0430-4504-9a93-0fca5c0ef9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "all_defaults"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'beta': 1.0,\n",
              " 'checkpoint_index': -1,\n",
              " 'collect_mode': 'random_agent',\n",
              " 'color': False,\n",
              " 'cuda_id': 0,\n",
              " 'encoder_type': 'Nature',\n",
              " 'end_with_relu': False,\n",
              " 'entropy_threshold': 0.6,\n",
              " 'env_name': 'MsPacmanNoFrameskip-v4',\n",
              " 'epochs': 100,\n",
              " 'feature_size': 256,\n",
              " 'gru_layers': 2,\n",
              " 'gru_size': 256,\n",
              " 'linear': True,\n",
              " 'lr': 0.0003,\n",
              " 'method': 'infonce-stdim',\n",
              " 'naff_fc_size': 2048,\n",
              " 'no_downsample': True,\n",
              " 'num_frame_stack': 1,\n",
              " 'num_processes': 8,\n",
              " 'num_rew_evals': 10,\n",
              " 'num_runs': 1,\n",
              " 'patience': 15,\n",
              " 'pred_offset': 1,\n",
              " 'pretraining_steps': 100000,\n",
              " 'probe_collect_mode': 'random_agent',\n",
              " 'probe_lr': 0.0003,\n",
              " 'probe_steps': 50000,\n",
              " 'seed': 42,\n",
              " 'sequence_length': 100,\n",
              " 'steps_end': 99,\n",
              " 'steps_start': 0,\n",
              " 'steps_step': 4,\n",
              " 'train_encoder': True,\n",
              " 'use_multiple_predictors': False,\n",
              " 'wandb_entity': None,\n",
              " 'wandb_proj': 'atari-reps',\n",
              " 'weights_path': 'None'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aFJO7vdqOY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from a2c_ppo_acktr.utils import init\n",
        "import time\n",
        "from atariari.benchmark.utils import download_run\n",
        "from atariari.benchmark.episodes import checkpointed_steps_full_sorted\n",
        "import os\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class Conv2dSame(torch.nn.Module): # Nawid - Performs convolution in the same way as 'same' tensorflow format I assume\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, bias=True, padding_layer=nn.ReflectionPad2d):\n",
        "        super().__init__()\n",
        "        ka = kernel_size // 2\n",
        "        kb = ka - 1 if kernel_size % 2 == 0 else ka\n",
        "        self.net = torch.nn.Sequential(\n",
        "            padding_layer((ka, kb, ka, kb)),\n",
        "            torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            Conv2dSame(in_channels, out_channels, 3),\n",
        "            nn.ReLU(),\n",
        "            Conv2dSame(in_channels, out_channels, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.block(x)\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImpalaCNN(nn.Module): # Nawid -  CNN architecture in the Impala paper\n",
        "    def __init__(self, input_channels, args):\n",
        "        super(ImpalaCNN, self).__init__()\n",
        "        self.hidden_size = args['feature_size']\n",
        "        self.depths = [16, 32, 32, 32]\n",
        "        self.downsample = not args['no_downsample']\n",
        "        self.layer1 = self._make_layer(input_channels, self.depths[0])\n",
        "        self.layer2 = self._make_layer(self.depths[0], self.depths[1])\n",
        "        self.layer3 = self._make_layer(self.depths[1], self.depths[2])\n",
        "        self.layer4 = self._make_layer(self.depths[2], self.depths[3])\n",
        "        if self.downsample:\n",
        "            self.final_conv_size = 32 * 9 * 9\n",
        "        else:\n",
        "            self.final_conv_size = 32 * 12 * 9\n",
        "        self.final_linear = nn.Linear(self.final_conv_size, self.hidden_size)\n",
        "        self.flatten = Flatten()\n",
        "        self.train()\n",
        "\n",
        "    def _make_layer(self, in_channels, depth): # Nawid-  Used to make a layer\n",
        "        return nn.Sequential(\n",
        "            Conv2dSame(in_channels, depth, 3),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(depth, depth),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(depth, depth)\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def local_layer_depth(self):\n",
        "        return self.depths[-2]\n",
        "\n",
        "    def forward(self, inputs, fmaps=False):\n",
        "        #print(inputs.size())\n",
        "        f5 = self.layer3(self.layer2(self.layer1(inputs))) # Nawid -Uses the output of the third layer and then sees whether we want to downsample or not\n",
        "        \n",
        "        if not self.downsample:\n",
        "            out = self.layer4(f5)\n",
        "            \n",
        "        else:\n",
        "            out = f5\n",
        "            \n",
        "        \n",
        "        #print('before out size',out.size())\n",
        "        out = F.relu(self.final_linear(self.flatten(out))) # Nawid- global feature vector\n",
        "        #print('after out size', out.size())\n",
        "\n",
        "        if fmaps:\n",
        "            return {\n",
        "                'f5': f5.permute(0, 2, 3, 1), # Nawid - Make the channels in the last dimension\n",
        "                'out': out\n",
        "            }\n",
        "\n",
        "        return out\n",
        "\n",
        "class NatureCNN(nn.Module): # Nawid - Nature CNN\n",
        "\n",
        "    def __init__(self, input_channels, args):\n",
        "        super().__init__()\n",
        "        self.feature_size = args['feature_size']\n",
        "        self.hidden_size = self.feature_size\n",
        "        self.downsample = not args['no_downsample']\n",
        "        self.input_channels = input_channels\n",
        "        self.end_with_relu = args['end_with_relu']\n",
        "        self.args = args\n",
        "        init_ = lambda m: init(m,\n",
        "                               nn.init.orthogonal_,\n",
        "                               lambda x: nn.init.constant_(x, 0),\n",
        "                               nn.init.calculate_gain('relu'))\n",
        "        self.flatten = Flatten()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.final_conv_size = 32 * 7 * 7\n",
        "            self.final_conv_shape = (32, 7, 7)\n",
        "            self.main = nn.Sequential(\n",
        "                init_(nn.Conv2d(input_channels, 32, 8, stride=4)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(32, 64, 4, stride=2)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(64, 32, 3, stride=1)),\n",
        "                nn.ReLU(),\n",
        "                Flatten(),\n",
        "                init_(nn.Linear(self.final_conv_size, self.feature_size)),\n",
        "                #nn.ReLU()\n",
        "            )\n",
        "        else:\n",
        "            self.final_conv_size = 64 * 9 * 6\n",
        "            self.final_conv_shape = (64, 9, 6)\n",
        "            self.main = nn.Sequential(\n",
        "                init_(nn.Conv2d(input_channels, 32, 8, stride=4)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(32, 64, 4, stride=2)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(64, 128, 4, stride=2)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(128, 64, 3, stride=1)),\n",
        "                nn.ReLU(),\n",
        "                Flatten(),\n",
        "                init_(nn.Linear(self.final_conv_size, self.feature_size)),\n",
        "                #nn.ReLU()\n",
        "            )\n",
        "        self.train()\n",
        "\n",
        "    @property\n",
        "    def local_layer_depth(self):\n",
        "        return self.main[4].out_channels\n",
        "\n",
        "    def forward(self, inputs, fmaps=False):\n",
        "        f5 = self.main[:6](inputs)\n",
        "        f7 = self.main[6:8](f5)\n",
        "        out = self.main[8:](f7)\n",
        "        if self.end_with_relu:\n",
        "            assert self.args.method != \"vae\", \"can't end with relu and use vae!\"\n",
        "            out = F.relu(out)\n",
        "        if fmaps: # Nawid - obtains the different feature maps as well as global feature vector\n",
        "            return {\n",
        "                'f5': f5.permute(0, 2, 3, 1),\n",
        "                'f7': f7.permute(0, 2, 3, 1),\n",
        "                'out': out\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class PPOEncoder(nn.Module):\n",
        "    def __init__(self, env_name, checkpoint_index):\n",
        "        super().__init__()\n",
        "        checkpoint_step = checkpointed_steps_full_sorted[checkpoint_index]\n",
        "        filepath = download_run(env_name, checkpoint_step)\n",
        "        while not os.path.exists(filepath):\n",
        "            time.sleep(5)\n",
        "\n",
        "        self.masks = torch.zeros(1, 1)\n",
        "        self.ppo_model, ob_rms = torch.load(filepath, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, _, _, feature_vectors, _ = self.ppo_model.act(x,\n",
        "                                                            None,\n",
        "                                                            self.masks,\n",
        "                                                            deterministic=False)\n",
        "        return feature_vectors\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePMZh0WfleFC",
        "colab_type": "text"
      },
      "source": [
        "# Model-based RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUzSa9NikYHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NNDynamicModel(nn.Module):\n",
        "    '''\n",
        "    Model that predicts the next state, given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, obs_output_dim):\n",
        "        super(NNDynamicModel,self).__init__()\n",
        "        self.Linear1 = nn.Linear(input_dim,512)\n",
        "        self.BN1 = nn.BatchNorm1d(num_features = 512)\n",
        "        self.Linear2 = nn.Linear(512,256)\n",
        "        self.BN2 = nn.BatchNorm1d(num_features=256)\n",
        "        self.Linear3 = nn.Linear(256, obs_output_dim) \n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.Linear1(x.float())\n",
        "        x = self.BN1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Linear2(x)\n",
        "        x = self.BN2(x)\n",
        "        x = F.relu(x)\n",
        "        output = self.Linear3(x)        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mHFTQIJlts2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NNRewardModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the reward given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, reward_output_dim):\n",
        "        super(NNRewardModel, self).__init__()\n",
        "        \n",
        "        self.Linear1 = nn.Linear(input_dim,512)\n",
        "        self.BN1 = nn.BatchNorm1d(num_features = 512)\n",
        "        self.Linear2 = nn.Linear(512,256)\n",
        "        self.BN2 = nn.BatchNorm1d(num_features=256)\n",
        "        self.Linear3 = nn.Linear(256, reward_output_dim) \n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.Linear1(x.float())\n",
        "        #print('first linear x size',x.size())\n",
        "        x = self.BN1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Linear2(x)\n",
        "        #print('second linear x size',x.size())\n",
        "        x = self.BN2(x)\n",
        "        x = F.relu(x)\n",
        "        output = self.Linear3(x)\n",
        "        #print('third linear x size',output.size())        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR-jezFVFWm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_random_trajectories(num_traj, env_name,encoder):\n",
        "    '''\n",
        "    Run num_traj random trajectories to gather information about the next state and reward.\n",
        "    Data used to train the models in a supervised way.\n",
        "    '''\n",
        "\n",
        "    dataset_random = []\n",
        "    game_rewards = []\n",
        "    with torch.no_grad():\n",
        "        for n in range(num_traj):\n",
        "            print(n)        \n",
        "            env = make_vec_envs(ENV_NAME,1,workers,downsample=False) # Nawid- Makes several different vectorised environment\n",
        "            obs = env.reset()\n",
        "            #print('does observation work',obs.shape)\n",
        "        \n",
        "            state = encoder(obs.float().to(device) / 255)\n",
        "        \n",
        "            while True:\n",
        "                sampled_action = torch.tensor([env.action_space.sample() for i in range(workers)]).unsqueeze(dim=1) # Nawid - This needs to be used for the case where the environment is vectorised\n",
        "                new_obs, reward, done, _ = env.step(sampled_action)\n",
        "            \n",
        "                new_state = encoder(new_obs.float().to(device) / 255)\n",
        "                #print('new_state',new_state.size())\n",
        "                #print('reward', reward.size())\n",
        "                #print('sampled_action', sampled_action.size())\n",
        "                dataset_random.append([state.cpu(), new_state.cpu(), reward, done,sampled_action.float()]) # Nawid - Appends the state instead of the observation, changed action to float in order to concatenate with the observation tensor to use as training data, and need to move the state to cpu in order to change it to numpy\n",
        "                #print('dataset_random',dataset_random.shape) \n",
        "                #dataset_random.append([obs, new_obs, reward, done,sampled_action]) # Nawid - Appends the state instead of the observation\n",
        "\n",
        "                obs = new_obs\n",
        "                game_rewards.append(reward)\n",
        "\n",
        "                if done:\n",
        "                    env.close()\n",
        "                    break\n",
        "\n",
        "        # print some stats\n",
        "        print('Mean R:',np.round(np.sum(game_rewards)/num_traj,2), 'Max R:', np.round(np.max(game_rewards),2), np.round(len(game_rewards)/num_traj))\n",
        "\n",
        "    return dataset_random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM-D95cFl3J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_MSEloss(y_truth, y_pred, device):\n",
        "    '''\n",
        "    Compute the MSE (Mean Squared Error)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_7VEcz2mBJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_dyna_model(random_dataset, rl_dataset, env_model, rew_model, batch_size, max_model_iter, num_examples_added, ENV_LEARNING_RATE, REW_LEARNING_RATE, device):\n",
        "    '''\n",
        "    Train the two models that predict the next state and the expected reward\n",
        "    '''\n",
        "\n",
        "    env_optimizer = optim.Adam(env_model.parameters(), lr=ENV_LEARNING_RATE)\n",
        "    rew_optimizer = optim.Adam(rew_model.parameters(), lr=REW_LEARNING_RATE)\n",
        "\n",
        "    if len(rl_dataset) > 0:\n",
        "        '''\n",
        "        # To use only a fraction of the random dataset\n",
        "        rand = np.arange(len(random_dataset))\n",
        "        np.random.shuffle(rand)\n",
        "        rand = rand[:int(len(rl_dataset)*0.8)] # 80% of rl dataset\n",
        "        d_concat = np.concatenate([np.array(random_dataset)[rand], rl_dataset], axis=0)'''\n",
        "\n",
        "        # Concatenate the random dataset with the RL dataset. Used only in the aggregation iterations\n",
        "        d_concat = np.concatenate([random_dataset, rl_dataset], axis=0)\n",
        "    else:\n",
        "        d_concat = np.array(random_dataset)\n",
        "\n",
        "    # Split the dataset into train(80%) and test(20%)\n",
        "    D_train = d_concat[:int(-num_examples_added*1/5)]\n",
        "    D_valid = d_concat[int(-num_examples_added*1/5):]\n",
        "\n",
        "    print(\"len(D):\", len(d_concat), 'len(Dtrain)', len(D_train))\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    sff = np.arange(len(D_train))\n",
        "    np.random.shuffle(sff)\n",
        "    D_train = D_train[sff]\n",
        "\n",
        "    print('does it work at this point')\n",
        "    # Create the input and output for the train\n",
        "    #for obs,_,_,_,act in D_train:\n",
        "    #    concatenated_array =  np.concatenate([obs,act],axis = 1)\n",
        "    \n",
        "    #obs_train = D_train[:,0] \n",
        "    #n_obs_train = D_train[:,1]\n",
        "    #reward_train = D_train[:,2]\n",
        "    #act_train = D_train[:,3]\n",
        " \n",
        "  \n",
        "    X_train = torch.empty(size=(len(D_train),feature_size + 1)) # env.action_space.n))\n",
        "    y_env_train = torch.empty(size=(len(D_train),feature_size))\n",
        "    y_rew_train = torch.empty(size=(len(D_train),1))\n",
        "    \n",
        "\n",
        "    for i, (obs,n_obs,rew,_,act) in enumerate(D_train):\n",
        "        X_train[i] = torch.cat((obs,act),axis=1)\n",
        "        y_env_train[i] = n_obs - obs # Nawid - Change of state is the label for the model training # y(state) = s(t+1) - s(t)\n",
        "        y_rew_train[i] = rew\n",
        "\n",
        "    print('X_train', X_train.size())\n",
        "    print('y_env_train', y_env_train.size())\n",
        "    print('y_rew_train',y_rew_train.size())\n",
        "    \n",
        "    X_valid = torch.empty(size=(len(D_train),feature_size + 1)) # env.action_space.n))\n",
        "    y_env_valid = torch.empty(size=(len(D_train),feature_size))\n",
        "    y_rew_valid = torch.empty(size=(len(D_train),1))\n",
        "\n",
        "    for i, (obs,n_obs,rew,_,act) in enumerate(D_valid):\n",
        "        X_valid[i] = torch.cat((obs,act),axis=1)\n",
        "        y_env_valid[i] = n_obs - obs # Nawid - Change of state is the label for the model training # y(state) = s(t+1) - s(t)\n",
        "        y_rew_valid[i] = rew\n",
        "\n",
        "\n",
        "    # Standardize the input features by removing the mean and scaling to unit variance\n",
        "    input_scaler = StandardScaler()\n",
        "    X_train = input_scaler.fit_transform(X_train)\n",
        "    X_valid = input_scaler.transform(X_valid)\n",
        "\n",
        "    # Standardize the outputs by removing the mean and scaling to unit variance\n",
        "\n",
        "    env_output_scaler = StandardScaler()\n",
        "    y_env_train = env_output_scaler.fit_transform(y_env_train)\n",
        "    y_env_valid = env_output_scaler.transform(y_env_valid)\n",
        "\n",
        "    rew_output_scaler = StandardScaler()\n",
        "    y_rew_train = rew_output_scaler.fit_transform(y_rew_train)\n",
        "    y_rew_valid = rew_output_scaler.transform(y_rew_valid)\n",
        "    print('does it work after setting up the scaling')\n",
        "    # store all the scalers in a variable to later uses\n",
        "    norm = (input_scaler, env_output_scaler, rew_output_scaler)\n",
        "\n",
        "    losses_env = []\n",
        "    losses_rew = []\n",
        "\n",
        "    # go through max_model_iter supervised iterations\n",
        "    for it in tqdm(range(max_model_iter)):\n",
        "        # create mini batches of size batch_size\n",
        "        for mb in range(0, len(X_train), batch_size):\n",
        "            print('Does it work before making batches')\n",
        "            if len(X_train) > mb+BATCH_SIZE:\n",
        "                X_mb = X_train[mb:mb+BATCH_SIZE]\n",
        "                print('Does it work after making batches')\n",
        "                \n",
        "                y_env_mb = y_env_train[mb:mb+BATCH_SIZE]\n",
        "                y_rew_mb = y_rew_train[mb:mb+BATCH_SIZE]\n",
        "\n",
        "\n",
        "                print('X_mb size',X_mb.shape)\n",
        "                print('y_env_mb size',y_env_train.shape)\n",
        "                print('y_rew_mb size',y_rew_mb.shape)\n",
        "\n",
        "                # Add gaussian noise with mean 0 and variance 0.0001 as in the paper\n",
        "                X_mb += np.random.normal(loc=0, scale=0.001, size=X_mb.shape)\n",
        "\n",
        "                ## Optimization of the 'env_model' neural net\n",
        "\n",
        "                env_optimizer.zero_grad()\n",
        "                # forward pass of the model to compute the output\n",
        "                print('Does it work before prediction')\n",
        "                pred_state = env_model(torch.tensor(X_mb).to(device))\n",
        "                print('Does it work after prediction')\n",
        "                # compute the MSE loss\n",
        "                loss = model_MSEloss(y_env_mb, pred_state, device)\n",
        "                print('Does it work after loss')\n",
        "\n",
        "                if it == (max_model_iter - 1):\n",
        "                    losses_env.append(loss.cpu().detach().numpy())\n",
        "\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                # optimization step\n",
        "                env_optimizer.step()\n",
        "\n",
        "\n",
        "                ## Optimization of the 'rew_model' neural net\n",
        "                rew_optimizer.zero_grad()\n",
        "                # forward pass of the model to compute the output\n",
        "                print('Does it work before reward prediction')\n",
        "                pred_rew = rew_model(torch.tensor(X_mb).to(device))\n",
        "                print('Does it work after reward prediction')\n",
        "                # compute the MSE loss\n",
        "                loss = model_MSEloss(y_rew_mb, pred_rew, device)\n",
        "\n",
        "                if it == (max_model_iter - 1):\n",
        "                    losses_rew.append(loss.cpu().detach().numpy())\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                # optimization step\n",
        "                rew_optimizer.step()\n",
        "\n",
        "        # Evalute the models every 10 iterations and print the losses\n",
        "        if it % 10 == 0:\n",
        "          env_model.eval()\n",
        "          rew_model.eval()\n",
        "\n",
        "          pred_state = env_model(torch.tensor(X_valid).to(device))\n",
        "          pred_rew = rew_model(torch.tensor(X_valid).to(device))\n",
        "          env_model.train(True)\n",
        "          rew_model.train(True)\n",
        "\n",
        "          valid_env_loss = model_MSEloss(y_env_valid, pred_state, device)\n",
        "          valid_rew_loss = model_MSEloss(y_rew_valid, pred_rew, device)\n",
        "\n",
        "          print('..', it, valid_env_loss.cpu().detach().numpy(), valid_rew_loss.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "    ## Evaluate the MSE losses\n",
        "\n",
        "    env_model.eval()\n",
        "    rew_model.eval()\n",
        "\n",
        "    pred_state = env_model(torch.tensor(X_valid).to(device))\n",
        "    pred_rew = rew_model(torch.tensor(X_valid).to(device))\n",
        "    env_model.train(True)\n",
        "    rew_model.train(True)\n",
        "\n",
        "    valid_env_loss = model_MSEloss(y_env_valid, pred_state, device)\n",
        "    valid_rew_loss = model_MSEloss(y_rew_valid, pred_rew, device)\n",
        "\n",
        "    return np.mean(losses_env), np.mean(losses_rew), valid_env_loss.cpu().detach().numpy(), valid_rew_loss.cpu().detach().numpy(), norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEUgQwuWmMBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_model_based_control(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, norm, device):\n",
        "    '''\n",
        "    Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "    '''\n",
        "    best_reward = -1e9\n",
        "    best_next_action = []\n",
        "\n",
        "    input_scaler, env_output_scaler, rew_output_scaler = norm\n",
        "\n",
        "    m_obs = np.array([real_obs for _ in range(num_sequences)])\n",
        "\n",
        "    # array that contains the rewards for all the sequence\n",
        "    unroll_rewards = np.zeros((num_sequences, 1))\n",
        "    first_sampled_actions = []\n",
        "\n",
        "    env_model.eval()\n",
        "    rew_model.eval()\n",
        "\n",
        "    ## Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon_length' times.\n",
        "    ## i.e. roll a given number of trajectories in a single batch (to increase speed)\n",
        "\n",
        "    for t in range(horizon_length):\n",
        "      # sampled actions for each sequence\n",
        "      sampled_actions = [sample_action() for _ in range(num_sequences)]\n",
        "      # scale the input\n",
        "      sampled_actions = np.expand_dims(sampled_actions,axis=1)   \n",
        "      models_input = input_scaler.transform(np.concatenate([m_obs, sampled_actions], axis=1))\n",
        "      # compute the next state for each sequence\n",
        "      pred_obs = env_model(torch.tensor(models_input).to(device))\n",
        "      # and the reward\n",
        "      pred_rew = rew_model(torch.tensor(models_input).to(device))\n",
        "\n",
        "      # inverse scaler transofrmation\n",
        "      pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
        "      # and add previous observation\n",
        "      m_obs = pred_obs + m_obs\n",
        "\n",
        "      assert(pred_rew.cpu().detach().numpy().shape == unroll_rewards.shape)\n",
        "\n",
        "      # sum of the expected rewards\n",
        "      unroll_rewards += pred_rew.cpu().detach().numpy()\n",
        "\n",
        "      if t == 0:\n",
        "        first_sampled_actions = sampled_actions\n",
        "\n",
        "    env_model.train(True)\n",
        "    rew_model.train(True)\n",
        "\n",
        "    # Best the position of the sequence with the higher reward\n",
        "    arg_best_reward = np.argmax(unroll_rewards)\n",
        "    best_sum_reward = unroll_rewards[arg_best_reward].squeeze()\n",
        "    # take the first action of this sequence\n",
        "    best_action = first_sampled_actions[arg_best_reward].squeeze()\n",
        "\n",
        "    return best_action, best_sum_reward\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n-tAu7bmTpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'MsPacmanNoFrameskip-v4'\n",
        "\n",
        "feature_size = all_defaults['feature_size'] # Nawid- Dimensionality of the representation\n",
        "workers = 1 # Nawid - Choosing the number of workers for the network\n",
        "\n",
        "# Main loop hyperp\n",
        "AGGR_ITER = 3\n",
        "STEPS_PER_AGGR = 20000\n",
        "\n",
        "# Random MB Hyperp\n",
        "NUM_RAND_TRAJECTORIES = 2 #1000\n",
        "\n",
        "# cuda or cpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu' )\n",
        "\n",
        "# Supervised Model Hyperp\n",
        "ENV_LEARNING_RATE = 1e-3\n",
        "REW_LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 512\n",
        "TRAIN_ITER_MODEL = 55\n",
        "\n",
        "# Controller Hyper\n",
        "HORIZON_LENGTH = 10\n",
        "NUM_ACTIONS_SEQUENCES = 20000\n",
        "\n",
        "observation_channels = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OTGL9WVmb55",
        "colab_type": "code",
        "outputId": "96e0bc00-3c27-4641-a0ed-bb214caeb84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "def main():\n",
        "\n",
        "    torch.cuda.empty_cache() # Nawid - I am not sure if this helps - hopefully it does    \n",
        "    encoder = ImpalaCNN(observation_channels,all_defaults)\n",
        "    encoder.load_state_dict(torch.load('/content/MsPacmanNoFrameskip-v4_95.pt'))#,map_location=torch.device('cpu')))\n",
        "    for param in encoder.parameters():\n",
        "        param.requires_grad = False \n",
        "    #pytorch_total_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
        "    #print(pytorch_total_params)\n",
        "\n",
        "\n",
        "\n",
        "    encoder.eval()\n",
        "    \n",
        "    encoder.to(device)\n",
        "    #print('does it work here')\n",
        "    \n",
        "    # gather the dataset of random sequences\n",
        "    #rand_dataset = gather_random_trajectories(NUM_RAND_TRAJECTORIES, ENV_NAME)\n",
        "    rand_dataset = gather_random_trajectories(NUM_RAND_TRAJECTORIES, ENV_NAME, encoder)\n",
        "\n",
        "    rl_dataset = []\n",
        "    env = make_vec_envs(ENV_NAME,1,workers,downsample=False) # Nawid- Makes several different vectorised environment\n",
        "    \n",
        "    # Initialize the models\n",
        "    env_model = NNDynamicModel(env.action_space.n + feature_size, feature_size).to(device) # Nawid - Models need to be initialised with the represention size\n",
        "    #pytorch_total_params = sum(p.numel() for p in env_model.parameters() if p.requires_grad)\n",
        "    #print('env_model parameters',pytorch_total_params)\n",
        "\n",
        "\n",
        "    rew_model = NNRewardModel(env.action_space.n + feature_size, 1).to(device)\n",
        "    #reward_total_params = sum(p.numel() for p in rew_model.parameters() if p.requires_grad)\n",
        "    #print('reward_model parameters',reward_total_params)\n",
        "    \n",
        "    \n",
        "    game_reward = 0\n",
        "    num_examples_added = len(rand_dataset)\n",
        "\n",
        "    for n_iter in range(AGGR_ITER):\n",
        "\n",
        "        # supervised training of the dataset (random and rl if it exists)\n",
        "        train_env_loss, train_rew_loss, valid_env_loss, valid_rew_loss, norm = train_dyna_model(rand_dataset, rl_dataset, env_model, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL, num_examples_added, ENV_LEARNING_RATE, REW_LEARNING_RATE, device)\n",
        "        print('{} >> Eloss:{:.4f} EV loss:{:.4f} -- Rloss:{:.4f} RV loss:{:.4f}'.format(n_iter, train_env_loss, valid_env_loss, train_rew_loss, valid_rew_loss))\n",
        "\n",
        "        obs = env.reset()\n",
        "\n",
        "        num_examples_added = 0\n",
        "        game_reward = 0\n",
        "        game_pred_rews = []\n",
        "        rews = []\n",
        "\n",
        "        while num_examples_added < STEPS_PER_AGGR:\n",
        "            while True:\n",
        "\n",
        "                tt = time.time()\n",
        "                # Execute the control to roll the sequences and pick the first action of the sequence with the higher reward\n",
        "                action, pred_rew = multi_model_based_control(env_model, rew_model, obs, NUM_ACTIONS_SEQUENCES, HORIZON_LENGTH, env.action_space.sample, norm, device)\n",
        "                game_pred_rews.append(pred_rew)\n",
        "\n",
        "                # one step in the environment with the action returned by the controller\n",
        "                new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "                input_scaler, env_output_scaler, rew_output_scaler = norm\n",
        "\n",
        "                ## Compute the reward and print some stats\n",
        "                action = np.expand_dims(action,axis=0) # Nawid-  Need to use this to make the dimensions the same during the concatenation   \n",
        "                models_input = input_scaler.transform([np.concatenate([obs, action])])\n",
        "                rew_model.eval()\n",
        "                p_rew = rew_model(torch.tensor(models_input).to(device))\n",
        "                rew_model.train(True)\n",
        "                unnorm_rew = rew_output_scaler.inverse_transform([float(p_rew.cpu().data[0])]).squeeze()\n",
        "                print('  >> ',len(game_pred_rews), 'gt:',np.round(reward,3), 'pred:',np.round(unnorm_rew, 3),\n",
        "                      'sum:', np.round(pred_rew,3), '|', game_reward, np.round(time.time()-tt, 4), HORIZON_LENGTH)\n",
        "\n",
        "                # add the last step to the RL dataset\n",
        "                rl_dataset.append([obs, new_obs, reward, done, action])\n",
        "\n",
        "\n",
        "                num_examples_added += 1\n",
        "                obs = new_obs\n",
        "                game_reward += reward\n",
        "\n",
        "                # if the environment is done, reset it and print some stats\n",
        "                if done:\n",
        "                    obs = env.reset()\n",
        "                    print('  >> R: {:.2f}, Mean sum:{:.2f}, {}'.format(game_reward, np.mean(game_pred_rews), num_examples_added))\n",
        "\n",
        "                    rews.append(game_reward)\n",
        "                    game_reward = 0\n",
        "                    game_pred_rews = []\n",
        "                    break\n",
        "\n",
        "        print('  >> Mean: {:.2f}', np.mean(rews))\n",
        "\n",
        "main()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "Mean R: 11.0 Max R: 1.0 194.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "len(D): 388 len(Dtrain) 311\n",
            "does it work at this point\n",
            "X_train torch.Size([311, 257])\n",
            "y_env_train torch.Size([311, 256])\n",
            "y_rew_train torch.Size([311, 1])\n",
            "does it work after setting up the scaling\n",
            "Does it work before making batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-0d305aff6e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  >> Mean: {:.2f}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-0d305aff6e74>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# supervised training of the dataset (random and rl if it exists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtrain_env_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rew_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_env_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_rew_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dyna_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_ITER_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples_added\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENV_LEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREW_LEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} >> Eloss:{:.4f} EV loss:{:.4f} -- Rloss:{:.4f} RV loss:{:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_env_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_env_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rew_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_rew_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-567efe96aab3>\u001b[0m in \u001b[0;36mtrain_dyna_model\u001b[0;34m(random_dataset, rl_dataset, env_model, rew_model, batch_size, max_model_iter, num_examples_added, ENV_LEARNING_RATE, REW_LEARNING_RATE, device)\u001b[0m\n\u001b[1;32m    149\u001b[0m           \u001b[0mrew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m           \u001b[0mpred_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m           \u001b[0mpred_rew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrew_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m           \u001b[0menv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e63b3a22955d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [311 x 257], m2: [265 x 512] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLF9YU3yHXrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}