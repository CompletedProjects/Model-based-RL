{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_based_Pacman_Single_Agent_LSTM_030520.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dChd_BVNUKHR",
        "_eIBf1pknbf_"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1S07wznL92WKuf5URDkiOB39R-bYRm5VM",
      "authorship_tag": "ABX9TyNnVLFajA7mAizJjmLpSljw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/Model_based_Pacman_Single_Agent_LSTM_030520.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4jLAGgyjflE",
        "colab_type": "text"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hdnkq3rcJNF",
        "colab_type": "text"
      },
      "source": [
        "Used to save Pacman video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp0jzlXIcFfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fss3NXoMcIjw",
        "colab_type": "code",
        "outputId": "c69b1252-f471-4a1f-9068-7784dc3a86d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!apt-get install ffmpeg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (46.1.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 108 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "02519cdb-7f46-4699-9ae3-ea5ab4420e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-b9pphlq3\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-b9pphlq3\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.18.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=ce0c74d77add69a6a6e9153d6915d8a7f2ebd63742f8615272f484083c3d1cee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c7ojy4y6/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-pb_5o95h\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-pb_5o95h\n",
            "Collecting gym<0.16.0,>=0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/01/8771e8f914a627022296dab694092a11a7d417b6c8364f0a44a8debca734/gym-0.15.7.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.38.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines, gym\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=c4cf5d5ac8f856c36b6382639d682092ece7cfbf69476956280b864aa3629b5f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-srp7sv0f/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-cp36-none-any.whl size=1648840 sha256=cba79e6ab76503f9e1bd07489a892eb63a2080bbf104063b04597797cc9cccea\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/60/6a/f9c27ae133abaf5a5687ed2fa8ed19627d7fac5d843a27572b\n",
            "Successfully built baselines gym\n",
            "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gym, baselines\n",
            "  Found existing installation: gym 0.17.1\n",
            "    Uninstalling gym-0.17.1:\n",
            "      Successfully uninstalled gym-0.17.1\n",
            "Successfully installed baselines-0.1.6 gym-0.15.7\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.4MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/1a/0df85d2bddbca33665d2148173d3281b290ac054b5f50163ea735740ac7b/GitPython-3.1.1-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 35.3MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 44.8MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/52/ca35448b56c53a079d3ffe18b1978c6e424f6d4df02404877094c89f5bfb/gitdb-4.0.4-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hCollecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/27/b1/e379cfb7c07bbf8faee29c4a1a2469dbea525f047c2b454c4afdefa20a30/smmap-3.0.2-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=8396a2c5e705d794b590aa91ed72ab2af8ea002ab8bb073d4508f6ecbf6bc7a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=c468688e8f19f667ac5b39f138a1134de7accdbb0f5d90512ac5bdd54afd9ff2\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=72c0702cb8ec8b20c6d6edc3d5f148af9ab6d8ba5770beffea02222c3f5d2df9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=b05db7cca40e9ac923c99c007119286f829013eeded330c0524130f6096b0a17\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=48bd12b301c1dcbde23d2ced5004c7071913e5a2dc8274de3f420e715a790dad\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog subprocess32 gql pathtools graphql-core\n",
            "Installing collected packages: smmap, gitdb, GitPython, configparser, shortuuid, pathtools, watchdog, sentry-sdk, docker-pycreds, subprocess32, graphql-core, gql, wandb\n",
            "Successfully installed GitPython-3.1.1 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.4 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.2 subprocess32-3.5.4 wandb-0.8.35 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZ9gMrxp5vU",
        "colab_type": "code",
        "outputId": "86d464ad-304e-4b27-930c-af03c0580326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI287_Yjxn3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from benchmark.envs import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from benchmark.wrapper import AtariARIWrapper\n",
        "\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from custom.EarlyStopping import EarlyStopping_loss\n",
        "from custom.model import *\n",
        "from custom.custom_wrappers import DeterminsticNoopResetEnv \n",
        "# Imported required for the Model-based RL\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "from baselines.common.atari_wrappers import EpisodicLifeEnv\n",
        "from torch.nn.utils import clip_grad_norm_\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "a787fc5f-8435-4c4e-87d2-1c87157523b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login ######"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRn5uXDhgmQs",
        "colab_type": "text"
      },
      "source": [
        "# General Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2y__x5ysYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class General_functions():\n",
        "    def __init__(self, ENV_NAME,n_actions,possible_positions):\n",
        "        self.ENV_NAME = ENV_NAME\n",
        "        self.env = gym.make(self.ENV_NAME)\n",
        "        self.env = EpisodicLifeEnv(DeterminsticNoopResetEnv(self.env,66))        \n",
        "        self.env = AtariARIWrapper(self.env)\n",
        "        self.initial_info_labels = self.env.labels()\n",
        "    \n",
        "        self.prev_action_counter =  False\n",
        "        self.repeated_end = False # Nawid -  Sets self.repeated end as false initially\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.n_actions = n_actions\n",
        "        # possible positions is numpy array with most of the possible positions that the agent can go to\n",
        "        self.possible_positions = possible_positions\n",
        "        self.possible_positions_list = self.possible_positions.tolist()\n",
        "        \n",
        "        self.key_list = ['player_x', 'player_y']\n",
        "\n",
        "\n",
        "    def one_hot(self,i):\n",
        "        a = np.zeros(self.n_actions, 'uint8')\n",
        "        a[i-1] = 1\n",
        "        return a\n",
        "\n",
        "    def state_conversion(self,obs,info_labels):\n",
        "        state = [info_labels[word] for word in self.key_list if word in info_labels]                \n",
        "        state = np.array(state)\n",
        "        return state\n",
        "        \n",
        "    def next_position(self,state, action):\n",
        "        next_position = state.copy()\n",
        "        if action == 1:\n",
        "            next_position[1] = next_position[1] - 2 \n",
        "        elif action == 2:\n",
        "            next_position[0] = next_position[0] + 2\n",
        "        elif action == 3:\n",
        "            next_position[0] = next_position[0] - 2\n",
        "        elif action == 4:\n",
        "            next_position[1] = next_position[1] + 2\n",
        "\n",
        "        if next_position.tolist() in self.possible_positions_list: # possible positions will be a list which is fed into the network \n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "            \n",
        "    def random_action_selection(self, state,prev_action = None):\n",
        "        while True:\n",
        "            action = np.random.randint(1,5) \n",
        "            feasible_action = self.next_position(state,action)\n",
        "            if feasible_action:\n",
        "                self.prev_action_counter = True\n",
        "                #print('action:',action)\n",
        "                return action\n",
        "            else:\n",
        "                if self.prev_action_counter and prev_action !=None:\n",
        "                    #print('repeated action:',prev_action)\n",
        "                    return prev_action\n",
        "    \n",
        "\n",
        "    \n",
        "    def check_all_agents(self,info_label,next_info_label):\n",
        "        repeated = np.equal(info_label,next_info_label).all()\n",
        "        if repeated:\n",
        "            self.repeated_end= True\n",
        "\n",
        "    def check_state(self,state, next_state):\n",
        "        repeated_state = np.equal(state, next_state).all()\n",
        "        if repeated_state:\n",
        "            self.prev_action_counter = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJgI07xT7eo",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWTGqLduK7au",
        "colab_type": "code",
        "outputId": "5d6bb7c5-4333-460f-98b4-2dd2cfa9dc06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class Data_collection(General_functions):\n",
        "    def __init__(self,ENV_NAME,n_actions, possible_positions):\n",
        "        super(Data_collection,self).__init__(ENV_NAME,n_actions,possible_positions)\n",
        "        self.dataset_random = [] # Where the data is saved each time\n",
        "\n",
        "    def collate_data(self,random_dataset, rl_dataset):\n",
        "        rand_data = np.array(random_dataset)\n",
        "        num_rand_examples = len(rand_data)\n",
        "        D_train = rand_data[:int(-num_rand_examples*1/5)] \n",
        "        D_valid = rand_data[int(-num_rand_examples*1/5):]\n",
        "        print(\"number random examples:\",num_rand_examples, 'len(D_train_rand)', len(D_train),'len(D_valid_rand)', len(D_valid))\n",
        "        if len(rl_dataset) > 0:\n",
        "            # Adds the rl dataset to the random dataset if there is any present\n",
        "            rl_data = np.array(rl_dataset)\n",
        "            num_rl_examples = len(rl_data)\n",
        "            D_rl_train = rl_data[:int(-num_rl_examples*1/5)] \n",
        "            D_rl_valid = rl_data[int(-num_rl_examples*1/5):]\n",
        "                        \n",
        "            D_train = np.concatenate([D_train, D_rl_train], axis = 0)\n",
        "            D_valid = np.concatenate([D_valid, D_rl_valid], axis = 0)\n",
        "            print(\"number rl examples:\",num_rl_examples, 'len(D_rl_train)', len(D_rl_train),'len(D_valid_rand)', len(D_rl_valid))\n",
        "            \n",
        "        #print(\"len(D_train):\", len(D_train), 'len(D_valid)', len(D_valid))\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        '''        \n",
        "        sff = np.arange(len(D_train))\n",
        "        np.random.shuffle(sff)\n",
        "        D_train = D_train[sff]\n",
        "        '''\n",
        "        #print('D_train shape',D_train.shape)\n",
        "\n",
        "        # Create the input and output for the train\n",
        "        X_train_obs = np.array([obs for obs,_,_,_,_ in D_train]) # Takes obs and action\n",
        "        X_train_obs = X_train_obs.astype(np.float32)\n",
        "        #X_train_obs = X_train_obs.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        X_train_act = np.array([act for _,_,_,_,act in D_train])\n",
        "        \n",
        "        # Env output\n",
        "        y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "        y_env_train = y_env_train.astype(np.float32)\n",
        "        #y_env_train = y_env_train.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        y_env_train = y_env_train - X_train_obs \n",
        "        \n",
        "        # Next state output\n",
        "        X_val_obs = np.array([obs for obs,_,_,_,_ in D_valid]) # Takes obs and action\n",
        "        X_val_obs = X_val_obs.astype(np.float32)\n",
        "        #X_val_obs = X_val_obs.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)        \n",
        "        X_val_act = np.array([act for _,_,_,_,act in D_valid])\n",
        "\n",
        "        y_env_val = np.array([no for _,no,_,_,_ in D_valid])\n",
        "        y_env_val = y_env_val.astype(np.float32)\n",
        "        #y_env_val = y_env_val.astype(np.int16)\n",
        "        y_env_val = y_env_val - X_val_obs \n",
        "\n",
        "        env_train_data, env_val_data = (X_train_obs, X_train_act, y_env_train), (X_val_obs, X_val_act, y_env_val)\n",
        "        return env_train_data, env_val_data \n",
        "\n",
        "    def setup_dataset(self,env_train_data, env_val_data):\n",
        "        # Unpack data\n",
        "        (X_env_train_obs, X_env_train_act, y_env_train), (X_env_val_obs, X_env_val_act, y_env_val) = env_train_data, env_val_data\n",
        "    \n",
        "        # Concatentates the normalised states with the one hot vector for the actions\n",
        "        X_env_train = np.concatenate((X_env_train_obs,X_env_train_act),axis=1)\n",
        "        X_env_val = np.concatenate((X_env_val_obs,X_env_val_act),axis=1)\n",
        "\n",
        "        # Pack data tuples\n",
        "        env_train_data, env_val_data = (X_env_train, y_env_train),(X_env_val, y_env_val) \n",
        "        return env_train_data, env_val_data\n",
        "    \n",
        "    \n",
        "    def gather_random_trajectories(self,num_traj):\n",
        "        #dataset_random = []\n",
        "        for n in range(num_traj):\n",
        "            if n % 10 ==0:\n",
        "                print('trajectory number:',n)\n",
        "                # Initial set up\n",
        "            #self.env.seed(0)\n",
        "            self.env = gym.make(self.ENV_NAME)\n",
        "            self.env = EpisodicLifeEnv(DeterminsticNoopResetEnv(self.env,noop_max=66))\n",
        "            self.env = AtariARIWrapper(self.env)\n",
        "            obs = self.env.reset()\n",
        "            \n",
        "            self.repeated_end = False\n",
        "            info_labels = self.env.labels() # Nawid - Used to get the current state\n",
        "            state = self.state_conversion(obs, info_labels) # Used to get the initial state\n",
        "            prev_action = None # Initialise prev action has having no action\n",
        "\n",
        "            while True:\n",
        "                sampled_action = self.random_action_selection(state,prev_action)\n",
        "                #sampled_action = np.random.randint(1,5)\n",
        "                 \n",
        "                sampled_action_one_hot = self.one_hot(sampled_action)\n",
        "                next_obs, reward, done, next_info = self.env.step(sampled_action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                \n",
        "                next_state = self.state_conversion(next_obs, next_info_labels)\n",
        "                self.check_state(state,next_state)\n",
        "                self.check_all_agents(info_labels, next_info_labels) # need to use the info labels to predict the state as the info labels have all the informaiton\n",
        "                    \n",
        "                if not self.repeated_end:\n",
        "                    self.dataset_random.append([state, next_state, reward, done,sampled_action_one_hot])\n",
        "\n",
        "                else:\n",
        "                    self.dataset_random[-1][-2] = True # sets the previous value to done\n",
        "                    done = True   \n",
        "                    \n",
        "                state = next_state.copy()\n",
        "                info_labels = next_info_labels.copy()\n",
        "                prev_action = sampled_action\n",
        "                if done:\n",
        "                    break    \n",
        "        \n",
        "        return self.dataset_random\n",
        "\n",
        "'''\n",
        "ENV_NAME = 'MsPacman-ramDeterministic-v4'\n",
        "n_actions = 4 #9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "\n",
        "NUM_RAND_TRAJECTORIES = 3000\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}_{}-{}_{}\".format(now.day,now.month,now.year, now.hour, now.minute)\n",
        "possible_positions = np.load('/content/drive/My Drive/MsPacman-data/possible_pacman_positions.npy',allow_pickle=True)\n",
        "data_collector = Data_collection(ENV_NAME,n_actions,possible_positions)\n",
        "\n",
        "rand_dataset = data_collector.gather_random_trajectories(NUM_RAND_TRAJECTORIES)\n",
        "filename = '/content/drive/My Drive/MsPacman-data/rand_traj_{}-'.format(NUM_RAND_TRAJECTORIES) + date_time\n",
        "np.save(filename,rand_dataset)\n",
        "\n",
        "rl_dataset = []\n",
        "env_train, env_val = data_object.collate_data(data1,rl_dataset)\n",
        "full_env_train, full_env_val = data_object.setup_dataset(env_train,env_val)\n",
        "'''"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trajectory number: 0\n",
            "trajectory number: 10\n",
            "trajectory number: 20\n",
            "trajectory number: 30\n",
            "trajectory number: 40\n",
            "trajectory number: 50\n",
            "trajectory number: 60\n",
            "trajectory number: 70\n",
            "trajectory number: 80\n",
            "trajectory number: 90\n",
            "trajectory number: 100\n",
            "trajectory number: 110\n",
            "trajectory number: 120\n",
            "trajectory number: 130\n",
            "trajectory number: 140\n",
            "trajectory number: 150\n",
            "trajectory number: 160\n",
            "trajectory number: 170\n",
            "trajectory number: 180\n",
            "trajectory number: 190\n",
            "trajectory number: 200\n",
            "trajectory number: 210\n",
            "trajectory number: 220\n",
            "trajectory number: 230\n",
            "trajectory number: 240\n",
            "trajectory number: 250\n",
            "trajectory number: 260\n",
            "trajectory number: 270\n",
            "trajectory number: 280\n",
            "trajectory number: 290\n",
            "trajectory number: 300\n",
            "trajectory number: 310\n",
            "trajectory number: 320\n",
            "trajectory number: 330\n",
            "trajectory number: 340\n",
            "trajectory number: 350\n",
            "trajectory number: 360\n",
            "trajectory number: 370\n",
            "trajectory number: 380\n",
            "trajectory number: 390\n",
            "trajectory number: 400\n",
            "trajectory number: 410\n",
            "trajectory number: 420\n",
            "trajectory number: 430\n",
            "trajectory number: 440\n",
            "trajectory number: 450\n",
            "trajectory number: 460\n",
            "trajectory number: 470\n",
            "trajectory number: 480\n",
            "trajectory number: 490\n",
            "trajectory number: 500\n",
            "trajectory number: 510\n",
            "trajectory number: 520\n",
            "trajectory number: 530\n",
            "trajectory number: 540\n",
            "trajectory number: 550\n",
            "trajectory number: 560\n",
            "trajectory number: 570\n",
            "trajectory number: 580\n",
            "trajectory number: 590\n",
            "trajectory number: 600\n",
            "trajectory number: 610\n",
            "trajectory number: 620\n",
            "trajectory number: 630\n",
            "trajectory number: 640\n",
            "trajectory number: 650\n",
            "trajectory number: 660\n",
            "trajectory number: 670\n",
            "trajectory number: 680\n",
            "trajectory number: 690\n",
            "trajectory number: 700\n",
            "trajectory number: 710\n",
            "trajectory number: 720\n",
            "trajectory number: 730\n",
            "trajectory number: 740\n",
            "trajectory number: 750\n",
            "trajectory number: 760\n",
            "trajectory number: 770\n",
            "trajectory number: 780\n",
            "trajectory number: 790\n",
            "trajectory number: 800\n",
            "trajectory number: 810\n",
            "trajectory number: 820\n",
            "trajectory number: 830\n",
            "trajectory number: 840\n",
            "trajectory number: 850\n",
            "trajectory number: 860\n",
            "trajectory number: 870\n",
            "trajectory number: 880\n",
            "trajectory number: 890\n",
            "trajectory number: 900\n",
            "trajectory number: 910\n",
            "trajectory number: 920\n",
            "trajectory number: 930\n",
            "trajectory number: 940\n",
            "trajectory number: 950\n",
            "trajectory number: 960\n",
            "trajectory number: 970\n",
            "trajectory number: 980\n",
            "trajectory number: 990\n",
            "trajectory number: 1000\n",
            "trajectory number: 1010\n",
            "trajectory number: 1020\n",
            "trajectory number: 1030\n",
            "trajectory number: 1040\n",
            "trajectory number: 1050\n",
            "trajectory number: 1060\n",
            "trajectory number: 1070\n",
            "trajectory number: 1080\n",
            "trajectory number: 1090\n",
            "trajectory number: 1100\n",
            "trajectory number: 1110\n",
            "trajectory number: 1120\n",
            "trajectory number: 1130\n",
            "trajectory number: 1140\n",
            "trajectory number: 1150\n",
            "trajectory number: 1160\n",
            "trajectory number: 1170\n",
            "trajectory number: 1180\n",
            "trajectory number: 1190\n",
            "trajectory number: 1200\n",
            "trajectory number: 1210\n",
            "trajectory number: 1220\n",
            "trajectory number: 1230\n",
            "trajectory number: 1240\n",
            "trajectory number: 1250\n",
            "trajectory number: 1260\n",
            "trajectory number: 1270\n",
            "trajectory number: 1280\n",
            "trajectory number: 1290\n",
            "trajectory number: 1300\n",
            "trajectory number: 1310\n",
            "trajectory number: 1320\n",
            "trajectory number: 1330\n",
            "trajectory number: 1340\n",
            "trajectory number: 1350\n",
            "trajectory number: 1360\n",
            "trajectory number: 1370\n",
            "trajectory number: 1380\n",
            "trajectory number: 1390\n",
            "trajectory number: 1400\n",
            "trajectory number: 1410\n",
            "trajectory number: 1420\n",
            "trajectory number: 1430\n",
            "trajectory number: 1440\n",
            "trajectory number: 1450\n",
            "trajectory number: 1460\n",
            "trajectory number: 1470\n",
            "trajectory number: 1480\n",
            "trajectory number: 1490\n",
            "trajectory number: 1500\n",
            "trajectory number: 1510\n",
            "trajectory number: 1520\n",
            "trajectory number: 1530\n",
            "trajectory number: 1540\n",
            "trajectory number: 1550\n",
            "trajectory number: 1560\n",
            "trajectory number: 1570\n",
            "trajectory number: 1580\n",
            "trajectory number: 1590\n",
            "trajectory number: 1600\n",
            "trajectory number: 1610\n",
            "trajectory number: 1620\n",
            "trajectory number: 1630\n",
            "trajectory number: 1640\n",
            "trajectory number: 1650\n",
            "trajectory number: 1660\n",
            "trajectory number: 1670\n",
            "trajectory number: 1680\n",
            "trajectory number: 1690\n",
            "trajectory number: 1700\n",
            "trajectory number: 1710\n",
            "trajectory number: 1720\n",
            "trajectory number: 1730\n",
            "trajectory number: 1740\n",
            "trajectory number: 1750\n",
            "trajectory number: 1760\n",
            "trajectory number: 1770\n",
            "trajectory number: 1780\n",
            "trajectory number: 1790\n",
            "trajectory number: 1800\n",
            "trajectory number: 1810\n",
            "trajectory number: 1820\n",
            "trajectory number: 1830\n",
            "trajectory number: 1840\n",
            "trajectory number: 1850\n",
            "trajectory number: 1860\n",
            "trajectory number: 1870\n",
            "trajectory number: 1880\n",
            "trajectory number: 1890\n",
            "trajectory number: 1900\n",
            "trajectory number: 1910\n",
            "trajectory number: 1920\n",
            "trajectory number: 1930\n",
            "trajectory number: 1940\n",
            "trajectory number: 1950\n",
            "trajectory number: 1960\n",
            "trajectory number: 1970\n",
            "trajectory number: 1980\n",
            "trajectory number: 1990\n",
            "trajectory number: 2000\n",
            "trajectory number: 2010\n",
            "trajectory number: 2020\n",
            "trajectory number: 2030\n",
            "trajectory number: 2040\n",
            "trajectory number: 2050\n",
            "trajectory number: 2060\n",
            "trajectory number: 2070\n",
            "trajectory number: 2080\n",
            "trajectory number: 2090\n",
            "trajectory number: 2100\n",
            "trajectory number: 2110\n",
            "trajectory number: 2120\n",
            "trajectory number: 2130\n",
            "trajectory number: 2140\n",
            "trajectory number: 2150\n",
            "trajectory number: 2160\n",
            "trajectory number: 2170\n",
            "trajectory number: 2180\n",
            "trajectory number: 2190\n",
            "trajectory number: 2200\n",
            "trajectory number: 2210\n",
            "trajectory number: 2220\n",
            "trajectory number: 2230\n",
            "trajectory number: 2240\n",
            "trajectory number: 2250\n",
            "trajectory number: 2260\n",
            "trajectory number: 2270\n",
            "trajectory number: 2280\n",
            "trajectory number: 2290\n",
            "trajectory number: 2300\n",
            "trajectory number: 2310\n",
            "trajectory number: 2320\n",
            "trajectory number: 2330\n",
            "trajectory number: 2340\n",
            "trajectory number: 2350\n",
            "trajectory number: 2360\n",
            "trajectory number: 2370\n",
            "trajectory number: 2380\n",
            "trajectory number: 2390\n",
            "trajectory number: 2400\n",
            "trajectory number: 2410\n",
            "trajectory number: 2420\n",
            "trajectory number: 2430\n",
            "trajectory number: 2440\n",
            "trajectory number: 2450\n",
            "trajectory number: 2460\n",
            "trajectory number: 2470\n",
            "trajectory number: 2480\n",
            "trajectory number: 2490\n",
            "trajectory number: 2500\n",
            "trajectory number: 2510\n",
            "trajectory number: 2520\n",
            "trajectory number: 2530\n",
            "trajectory number: 2540\n",
            "trajectory number: 2550\n",
            "trajectory number: 2560\n",
            "trajectory number: 2570\n",
            "trajectory number: 2580\n",
            "trajectory number: 2590\n",
            "trajectory number: 2600\n",
            "trajectory number: 2610\n",
            "trajectory number: 2620\n",
            "trajectory number: 2630\n",
            "trajectory number: 2640\n",
            "trajectory number: 2650\n",
            "trajectory number: 2660\n",
            "trajectory number: 2670\n",
            "trajectory number: 2680\n",
            "trajectory number: 2690\n",
            "trajectory number: 2700\n",
            "trajectory number: 2710\n",
            "trajectory number: 2720\n",
            "trajectory number: 2730\n",
            "trajectory number: 2740\n",
            "trajectory number: 2750\n",
            "trajectory number: 2760\n",
            "trajectory number: 2770\n",
            "trajectory number: 2780\n",
            "trajectory number: 2790\n",
            "trajectory number: 2800\n",
            "trajectory number: 2810\n",
            "trajectory number: 2820\n",
            "trajectory number: 2830\n",
            "trajectory number: 2840\n",
            "trajectory number: 2850\n",
            "trajectory number: 2860\n",
            "trajectory number: 2870\n",
            "trajectory number: 2880\n",
            "trajectory number: 2890\n",
            "trajectory number: 2900\n",
            "trajectory number: 2910\n",
            "trajectory number: 2920\n",
            "trajectory number: 2930\n",
            "trajectory number: 2940\n",
            "trajectory number: 2950\n",
            "trajectory number: 2960\n",
            "trajectory number: 2970\n",
            "trajectory number: 2980\n",
            "trajectory number: 2990\n",
            "number random examples: 809 len(D_train_rand) 648 len(D_valid_rand) 161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PdceXaoEkts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class state_predictor(nn.Module):\n",
        "  def __init__(self,input_size, hidden_state_size,output_size):\n",
        "    super(state_predictor, self).__init__()\n",
        "    \n",
        "    self.lstm=nn.LSTM(input_size,hidden_state_size)\n",
        "    self.linear=nn.Linear(hidden_state_size, output_size)\n",
        "\n",
        "  def forward(self,x,h):\n",
        "    # h: hidden_state, c=output\n",
        "    # x= x.view(batch_size,timesteps,embed_size)\n",
        "    #print(h[0].shape)\n",
        "    lstm_out,(h,c)=self.lstm(x,h)\n",
        "    #print(out.size())\n",
        "    #(batch_size*timesteps, hidden_size)\n",
        "    #out.size(0):batch_size; out.size(1):timesteps, out.size(2): hidden_size\n",
        "    fc_input=lstm_out.reshape(lstm_out.size(0)*lstm_out.size(1),lstm_out.size(2)) # flattens the matrix\n",
        "    out = self.linear(fc_input)\n",
        "    out = out.reshape(lstm_out.size(0), lstm_out.size(1), out.size(1))\n",
        "    return out, (h,c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk1IqBFLFOYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 1 \n",
        "num_directions = 1\n",
        "\n",
        "input_dim = 2 + n_actions\n",
        "timesteps = 5\n",
        "batch_size = 10\n",
        "hidden_dim = 1024\n",
        "output_dim = 2\n",
        "learning_rate = 0.002\n",
        "model = state_predictor(input_dim,hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.MSELoss()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua_L4DiaFKx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "3b2d30a5-8079-42ae-c692-19b4ac89910f"
      },
      "source": [
        "x_train, y_train = full_env_train\n",
        "print(x_train.dtype)\n",
        "for it in range(20):\n",
        "    for i in range(0,len(x_train), timesteps*batch_size):\n",
        "        if len(x_train) > i + (timesteps*batch_size):\n",
        "            x = x_train[i:i+(timesteps*batch_size),:]\n",
        "            y = y_train[i:i+(timesteps*batch_size),:]\n",
        "    \n",
        "            inputs = torch.tensor(x)\n",
        "            inputs = inputs.view(timesteps,batch_size,-1)\n",
        "    \n",
        "            targets = torch.tensor(y)\n",
        "            targets = targets.view(timesteps,batch_size,-1)\n",
        "    \n",
        "            states = (torch.zeros(num_directions*num_layers,batch_size,hidden_dim),\n",
        "                    torch.zeros(num_directions*num_layers,batch_size,hidden_dim))\n",
        "\n",
        "            outputs,_ = model(inputs, states)\n",
        "            #print('outputs:',outputs.size())\n",
        "            #print('targets:',targets.size())\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(),0.1)\n",
        "            optimizer.step()\n",
        "        \n",
        "    print(\"Epoch [{}], Loss: {:.4f}\".format(it, loss.item()))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float32\n",
            "Epoch [0], Loss: 1.2235\n",
            "Epoch [1], Loss: 0.9159\n",
            "Epoch [2], Loss: 2.0151\n",
            "Epoch [3], Loss: 1.1083\n",
            "Epoch [4], Loss: 0.8947\n",
            "Epoch [5], Loss: 0.7962\n",
            "Epoch [6], Loss: 0.6352\n",
            "Epoch [7], Loss: 0.8056\n",
            "Epoch [8], Loss: 0.6347\n",
            "Epoch [9], Loss: 0.7791\n",
            "Epoch [10], Loss: 0.4550\n",
            "Epoch [11], Loss: 0.5723\n",
            "Epoch [12], Loss: 0.3145\n",
            "Epoch [13], Loss: 0.4113\n",
            "Epoch [14], Loss: 0.3419\n",
            "Epoch [15], Loss: 0.7783\n",
            "Epoch [16], Loss: 0.6551\n",
            "Epoch [17], Loss: 0.3729\n",
            "Epoch [18], Loss: 0.3188\n",
            "Epoch [19], Loss: 0.4349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDqyI38UNyCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "b40830f9-4ed1-41f8-ea80-520a93311757"
      },
      "source": [
        "batch_size = 3\n",
        "test_inputs = torch.tensor(x_train[0:timesteps*batch_size,:])\n",
        "test_inputs = test_inputs.view(timesteps,batch_size,-1)\n",
        "print(test_inputs.size())\n",
        "\n",
        "targets = torch.tensor(y_train[0:timesteps*batch_size,:])\n",
        "targets = targets.view(timesteps,batch_size,-1)\n",
        "print(targets.size())\n",
        "    \n",
        "new_states = (torch.zeros(num_layers*num_directions,batch_size,hidden_dim),\n",
        "          torch.zeros(num_directions*num_layers,batch_size,hidden_dim))\n",
        "\n",
        "outputs,_ = model(test_inputs, new_states)\n",
        "print('outputs:',outputs)\n",
        "print('targets:',targets)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 6])\n",
            "torch.Size([5, 3, 2])\n",
            "outputs: tensor([[[ 3.3684e+00,  1.2121e-02],\n",
            "         [ 3.3787e+00,  9.9735e-04],\n",
            "         [-2.2580e+00,  4.9191e-02]],\n",
            "\n",
            "        [[-2.3351e+00,  1.1404e-01],\n",
            "         [-2.3247e+00,  1.0857e-01],\n",
            "         [ 3.3961e+00,  1.0153e-01]],\n",
            "\n",
            "        [[ 3.5075e+00,  2.2878e-02],\n",
            "         [ 3.4864e+00,  2.0179e-02],\n",
            "         [-2.2165e+00,  7.6310e-02]],\n",
            "\n",
            "        [[-2.3254e+00,  1.0348e-01],\n",
            "         [ 3.3554e+00, -1.4667e-02],\n",
            "         [ 3.4862e+00,  8.1130e-02]],\n",
            "\n",
            "        [[ 3.4652e+00,  1.0515e-02],\n",
            "         [ 3.4121e+00, -1.9552e-02],\n",
            "         [-2.3378e+00, -4.0889e-02]]], grad_fn=<ViewBackward>)\n",
            "targets: tensor([[[ 3.,  0.],\n",
            "         [ 3.,  0.],\n",
            "         [-2.,  0.]],\n",
            "\n",
            "        [[-2.,  0.],\n",
            "         [-3.,  0.],\n",
            "         [ 3.,  0.]],\n",
            "\n",
            "        [[ 3.,  0.],\n",
            "         [ 3.,  0.],\n",
            "         [-3.,  0.]],\n",
            "\n",
            "        [[-3.,  0.],\n",
            "         [ 3.,  0.],\n",
            "         [ 3.,  0.]],\n",
            "\n",
            "        [[ 3.,  0.],\n",
            "         [ 3.,  0.],\n",
            "         [-2.,  0.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dChd_BVNUKHR",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9wLqaPmqqm",
        "colab_type": "code",
        "outputId": "b060dbb0-01e7-4de9-b446-1947a424f0a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "wandb.init(entity=\"nerdk312\", project=\"ATARI_LABELS_MPC\")#,monitor_gym=True)\n",
        "ENV_NAME = 'MsPacman-ramDeterministic-v4' #'MsPacmanNoFrameskip-v4'  #'MsPacmanDeterministic-v4' # 'MsPacmanNoFrameskip-v4'\n",
        " \n",
        "# Main loop hyperp\n",
        "env_model_pretrain = False\n",
        "pretrained_env_model = '/content/gdrive/My Drive/MsPacman-data/Env model/Env_model_21_4_2020-8_37.pt'#'/content/gdrive/My Drive/MsPacman-data/Env model/Env_model_17_4_2020-11_0.pt' #'/content/Env_model_17_4_2020-10_32.pt' #'/content/Epoch_no_43_Env_model_13_4_2020-8_22_checkpoint.pt'\n",
        "ENV_LEARNING_RATE = 1e-4\n",
        "\n",
        "AGGR_ITER = 10\n",
        "STEPS_PER_AGGR = 1000\n",
        "\n",
        "# Random MB hyperp\n",
        "NUM_RAND_TRAJECTORIES = 3000\n",
        "\n",
        "# 'cuda' or 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# Supervised Model Hyperp\n",
        "BATCH_SIZE = 1024\n",
        "TRAIN_ITER_MODEL =  10000\n",
        "\n",
        "# Controller Hyperp\n",
        "HORIZON_LENGTH = 1\n",
        "NUM_ACTIONS_SEQUENCES = 1\n",
        "\n",
        "load_data = True\n",
        "if load_data:\n",
        "    loaded_trajectories = '/content/gdrive/My Drive/MsPacman-data/only_agent_position_6000_traj__prev_action_fix+failed_action_fix.npy' #'/content/gdrive/My Drive/MsPacman-data/rand_traj_3000-29_4_2020-9_8.npy' #'/content/gdrive/My Drive/MsPacman-data/only_agent_position_6000_traj_allowed_move_prev_action_fix.npy' #'/content/gdrive/My Drive/MsPacman-data/rand_traj_3000-28_4_2020-18_0.npy' #'/content/gdrive/My Drive/MsPacman-data/only_agent_position_3000_traj_rand+allowed_moves.npy' #'/content/gdrive/My Drive/MsPacman-data/rand_traj_3000-21_4_2020-7_47.npy'#'/content/gdrive/My Drive/MsPacman-data/Random trajectories/No_wall_movements/rand_traj_3000-_all_lives_16_4_2020-14_33.npy'#'/content/gdrive/My Drive/MsPacman-data/rand_traj_3000-16_4_2020-14_33.npy'#\n",
        "\n",
        "collect_data = False\n",
        "\n",
        "observation_channels = 1\n",
        "action_dim = 1\n",
        "n_actions = 4 #9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "reward_dim = 1\n",
        "\n",
        "# Time and date information\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}_{}-{}_{}\".format(now.day,now.month,now.year, now.hour, now.minute)\n",
        "\n",
        "possible_positions = np.load('/content/gdrive/My Drive/MsPacman-data/possible_pacman_positions.npy',allow_pickle=True)\n",
        "# Making the network deterministic - https://pytorch.org/docs/stable/notes/randomness.html\n",
        "random_seed = 0\n",
        "\n",
        "if random_seed:    \n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    #np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/2moxfoy2\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/2moxfoy2</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmKe2r4Ys7Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = wandb.config\n",
        "config.batch_size = BATCH_SIZE          \n",
        "config.horizon_length = HORIZON_LENGTH\n",
        "config.num_action_seq = NUM_ACTIONS_SEQUENCES\n",
        "config.num_rand_trajectories = NUM_RAND_TRAJECTORIES\n",
        "config.aggr_iter = AGGR_ITER\n",
        "config.steps_per_aggr = STEPS_PER_AGGR\n",
        "\n",
        "config.train_model_iter = TRAIN_ITER_MODEL\n",
        "config.env_lr = ENV_LEARNING_RATE\n",
        "config.env_model_pretrain = env_model_pretrain\n",
        "config.pretrained_env =  pretrained_env_model\n",
        "\n",
        "config.no_actions = n_actions\n",
        "config.load_data = load_data\n",
        "config.collect_data = collect_data\n",
        "config.random_seed = random_seed\n",
        "\n",
        "if load_data:\n",
        "    config.loaded_trajectories = loaded_trajectories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIBf1pknbf_",
        "colab_type": "text"
      },
      "source": [
        "# Main - Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk1xCx6x7NZ2",
        "colab_type": "text"
      },
      "source": [
        "Data collection and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVutEvgUib8y",
        "colab_type": "code",
        "outputId": "d38f5ddc-e357-40ac-ddf9-24d079ce18f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "data_collector = Data_collection(ENV_NAME,n_actions,possible_positions)\n",
        "if load_data:\n",
        "    rand_dataset = np.load(loaded_trajectories,allow_pickle=True)\n",
        "else:\n",
        "    rand_dataset = data_collector.gather_random_trajectories(NUM_RAND_TRAJECTORIES)\n",
        "    filename = '/content/gdrive/My Drive/MsPacman-data/rand_traj_{}-'.format(NUM_RAND_TRAJECTORIES) + date_time\n",
        "    np.save(filename,rand_dataset)\n",
        "\n",
        "#rand_dataset = np.array([x for x in rand_dataset if not same(x)])\n",
        "#rand_dataset = rand_dataset[0:100,:]\n",
        "empty_dataset = []\n",
        "env_train_data, env_val_data = data_collector.collate_data(rand_dataset,empty_dataset)\n",
        "\n",
        "norm_env_train_data, norm_env_val_data = data_collector.setup_dataset(env_train_data, env_val_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number random examples: 893074 len(D_train_rand) 714460 len(D_valid_rand) 178614\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}