{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simplified_Model_based_Pacman_ATARI_labels_120420.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Yr8b9ZmG-dAp"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzpGHXL8nttp/Ye18BmBea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/Simplified_Model_based_Pacman_ATARI_labels_120420.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4jLAGgyjflE",
        "colab_type": "text"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "3ee253e7-e5c5-46de-b74b-31202d2e6b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/mila-iqia/atari-representation-learning.git\n",
            "  Cloning git://github.com/mila-iqia/atari-representation-learning.git to /tmp/pip-req-build-ipb6kck2\n",
            "  Running command git clone -q git://github.com/mila-iqia/atari-representation-learning.git /tmp/pip-req-build-ipb6kck2\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->atariari==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: atariari\n",
            "  Building wheel for atariari (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atariari: filename=atariari-0.0.1-cp36-none-any.whl size=46584 sha256=a9cc4877d2903fa6f0aecd05632ba6586a3778cdf9ac22e59a3a72c4862651e8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-se83lk29/wheels/3d/69/51/5e436e5ae566c5b4dec5c53e65396d516459877a42a11d7aa4\n",
            "Successfully built atariari\n",
            "Installing collected packages: atariari\n",
            "Successfully installed atariari-0.0.1\n",
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-xwlmfwkg\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-xwlmfwkg\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=6975a1f6ec5973f57824b7dc3d9cdfb6e67b79d0c816e240aabe8a825ee6c55f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nvdbzz79/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-875kx41g\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-875kx41g\n",
            "Collecting gym<0.16.0,>=0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/01/8771e8f914a627022296dab694092a11a7d417b6c8364f0a44a8debca734/gym-0.15.7.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.38.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines, gym\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=635ec42310e7bd74c49e200556447a13f53c031f58407f5021caff5cf789e84f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d62s1f3b/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-cp36-none-any.whl size=1648840 sha256=620d7e62464803f6507fe6ece600d05d201709c03d537722cf5a817d6f776780\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/60/6a/f9c27ae133abaf5a5687ed2fa8ed19627d7fac5d843a27572b\n",
            "Successfully built baselines gym\n",
            "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gym, baselines\n",
            "  Found existing installation: gym 0.17.1\n",
            "    Uninstalling gym-0.17.1:\n",
            "      Successfully uninstalled gym-0.17.1\n",
            "Successfully installed baselines-0.1.6 gym-0.15.7\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/dd/ce719d36c4172b56c7579a79fcfd2f731c386b39f258bb186ef17b73fd7d/wandb-0.8.32-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/1a/0df85d2bddbca33665d2148173d3281b290ac054b5f50163ea735740ac7b/GitPython-3.1.1-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 30.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 32.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.1)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/52/ca35448b56c53a079d3ffe18b1978c6e424f6d4df02404877094c89f5bfb/gitdb-4.0.4-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/27/b1/e379cfb7c07bbf8faee29c4a1a2469dbea525f047c2b454c4afdefa20a30/smmap-3.0.2-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: gql, subprocess32, watchdog, graphql-core, pathtools\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=c2da72c598ee624d13442c03cd53f67effcc6b333802988da84efa6693de1a14\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=b3ece35bfe4ad5c4e5169792da36a5304b935d4d2057ff3ca283ec84ce5da26d\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=04a8a95062605f59e18a7950d87a5cf3b597732a74b9ca6f7908c4f06738fd49\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=6c13169106fbda6b9a130efe99f2c5caa6b1f11bf635656828f5229d8e3e7794\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=a5aa77d992c52ebf437da550d0216db2666650c472ed4e669b7762e9a890d4bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built gql subprocess32 watchdog graphql-core pathtools\n",
            "Installing collected packages: graphql-core, gql, smmap, gitdb, GitPython, configparser, subprocess32, pathtools, watchdog, shortuuid, sentry-sdk, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.1 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.4 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.2 subprocess32-3.5.4 wandb-0.8.32 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZ9gMrxp5vU",
        "colab_type": "code",
        "outputId": "9c6b1b20-6b31-4542-80f7-0a6251f160a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI287_Yjxn3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, BatchSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from atariari.benchmark.envs import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from atariari.benchmark.wrapper import AtariARIWrapper\n",
        "\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Imported required for the Model-based RL\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "110420cf-cf3f-461a-ff55-6f394a652ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login ##############"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKVfcEFEz9-t",
        "colab_type": "text"
      },
      "source": [
        "# Model setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaBzmIUDnqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping_loss(object):\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=5, verbose=False, wandb=None, name=\"\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0 #  Counter which checks for early stopping\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = 1e11 # Nawid - Set a very high initial best loss\n",
        "        self.name = name\n",
        "        self.wandb = wandb\n",
        "        self.epoch_counter = 0 # Counter which counts the epochs\n",
        "        self.checkpoint_counter = 0 # Counter which saves after 10 successful decreases in value\n",
        "\n",
        "    def __call__(self, val_loss, model,optimizer):\n",
        "\n",
        "        score = val_loss\n",
        "        self.epoch_counter +=1\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model,optimizer)\n",
        "        elif score >= self.best_score: # Nawid - Inverse signs to take into minimising loss instead of maximising accuracy\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping for {self.name} counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(f'{self.name} has stopped')\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model,optimizer)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, optimizer):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f'Validation loss decreased/improved for {self.name}  ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        \n",
        "        current_checkpoint = {\n",
        "            'epoch': self.epoch_counter,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': val_loss,\n",
        "            'lr':optimizer.state_dict()['param_groups'][0]['lr']} # Saves the epoch number, model state dict, optimizer state, loss and learning rate\n",
        "        \n",
        "\n",
        "        save_dir = self.wandb.run.dir\n",
        "        torch.save(current_checkpoint, save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.wandb.save(save_dir + \"/\" + self.name + \".pt\")\n",
        "        #torch.save(model.state_dict(), save_dir + \"/\" + 'Epoch_no_{}_'.format(self.epoch_counter)+self.name + \".pt\")\n",
        "        #self.wandb.save(save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "        self.checkpoint_counter += 1\n",
        "\n",
        "        if self.checkpoint_counter % 10 == 0: # Saves the optimiser and the state model\n",
        "            checkpoint = {\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict' : optimizer.state_dict(),\n",
        "              'loss': val_loss,\n",
        "              'lr': optimizer.state_dict()['param_groups'][0]['lr']}\n",
        "              \n",
        "            torch.save(checkpoint, save_dir + \"/\" + 'Epoch_no_{}_'.format(self.epoch_counter)+self.name + \"_checkpoint.pt\") \n",
        "            self.wandb.save(save_dir + \"/\" + 'Epoch_no_{}_'.format(self.epoch_counter)+self.name + \"_checkpoint.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGPEetVzTtOb",
        "colab": {}
      },
      "source": [
        "class NNDynamicsModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the next state, given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, obs_output_dim):\n",
        "        super(NNDynamicsModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, obs_output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "class NNRewardModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the reward given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, reward_output_dim):\n",
        "        super(NNRewardModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, reward_output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "def model_MSEloss(y_pred,y_truth, device):\n",
        "    '''\n",
        "    Compute the MSE (Mean Squared Error)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3T5LHY6hRW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_data,val_data,model,batch_size, max_model_iter, optimizer, device,early_stopper,desired_model='Env_model'):\n",
        "    ''' \n",
        "    General function to train either of the two models\n",
        "    '''\n",
        "    # Unpack data\n",
        "    (X_train, y_train), (X_val, y_val) =  train_data, val_data\n",
        "    losses_env = []\n",
        "\n",
        "    loss_function = model_MSEloss\n",
        "\n",
        "    # go through max_model iter supervised iterations\n",
        "    for it in tqdm(range(max_model_iter)):\n",
        "        # create mini batches of size batch_size\n",
        "        for mb in range(0,len(X_train), batch_size): # Nawid- Batch size is the step size\n",
        "            if len(X_train) > mb + BATCH_SIZE:\n",
        "                X_mb = X_train[mb:mb+BATCH_SIZE]\n",
        "                y_mb = y_train[mb:mb+BATCH_SIZE]\n",
        "                #X_mb += np.random.normal(loc = 0, scale = 0.001, size= X_mb.shape)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # forward pass of model to compute the output\n",
        "                pred_mb = model(torch.tensor(X_mb).to(device))\n",
        "                \n",
        "                # compute the loss\n",
        "                loss = loss_function(pred_mb,y_mb,device) #Nawid-  Uses Mse loss if dynamics model or uses BCE loss if reward/done model\n",
        "                wandb.log({'{} Training loss'.format(desired_model):loss.cpu().detach().numpy()})\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                # optimization step\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Nawid - Calculate the validation loss after each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_val = model(torch.tensor(X_val).to(device))\n",
        "            val_loss = loss_function(pred_val, y_val,device)\n",
        "            wandb.log({'{} Validation loss'.format(desired_model):val_loss})\n",
        "        \n",
        "        # Checks whether to early stop after each epoch\n",
        "        \n",
        "        early_stopper(val_loss,model,optimizer)\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('epoch: {} completed'.format(it))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRn5uXDhgmQs",
        "colab_type": "text"
      },
      "source": [
        "# General Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2y__x5ysYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class General_functions():\n",
        "    def __init__(self, ENV_NAME, feature_size,n_actions,possible_positions,state_mode = 'ATARIARI',encoder=None):\n",
        "        self.state_mode = state_mode\n",
        "        self.env = AtariARIWrapper(gym.make(ENV_NAME))\n",
        "        self.initial_info_labels = self.env.labels()\n",
        "\n",
        "        self.feature_size = feature_size \n",
        "\n",
        "        self.repeated_initial = True # Nawid - set repeated initial as true initially\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.encoder= encoder\n",
        "        self.n_actions = n_actions\n",
        "        # possible positions is numpy array with most of the possible positions that the agent can go to\n",
        "        self.possible_positions = possible_positions\n",
        "        self.possible_positions_list = self.possible_positions.tolist()\n",
        "\n",
        "    def one_hot(self,i):\n",
        "        a = np.zeros(self.n_actions, 'uint8')\n",
        "        a[i] = 1\n",
        "        return a\n",
        "\n",
        "    def state_conversion(self,obs,info_labels):\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            state = []\n",
        "            i = 0             \n",
        "            for key in info_labels:\n",
        "                if i < self.feature_size: # Nawid - Only the first 14 info are crucial I believe\n",
        "                    state.append(info_labels[key])\n",
        "                    i +=1\n",
        "                else:\n",
        "                    state = np.array(state)\n",
        "                    return state\n",
        "                    \n",
        "                '''\n",
        "                if key == 'player_x' or key == 'player_y':\n",
        "                    state.append(info_labels[key])\n",
        "                \n",
        "            state = np.array(state)\n",
        "            return state\n",
        "                '''\n",
        "                #print(counter,key)\n",
        "    \n",
        "    def next_position(self,state, action):\n",
        "        #print('state', state, state.shape)\n",
        "\n",
        "        next_position = state[8:]\n",
        "        #next_position = np.reshape(next_position, (1,2))\n",
        "        if action == 0:\n",
        "            pass\n",
        "        elif action == 1:\n",
        "            next_position[1] = next_position[1] - 2 \n",
        "        elif action == 2:\n",
        "            next_position[0] = next_position[0] + 2\n",
        "        elif action == 3:\n",
        "            next_position[0] = next_position[0] - 2\n",
        "        elif action == 4:\n",
        "            next_position[1] = next_position[1] + 2\n",
        "        \n",
        "        #print('next position',next_position,next_position.shape)\n",
        "        if next_position.tolist() in self.possible_positions_list: # possible positions will be a list which is fed into the network \n",
        "            #print('present')\n",
        "            return True\n",
        "        else:\n",
        "            #print('not present')\n",
        "            return False\n",
        "\n",
        "        \n",
        "    def random_action_selection(self, state):\n",
        "        while True:\n",
        "            action = np.random.choice(5, 1, p=[0.04, 0.24, 0.24, 0.24, 0.24]).squeeze() # Samples a random action with these given probabilites\n",
        "            feasible_action = self.next_position(state,action)\n",
        "            if feasible_action:\n",
        "                #print('feasible action')\n",
        "                return action\n",
        "    \n",
        "            \n",
        "    def check_initial(self, next_info_labels):\n",
        "        # Checks if there has been a change from the initial info labels to show that the initial lag period is over \n",
        "        if self.initial_info_labels == next_info_labels:\n",
        "            pass\n",
        "        else:\n",
        "            self.repeated_initial = False\n",
        "\n",
        "    def reward_conversion(self,reward,done,info_labels,next_info_labels):\n",
        "        # checks whether there has been a change in lives- change done to 1, otherwise the done should be fine regardless\n",
        "        if not info_labels['num_lives'] == next_info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "            done = True \n",
        "            self.repeated_initial = True\n",
        "            self.initial_info_labels  = next_info_labels # Sets the new initial labels.\n",
        "        return reward, done  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJgI07xT7eo",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT3xItE78OyC",
        "colab_type": "code",
        "outputId": "26a1e0cb-8d11-49ab-9470-2dfbfd066c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "class Data_collection(General_functions):\n",
        "    def __init__(self,ENV_NAME, feature_size,n_actions, possible_positions, state_model = 'ATARIARI', encoder= None):\n",
        "        super(Data_collection,self).__init__(ENV_NAME,feature_size,n_actions,possible_positions, state_model, encoder)\n",
        "        \n",
        "    def gather_random_trajectories(self,num_traj):\n",
        "        dataset_random = []\n",
        "        #Env name could either be the RAM case or the generic case\n",
        "        # set pacman_done as initially zero\n",
        "        pacman_done = 0\n",
        "        i = 0\n",
        "        for n in range(num_traj):\n",
        "            if n % 10 ==0:\n",
        "                print('trajectory number :',n)\n",
        "            # Initial set up\n",
        "            obs = self.env.reset()\n",
        "            self.env.seed(0)\n",
        "\n",
        "            self.repeated_initial = True # Nawid- Used to represent the initial state\n",
        "            info_labels = self.env.labels() # Nawid -  Used to get the current state\n",
        "            state = self.state_conversion(obs, info_labels) # Used to get the initial state\n",
        "            while True:\n",
        "                # Choosing action and env step         \n",
        "                #sampled_action = np.random.randint(0,n_actions)\n",
        "                sampled_action = self.random_action_selection(state)\n",
        "\n",
        "                sampled_action_one_hot = self.one_hot(sampled_action)\n",
        "                next_obs, reward, done, next_info = self.env.step(sampled_action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                #print(next_info_labels)\n",
        "                # self.repeated initial is set to true at first and it is is turned to true in the reward_collection class when done is true\n",
        "                \n",
        "                self.check_initial(next_info_labels) # checks if the initial value and the current value is the same, if it is not then it changes the repeated_initial bool\n",
        "                if self.repeated_initial: # If the initial state is repeating\n",
        "                    pass\n",
        "                        \n",
        "                else:\n",
        "                    # New state achieved, so save data\n",
        "                    state = self.state_conversion(obs, info_labels)\n",
        "                    next_state = self.state_conversion(next_obs, next_info_labels)\n",
        "                    # looks if ep is done and sets the new initial info state and sets the repeated_initial bool back to true\n",
        "                    reward,pacman_done = self.reward_conversion(reward, done, info_labels, next_info_labels) \n",
        "                    dataset_random.append([state, next_state,reward,pacman_done,sampled_action_one_hot])\n",
        "                    \n",
        "                obs = next_obs\n",
        "                info_labels = next_info_labels\n",
        "\n",
        "                # ends the loop when its done or pacman_done in the single life setting\n",
        "                if done or (single_life and pacman_done):\n",
        "                    break \n",
        "        return dataset_random\n",
        "\n",
        "    def collate_data(self,random_dataset, rl_dataset):\n",
        "        rand_data = np.array(random_dataset)\n",
        "        num_rand_examples = len(rand_data)\n",
        "        D_train = rand_data[:int(-num_rand_examples*1/5)] \n",
        "        D_valid = rand_data[int(-num_rand_examples*1/5):]\n",
        "        print(\"number random examples:\",num_rand_examples, 'len(D_train_rand)', len(D_train),'len(D_valid_rand)', len(D_valid))\n",
        "        if len(rl_dataset) > 0:\n",
        "            # Adds the rl dataset to the random dataset if there is any present\n",
        "            rl_data = np.array(rl_dataset)\n",
        "            num_rl_examples = len(rl_data)\n",
        "            D_rl_train = rl_data[:int(-num_rl_examples*1/5)] \n",
        "            D_rl_valid = rl_data[int(-num_rl_examples*1/5):]\n",
        "                        \n",
        "            D_train = np.concatenate([D_train, D_rl_train], axis = 0)\n",
        "            D_valid = np.concatenate([D_valid, D_rl_valid], axis = 0)\n",
        "            print(\"number rl examples:\",num_rl_examples, 'len(D_rl_train)', len(D_rl_train),'len(D_valid_rand)', len(D_rl_valid))\n",
        "            \n",
        "        #print(\"len(D_train):\", len(D_train), 'len(D_valid)', len(D_valid))\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        \n",
        "        sff = np.arange(len(D_train))\n",
        "        np.random.shuffle(sff)\n",
        "        D_train = D_train[sff]\n",
        "        #print('D_train shape',D_train.shape)\n",
        "\n",
        "\n",
        "        # Create the input and output for the train\n",
        "        X_train_obs = np.array([obs for obs,_,_,_,_ in D_train]) # Takes obs and action\n",
        "        X_train_obs = X_train_obs.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        X_train_act = np.array([act for _,_,_,_,act in D_train])\n",
        "        \n",
        "\n",
        "        # Env output\n",
        "        y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "        y_env_train = y_env_train.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        y_env_train = y_env_train - X_train_obs \n",
        "        #y_env_train = y_env_train - np.array([obs for obs,_,_,_,_ in D_train]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        \n",
        "        # Next state output\n",
        "        X_val_obs = np.array([obs for obs,_,_,_,_ in D_valid]) # Takes obs and action\n",
        "        X_val_obs = X_val_obs.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)        \n",
        "        X_val_act = np.array([act for _,_,_,_,act in D_valid])\n",
        "\n",
        "        y_env_val = np.array([no for _,no,_,_,_ in D_valid])\n",
        "        y_env_val = y_env_val.astype(np.int16)\n",
        "        y_env_val = y_env_val - X_val_obs \n",
        "        #y_env_val = y_env_val - np.array([obs for obs,_,_,_,_ in D_valid]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        \n",
        "    \n",
        "        env_train_data, env_val_data = (X_train_obs, X_train_act, y_env_train), (X_val_obs, X_val_act, y_env_val)\n",
        "        \n",
        "\n",
        "        return env_train_data, env_val_data \n",
        "\n",
        "    def setup_dataset(self,env_train_data, env_val_data):\n",
        "        \n",
        "        # Unpack data\n",
        "        (X_env_train_obs, X_env_train_act, y_env_train), (X_env_val_obs, X_env_val_act, y_env_val) = env_train_data, env_val_data\n",
        "    \n",
        "        # Concatentates the normalised states with the one hot vector for the actions\n",
        "        X_env_train = np.concatenate((X_env_train_obs,X_env_train_act),axis=1)\n",
        "        X_env_val = np.concatenate((X_env_val_obs,X_env_val_act),axis=1)\n",
        "\n",
        "        # Pack data tuples\n",
        "        env_train_data, env_val_data = (X_env_train, y_env_train),(X_env_val, y_env_val) \n",
        "\n",
        "\n",
        "        return env_train_data, env_val_data\n",
        "    \n",
        "'''\n",
        "data_object = Data_collection(ENV_NAME,feature_size,n_actions,possible_positions)\n",
        "data = data_object.gather_random_trajectories(10)\n",
        "print(data[0])\n",
        "rl_dataset = []\n",
        "env_train, env_val = data_object.collate_data(data,rl_dataset)\n",
        "print(env_train)\n",
        "norm_env_train, norm_env_val, = data_object.setup_dataset(env_train, env_val)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndata_object = Data_collection(ENV_NAME,feature_size,n_actions,possible_positions)\\ndata = data_object.gather_random_trajectories(10)\\nprint(data[0])\\nrl_dataset = []\\nenv_train, env_val = data_object.collate_data(data,rl_dataset)\\nprint(env_train)\\nnorm_env_train, norm_env_val, = data_object.setup_dataset(env_train, env_val)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jEdA4yE3GfE",
        "colab_type": "text"
      },
      "source": [
        "# Controller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJqKzdpT6vLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class multi_model_based_control(General_functions):\n",
        "    def __init__(self,ENV_NAME, feature_size,n_actions,possible_positions,env_model,rew_model, num_sequences,horizon_length, state_model = 'ATARIARI', encoder= None):\n",
        "        super(multi_model_based_control,self).__init__(ENV_NAME,feature_size,n_actions,possible_positions, state_model, encoder)\n",
        "        \n",
        "        self.env_model = env_model\n",
        "        self.rew_model = rew_model\n",
        "        self.horizon_length = horizon_length\n",
        "        self.num_sequences = num_sequences        \n",
        "        self.n_actions = n_actions\n",
        "        self.global_step = 0\n",
        "        self.rl_dataset = []\n",
        "\n",
        "    def random_sampling_shooting(self,real_obs):\n",
        "        '''\n",
        "        Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "        '''\n",
        "        best_reward = -1e9\n",
        "        best_next_action = []\n",
        "        m_obs = np.array([real_obs for _ in range(self.num_sequences)])\n",
        "        \n",
        "\n",
        "        # array that contains the rewards for all the sequence\n",
        "        unroll_rewards = np.zeros((self.num_sequences, 1)) \n",
        "        first_sampled_actions = []\n",
        "\n",
        "        self.env_model.eval()\n",
        "        self.rew_model.eval()\n",
        "\n",
        "        # Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon length' times\n",
        "        ## i.e roll a given number of trajectories in a single batch (to increase speed)\n",
        "        for t in range(self.horizon_length):\n",
        "            # sample actions for each sequence\n",
        "            sampled_actions = np.array([np.random.randint(0,self.n_actions) for _ in range(self.num_sequences)])\n",
        "            #print('sampled actions', sampled_actions.shape)\n",
        "            #print(sampled_actions[0])\n",
        "            sampled_actions_one_hot = np.array([self.one_hot(action) for action in sampled_actions])\n",
        "            #print('sampled actions one hot',sampled_actions_one_hot.shape)\n",
        "            #print(sampled_actions_one_hot[0])\n",
        "\n",
        "        \n",
        "            env_model_input = np.concatenate([m_obs, sampled_actions_one_hot], axis = 1)            \n",
        "            #print('env model input shape',env_model_input.shape)\n",
        "            # compute the next state for each sequence\n",
        "            #print('m obs',m_obs[0])\n",
        "            pred_obs = self.env_model(torch.tensor(env_model_input).to(self.device))\n",
        "            #print('pred obs',pred_obs.shape)\n",
        "            #print('pred_obs',pred_obs[0])\n",
        "\n",
        "            # add previous observation\n",
        "            next_obs = pred_obs.cpu().detach().numpy() + m_obs\n",
        "            #print('next_obs',next_obs.shape)\n",
        "            #print('next_obs',next_obs[0])\n",
        "\n",
        "\n",
        "            pred_rew = self.rew_model.predict_reward(next_obs)\n",
        "            #print('pred rew',pred_rew.shape)\n",
        "            #print('unroll rewards', unroll_rewards.shape)\n",
        "            unroll_rewards += pred_rew\n",
        "        \n",
        "            m_obs = next_obs # Nawid - Update the state after calculating the new state and calculating the rewards\n",
        "\n",
        "            if t ==0:\n",
        "                first_sampled_actions = sampled_actions\n",
        "                #print('first sampled actions',first_sampled_actions.shape) \n",
        "        self.env_model.train()\n",
        "        self.rew_model.train()\n",
        "\n",
        "        # Best the position of the sequence with the higher reward\n",
        "        \n",
        "        arg_best_reward = np.argmax(unroll_rewards)\n",
        "        #print('arg_best reward', arg_best_reward)\n",
        "        #print('best reward',unroll_rewards[arg_best_reward])\n",
        "\n",
        "        #print('differet reward',unroll_rewards[arg_best_reward - 1])\n",
        "\n",
        "        best_sum_reward = unroll_rewards[arg_best_reward].squeeze()\n",
        "        # take the first action of this sequence\n",
        "        best_action = first_sampled_actions[arg_best_reward]\n",
        "        #best_action =  np.squeeze(best_action)\n",
        "        return best_action, best_sum_reward\n",
        "    \n",
        "    def on_policy_collection(self,STEPS_PER_AGGR):\n",
        "        obs = self.env.reset()\n",
        "        self.env.seed(0) # Set the random seed of the environment\n",
        "\n",
        "        num_examples_added = 0\n",
        "        game_reward = 0\n",
        "        # records how long the agent survives for\n",
        "        timesteps = 0\n",
        "        controller_pred_rews = []\n",
        "        rews = []\n",
        "        # records how many simulations have occurred\n",
        "\n",
        "        while num_examples_added < STEPS_PER_AGGR:\n",
        "            while True:\n",
        "                tt = time.time()\n",
        "                if self.repeated_initial: # Placed the repeated initial before the check_initial info labels since if the actions were chosen by MPC for the 264 intial actions, it would increase the computational time by quite a lot\n",
        "                    obs,_,_,info = self.env.step(0) # Any action taken\n",
        "                    info_labels = info['labels']\n",
        "                    # Checks if the initial set is being repeated and if it isnt, it sets the value off\n",
        "                    self.check_initial(info_labels) \n",
        "                else:\n",
        "                    # new state achieved\n",
        "                    state = self.state_conversion(obs,info_labels)\n",
        "                    action, pred_rew = self.random_sampling_shooting(state)\n",
        "                    action_one_hot = self.one_hot(action)\n",
        "                    controller_pred_rews.append(pred_rew)\n",
        "\n",
        "                    # one step in the environment with the action returned by\n",
        "                    next_obs, reward, done, next_info = self.env.step(action)\n",
        "                    next_info_labels = next_info['labels']\n",
        "                \n",
        "                    next_state = self.state_conversion(next_obs, next_info_labels)\n",
        "                    # Obtains the reward, done and sets whether repeated initial should be true or not\n",
        "                    reward, pacman_done = self.reward_conversion(reward,done,info_labels, next_info_labels) \n",
        "\n",
        "                    # add to the RL dataset                \n",
        "                    self.rl_dataset.append([state, next_state, reward, pacman_done, action_one_hot])\n",
        "\n",
        "                    num_examples_added += 1\n",
        "                    timesteps +=1 \n",
        "                    game_reward += reward\n",
        "                    obs = next_obs\n",
        "                    info_labels = next_info_labels\n",
        "                                 \n",
        "                    if done or (single_life and pacman_done):\n",
        "                        self.global_step += 1\n",
        "                        obs = self.env.reset()\n",
        "                        self.env.seed(0) # Need to set the random seed after the environment is done\n",
        "                        wandb.log({'pred_rew': np.mean(controller_pred_rews)*100,'survival time':timesteps,'global step':self.global_step }) # scaled the prediction by 100 to make it visible for comparison on the plot \n",
        "                        print('  >> R: {:.2f}, Mean sum:{:.2f},Survival time: {},Num examples:{}'.format(game_reward, np.mean(controller_pred_rews),timesteps,num_examples_added))\n",
        "                        rews.append(game_reward)\n",
        "                        game_reward = 0\n",
        "                        timesteps = 0\n",
        "                        controller_pred_rews = []\n",
        "                        break\n",
        "\n",
        "        print('  >> Mean: {:.2f}', np.mean(rews)) \n",
        "        return self.rl_dataset   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Sj3DYW07pF",
        "colab_type": "text"
      },
      "source": [
        "# Oracle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOcVxA8SawkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reward_Oracle():\n",
        "    \n",
        "    def predict_reward(self, next_state):\n",
        "        #print('next_state',next_state[:,0])\n",
        "\n",
        "        agent_position = np.transpose(np.array((next_state[:,8], next_state[:,9])))\n",
        "        #print('agent position', agent_position.shape)\n",
        "\n",
        "        enemy1_position = np.transpose(np.array((next_state[:,0], next_state[:,4])))\n",
        "        #print('enemy_position1',enemy1_position.shape)\n",
        "        enemy2_position = np.transpose(np.array((next_state[:,1], next_state[:,5])))\n",
        "        #print('enemy_position2',enemy2_position.shape)\n",
        "        enemy3_position = np.transpose(np.array((next_state[:,2], next_state[:,6])))\n",
        "        #print('enemy_position3',enemy3_position.shape)\n",
        "        enemy4_position = np.transpose(np.array((next_state[:,3], next_state[:,7])))\n",
        "        #print('enemy_position4',enemy4_position.shape)\n",
        "\n",
        "        distance_1 = np.array([np.linalg.norm(agent_position-enemy1_position, axis=1)])\n",
        "        distance_2 = np.array([np.linalg.norm(agent_position-enemy2_position, axis=1)])\n",
        "        distance_3 = np.array([np.linalg.norm(agent_position-enemy3_position, axis=1)])\n",
        "        distance_4 = np.array([np.linalg.norm(agent_position-enemy4_position, axis=1)])\n",
        "        overall_dist = np.transpose(np.concatenate((distance_1,distance_2, distance_3, distance_4),axis = 0))\n",
        "        #print('overall dist',overall_dist.shape)\n",
        "        min_dist = np.amin(overall_dist, axis = 1)\n",
        "\n",
        "        #print('min dist',min_dist.shape)\n",
        "        min_dist[min_dist<=threshold_distance] = 0\n",
        "        min_dist[min_dist>threshold_distance] = 1\n",
        "        #min_dist[min_dist<10] = 0\n",
        "        #min_dist[min_dist>10] = 1\n",
        "        min_dist = np.expand_dims(min_dist, axis = 1)\n",
        "        return min_dist\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dChd_BVNUKHR",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9wLqaPmqqm",
        "colab_type": "code",
        "outputId": "b214db83-eae4-4c8b-ec76-9a4050c04997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "wandb.init(entity=\"nerdk312\", project=\"ATARI_LABELS_MPC\")\n",
        "ENV_NAME = 'MsPacman-ramDeterministic-v4' #'MsPacmanNoFrameskip-v4'  #'MsPacmanDeterministic-v4' # 'MsPacmanNoFrameskip-v4'\n",
        "feature_size = 10 # only predict the agent state 10 \n",
        "\n",
        "# Main loop hyperp\n",
        "env_model_pretrain = False\n",
        "pretrained_env_model = '/content/Env_model_17_4_2020-10_32.pt' #'/content/Epoch_no_43_Env_model_13_4_2020-8_22_checkpoint.pt'\n",
        "\n",
        "ENV_LEARNING_RATE = 1e-4\n",
        "rew_oracle = True\n",
        "\n",
        "AGGR_ITER = 100\n",
        "STEPS_PER_AGGR = 30000\n",
        "\n",
        "# Random MB hyperp\n",
        "NUM_RAND_TRAJECTORIES = 3000\n",
        "\n",
        "# 'cuda' or 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# Supervised Model Hyperp\n",
        "BATCH_SIZE = 1024\n",
        "TRAIN_ITER_MODEL =  100\n",
        "\n",
        "# Controller Hyperp\n",
        "HORIZON_LENGTH = 5\n",
        "NUM_ACTIONS_SEQUENCES = 200\n",
        "\n",
        "load_data = True\n",
        "if load_data:\n",
        "    loaded_trajectories = '/content/gdrive/My Drive/MsPacman-data/Random trajectories/No_wall_movements/rand_traj_3000-_all_lives_16_4_2020-14_33.npy'#'/content/gdrive/My Drive/MsPacman-data/rand_traj_3000-16_4_2020-14_33.npy'#'/content/gdrive/My Drive/MsPacman-data/rand_traj_10000-16_4_2020-12_14.npy' #'/content/gdrive/My Drive/MsPacman-data/rand_traj_5000-15_4_2020-12_9.npy' #'/content/gdrive/My Drive/MsPacman-data/rand_traj_3000-11_4_2020-11_21.npy'\n",
        "\n",
        "collect_data = False\n",
        "\n",
        "observation_channels = 1\n",
        "action_dim = 1\n",
        "n_actions = 5 #9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "reward_dim = 1\n",
        "\n",
        "# Decides if an episode is done or not\n",
        "single_life = False\n",
        "\n",
        "# Time and date information\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}_{}-{}_{}\".format(now.day,now.month,now.year, now.hour, now.minute)\n",
        "\n",
        "\n",
        "possible_positions = np.load('/content/gdrive/My Drive/MsPacman-data/possible_pacman_positions.npy',allow_pickle=True)\n",
        "# Making the network deterministic - https://pytorch.org/docs/stable/notes/randomness.html\n",
        "random_seed = 0\n",
        "\n",
        "threshold_distance = 5 \n",
        "\n",
        "normalise_on = False\n",
        "if random_seed:    \n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    #np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "config = wandb.config\n",
        "config.batch_size = BATCH_SIZE          \n",
        "config.horizon_length = HORIZON_LENGTH\n",
        "config.num_action_seq = NUM_ACTIONS_SEQUENCES\n",
        "config.num_rand_trajectories = NUM_RAND_TRAJECTORIES\n",
        "config.aggr_iter = AGGR_ITER\n",
        "config.steps_per_aggr = STEPS_PER_AGGR\n",
        "\n",
        "config.train_model_iter = TRAIN_ITER_MODEL\n",
        "config.env_lr = ENV_LEARNING_RATE\n",
        "config.env_model_pretrain = env_model_pretrain\n",
        "config.pretrained_env =  pretrained_env_model\n",
        "\n",
        "config.rew_oracle = rew_oracle\n",
        "     \n",
        "config.no_actions = n_actions\n",
        "config.load_data = load_data\n",
        "config.collect_data = collect_data\n",
        "config.single_life = single_life\n",
        "config.random_seed = random_seed\n",
        "\n",
        "config.threshold_distance = threshold_distance\n",
        "if load_data:\n",
        "    config.loaded_trajectories = loaded_trajectories"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/3mdl7wbx\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/3mdl7wbx</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIBf1pknbf_",
        "colab_type": "text"
      },
      "source": [
        "# Main - Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk1xCx6x7NZ2",
        "colab_type": "text"
      },
      "source": [
        "Data collection and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVutEvgUib8y",
        "colab_type": "code",
        "outputId": "baa3bdff-b73e-4ca0-8da6-861b7e272dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "data_collector = Data_collection(ENV_NAME,feature_size,n_actions,possible_positions)\n",
        "if load_data:\n",
        "    rand_dataset = np.load(loaded_trajectories,allow_pickle=True)\n",
        "else:\n",
        "    rand_dataset = data_collector.gather_random_trajectories(NUM_RAND_TRAJECTORIES)\n",
        "    filename = '/content/gdrive/My Drive/MsPacman-data/rand_traj_{}-'.format(NUM_RAND_TRAJECTORIES) + date_time\n",
        "    np.save(filename,rand_dataset)\n",
        "\n",
        "#rand_dataset = np.array([x for x in rand_dataset if not same(x)])\n",
        "#rand_dataset = rand_dataset[0:100,:]\n",
        "empty_dataset = []\n",
        "env_train_data, env_val_data = data_collector.collate_data(rand_dataset,empty_dataset)\n",
        "\n",
        "norm_env_train_data, norm_env_val_data = data_collector.setup_dataset(env_train_data, env_val_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number random examples: 1290118 len(D_train_rand) 1032095 len(D_valid_rand) 258023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QG3_91bq-Y9I"
      },
      "source": [
        "Dynamics model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h62P5907GbX",
        "colab_type": "code",
        "outputId": "b1c7a6dc-5ad7-4c5a-dad5-94559f9eb975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "\n",
        "env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "\n",
        "\n",
        "if env_model_pretrain:\n",
        "    print('checkpoint')\n",
        "    #env_model.load_state_dict(torch.load(pretrained_env_model))\n",
        "    checkpoint = torch.load(pretrained_env_model)\n",
        "    env_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    for parameter in env_model.parameters():\n",
        "        parameter.requires_grad = False\n",
        "    env_model.eval()\n",
        "\n",
        "else:\n",
        "    env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "    wandb.watch(env_model, log=\"all\")\n",
        "    env_model_name = 'Env_model'+ '_' + date_time\n",
        "    early_stopping_env = EarlyStopping_loss(patience=0, verbose=True, wandb=wandb, name=env_model_name)\n",
        "    for n_iter in range(AGGR_ITER):\n",
        "        if early_stopping_env.early_stop:\n",
        "            print('Early stopping')\n",
        "            break \n",
        "        train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)\n",
        "\n",
        "#rew_model = Reward_Oracle()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/100 [00:16<27:20, 16.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_17_4_2020-11_0  (100000000000.000000 --> 16.172340).  Saving model ...\n",
            "epoch: 0 completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  2%|▏         | 2/100 [00:33<27:04, 16.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_17_4_2020-11_0  (16.172340 --> 15.618386).  Saving model ...\n",
            "epoch: 1 completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  3%|▎         | 3/100 [00:48<26:10, 16.19s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_17_4_2020-11_0  (15.618386 --> 15.435287).  Saving model ...\n",
            "epoch: 2 completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  4%|▍         | 4/100 [01:03<25:24, 15.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_17_4_2020-11_0  (15.435287 --> 15.336782).  Saving model ...\n",
            "epoch: 3 completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  5%|▌         | 5/100 [01:20<25:26, 16.07s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_17_4_2020-11_0  (15.336782 --> 15.279190).  Saving model ...\n",
            "epoch: 4 completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  6%|▌         | 6/100 [01:35<24:52, 15.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_17_4_2020-11_0  (15.279190 --> 15.242322).  Saving model ...\n",
            "epoch: 5 completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  7%|▋         | 7/100 [01:50<24:12, 15.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_17_4_2020-11_0  (15.242322 --> 15.228338).  Saving model ...\n",
            "epoch: 6 completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  8%|▊         | 8/100 [02:07<24:20, 15.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_17_4_2020-11_0  (15.228338 --> 15.224058).  Saving model ...\n",
            "epoch: 7 completed\n",
            "EarlyStopping for Env_model_17_4_2020-11_0 counter: 1 out of 0\n",
            "Env_model_17_4_2020-11_0 has stopped\n",
            "Early stopping\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTOm9XOX7kOL",
        "colab_type": "text"
      },
      "source": [
        "Training the reward function and MPC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjOA58oJH627",
        "colab_type": "code",
        "outputId": "2c701c72-dfc6-441f-ef09-5423c73572e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rew_model = Reward_Oracle()\n",
        "rand_env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "\n",
        "Controller = multi_model_based_control(ENV_NAME,feature_size,n_actions,possible_positions, rand_env_model, rew_model, NUM_ACTIONS_SEQUENCES, HORIZON_LENGTH)\n",
        "for n_iter in range(AGGR_ITER):    \n",
        "    '''\n",
        "    if early_stopping_env.early_stop:~\n",
        "        print('Early stopping')\n",
        "        break \n",
        "    '''\n",
        "\n",
        "    #train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)\n",
        "    \n",
        "\n",
        "    rl_dataset = Controller.on_policy_collection(STEPS_PER_AGGR)\n",
        "\n",
        "    '''\n",
        "    if collect_data:\n",
        "        env_train_data, env_val_data, rew_train_data, rew_val_data = data_collector.collate_data(rand_dataset,rl_dataset)\n",
        "        norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data = data_collector.setup_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  >> R: 160.00, Mean sum:5.00,Survival time: 384,Num examples:384\n",
            "  >> R: 300.00, Mean sum:4.99,Survival time: 405,Num examples:789\n",
            "  >> R: 590.00, Mean sum:4.95,Survival time: 801,Num examples:1590\n",
            "  >> R: 160.00, Mean sum:5.00,Survival time: 243,Num examples:1833\n",
            "  >> R: 340.00, Mean sum:5.00,Survival time: 417,Num examples:2250\n",
            "  >> R: 190.00, Mean sum:4.94,Survival time: 365,Num examples:2615\n",
            "  >> R: 270.00, Mean sum:5.00,Survival time: 393,Num examples:3008\n",
            "  >> R: 270.00, Mean sum:5.00,Survival time: 351,Num examples:3359\n",
            "  >> R: 270.00, Mean sum:5.00,Survival time: 357,Num examples:3716\n",
            "  >> R: 130.00, Mean sum:5.00,Survival time: 183,Num examples:3899\n",
            "  >> R: 140.00, Mean sum:4.89,Survival time: 177,Num examples:4076\n",
            "  >> R: 180.00, Mean sum:4.99,Survival time: 303,Num examples:4379\n",
            "  >> R: 690.00, Mean sum:4.98,Survival time: 561,Num examples:4940\n",
            "  >> R: 270.00, Mean sum:4.91,Survival time: 333,Num examples:5273\n",
            "  >> R: 440.00, Mean sum:4.98,Survival time: 615,Num examples:5888\n",
            "  >> R: 340.00, Mean sum:4.99,Survival time: 377,Num examples:6265\n",
            "  >> R: 190.00, Mean sum:4.96,Survival time: 217,Num examples:6482\n",
            "  >> R: 210.00, Mean sum:5.00,Survival time: 333,Num examples:6815\n",
            "  >> R: 220.00, Mean sum:4.99,Survival time: 385,Num examples:7200\n",
            "  >> R: 100.00, Mean sum:5.00,Survival time: 195,Num examples:7395\n",
            "  >> R: 1150.00, Mean sum:4.97,Survival time: 609,Num examples:8004\n",
            "  >> R: 210.00, Mean sum:5.00,Survival time: 353,Num examples:8357\n",
            "  >> R: 160.00, Mean sum:4.71,Survival time: 313,Num examples:8670\n",
            "  >> R: 340.00, Mean sum:5.00,Survival time: 375,Num examples:9045\n",
            "  >> R: 190.00, Mean sum:4.93,Survival time: 275,Num examples:9320\n",
            "  >> R: 400.00, Mean sum:4.98,Survival time: 535,Num examples:9855\n",
            "  >> R: 400.00, Mean sum:5.00,Survival time: 671,Num examples:10526\n",
            "  >> R: 340.00, Mean sum:4.93,Survival time: 441,Num examples:10967\n",
            "  >> R: 230.00, Mean sum:5.00,Survival time: 371,Num examples:11338\n",
            "  >> R: 990.00, Mean sum:4.98,Survival time: 463,Num examples:11801\n",
            "  >> R: 270.00, Mean sum:4.99,Survival time: 349,Num examples:12150\n",
            "  >> R: 180.00, Mean sum:4.97,Survival time: 327,Num examples:12477\n",
            "  >> R: 130.00, Mean sum:4.94,Survival time: 377,Num examples:12854\n",
            "  >> R: 220.00, Mean sum:4.99,Survival time: 439,Num examples:13293\n",
            "  >> R: 150.00, Mean sum:4.99,Survival time: 209,Num examples:13502\n",
            "  >> R: 670.00, Mean sum:5.00,Survival time: 629,Num examples:14131\n",
            "  >> R: 190.00, Mean sum:4.93,Survival time: 303,Num examples:14434\n",
            "  >> R: 200.00, Mean sum:4.93,Survival time: 309,Num examples:14743\n",
            "  >> R: 240.00, Mean sum:4.87,Survival time: 363,Num examples:15106\n",
            "  >> R: 280.00, Mean sum:4.99,Survival time: 299,Num examples:15405\n",
            "  >> R: 250.00, Mean sum:5.00,Survival time: 335,Num examples:15740\n",
            "  >> R: 310.00, Mean sum:4.99,Survival time: 363,Num examples:16103\n",
            "  >> R: 210.00, Mean sum:4.93,Survival time: 299,Num examples:16402\n",
            "  >> R: 340.00, Mean sum:4.94,Survival time: 489,Num examples:16891\n",
            "  >> R: 180.00, Mean sum:4.99,Survival time: 357,Num examples:17248\n",
            "  >> R: 260.00, Mean sum:5.00,Survival time: 443,Num examples:17691\n",
            "  >> R: 340.00, Mean sum:4.99,Survival time: 399,Num examples:18090\n",
            "  >> R: 250.00, Mean sum:5.00,Survival time: 395,Num examples:18485\n",
            "  >> R: 190.00, Mean sum:5.00,Survival time: 309,Num examples:18794\n",
            "  >> R: 230.00, Mean sum:5.00,Survival time: 431,Num examples:19225\n",
            "  >> R: 320.00, Mean sum:5.00,Survival time: 399,Num examples:19624\n",
            "  >> R: 320.00, Mean sum:4.98,Survival time: 347,Num examples:19971\n",
            "  >> R: 370.00, Mean sum:4.95,Survival time: 599,Num examples:20570\n",
            "  >> R: 210.00, Mean sum:4.97,Survival time: 267,Num examples:20837\n",
            "  >> R: 210.00, Mean sum:4.99,Survival time: 257,Num examples:21094\n",
            "  >> R: 260.00, Mean sum:4.94,Survival time: 423,Num examples:21517\n",
            "  >> R: 210.00, Mean sum:4.96,Survival time: 321,Num examples:21838\n",
            "  >> R: 170.00, Mean sum:4.93,Survival time: 449,Num examples:22287\n",
            "  >> R: 390.00, Mean sum:4.91,Survival time: 507,Num examples:22794\n",
            "  >> R: 220.00, Mean sum:4.97,Survival time: 429,Num examples:23223\n",
            "  >> R: 620.00, Mean sum:5.00,Survival time: 453,Num examples:23676\n",
            "  >> R: 450.00, Mean sum:4.99,Survival time: 481,Num examples:24157\n",
            "  >> R: 160.00, Mean sum:4.98,Survival time: 253,Num examples:24410\n",
            "  >> R: 170.00, Mean sum:4.90,Survival time: 307,Num examples:24717\n",
            "  >> R: 230.00, Mean sum:4.93,Survival time: 325,Num examples:25042\n",
            "  >> R: 190.00, Mean sum:4.99,Survival time: 353,Num examples:25395\n",
            "  >> R: 320.00, Mean sum:5.00,Survival time: 323,Num examples:25718\n",
            "  >> R: 200.00, Mean sum:5.00,Survival time: 393,Num examples:26111\n",
            "  >> R: 290.00, Mean sum:5.00,Survival time: 327,Num examples:26438\n",
            "  >> R: 140.00, Mean sum:5.00,Survival time: 251,Num examples:26689\n",
            "  >> R: 580.00, Mean sum:4.99,Survival time: 515,Num examples:27204\n",
            "  >> R: 250.00, Mean sum:4.93,Survival time: 391,Num examples:27595\n",
            "  >> R: 210.00, Mean sum:5.00,Survival time: 363,Num examples:27958\n",
            "  >> R: 360.00, Mean sum:4.97,Survival time: 377,Num examples:28335\n",
            "  >> R: 190.00, Mean sum:4.99,Survival time: 369,Num examples:28704\n",
            "  >> R: 260.00, Mean sum:4.93,Survival time: 387,Num examples:29091\n",
            "  >> R: 220.00, Mean sum:4.99,Survival time: 397,Num examples:29488\n",
            "  >> R: 280.00, Mean sum:4.99,Survival time: 447,Num examples:29935\n",
            "  >> R: 570.00, Mean sum:4.98,Survival time: 599,Num examples:30534\n",
            "  >> Mean: {:.2f} 295.3164556962025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2d1d2ac754f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mrl_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mController\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_policy_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTEPS_PER_AGGR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     '''\n",
            "\u001b[0;32m<ipython-input-26-2dace914a6e3>\u001b[0m in \u001b[0;36mon_policy_collection\u001b[0;34m(self, STEPS_PER_AGGR)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0;31m# new state achieved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_rew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sampling_shooting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0maction_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'info_labels' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr8b9ZmG-dAp",
        "colab_type": "text"
      },
      "source": [
        "# Old code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-01PY0KCpeO",
        "colab_type": "code",
        "outputId": "b6e78749-ea5b-4d52-bf09-9ed209ad84cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Obtains all the data samples\n",
        "X_obs, X_act, y_env =env_train_data\n",
        "X_obs_val, X_act_val, y_env_val = env_val_data\n",
        "all_X_obs = np.concatenate((X_obs,X_obs_val))\n",
        "\n",
        "player_positions = all_X_obs[:,8:]\n",
        "print(player_positions.shape)\n",
        "enemy1_positions = np.zeros_like(player_positions)\n",
        "enemy2_positions = np.zeros_like(player_positions)\n",
        "enemy3_positions = np.zeros_like(player_positions)\n",
        "enemy4_positions = np.zeros_like(player_positions)\n",
        "\n",
        "# enemy x positions\n",
        "enemy1_positions[:,0] = all_X_obs[:,0]\n",
        "enemy2_positions[:,0] = all_X_obs[:,1]\n",
        "enemy3_positions[:,0] = all_X_obs[:,2]\n",
        "enemy4_positions[:,0] = all_X_obs[:,3]\n",
        "\n",
        "# enemy y positions\n",
        "enemy1_positions[:,1] = all_X_obs[:,4]\n",
        "enemy2_positions[:,1] = all_X_obs[:,5]\n",
        "enemy3_positions[:,1] = all_X_obs[:,6]\n",
        "enemy4_positions[:,1] = all_X_obs[:,7]\n",
        "all_agent_positions = np.concatenate((player_positions,enemy1_positions,enemy2_positions,enemy3_positions, enemy4_positions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1841002, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re8SW0HlOawR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique = np.unique(all_agent_positions, axis=0,return_counts=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uH3eN1GOjU_",
        "colab_type": "code",
        "outputId": "86e9094f-9ec8-4fe9-bcf3-d0da4b548f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "values = np.sort(unique[1])\n",
        "print(values[-1600:-1300])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[129 130 132 133 137 137 140 145 147 149 151 153 154 156 159 162 165 168\n",
            " 168 169 171 172 174 179 179 181 184 184 185 187 188 188 188 189 190 193\n",
            " 194 197 197 197 202 202 202 203 203 204 205 206 212 212 217 219 219 221\n",
            " 221 223 223 224 226 226 227 228 228 230 231 235 236 237 237 239 240 243\n",
            " 245 245 245 246 247 248 248 249 249 250 250 250 252 257 257 259 259 261\n",
            " 265 265 272 272 273 275 276 276 279 279 281 281 282 283 283 284 284 285\n",
            " 287 289 290 293 293 293 294 294 295 295 295 297 298 300 300 301 301 302\n",
            " 302 303 304 306 307 311 312 312 313 314 314 315 316 318 323 325 330 331\n",
            " 332 332 332 335 335 335 336 337 339 339 343 344 345 347 348 350 350 350\n",
            " 351 351 353 353 353 358 362 364 364 364 368 369 370 372 372 373 374 375\n",
            " 375 376 376 378 379 383 383 383 384 386 387 389 390 391 392 392 392 393\n",
            " 393 393 394 395 397 398 401 402 402 403 404 405 405 405 405 406 407 407\n",
            " 407 409 412 414 416 416 416 416 418 418 420 426 428 430 431 433 433 435\n",
            " 436 439 439 440 441 442 442 443 444 444 445 447 448 449 451 451 452 452\n",
            " 452 453 453 453 455 456 456 456 457 459 460 463 467 470 470 471 471 473\n",
            " 474 474 475 477 477 479 480 482 486 486 486 487 488 489 493 496 499 499\n",
            " 499 500 501 506 506 507 508 509 509 511 512 513]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqfr3XQ7skh4",
        "colab_type": "code",
        "outputId": "515ed50b-bb51-4094-bb53-3adf509deb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "player_x = X_obs[:,0]\n",
        "player_y = X_obs[:,1]\n",
        "max_x = np.amax(player_x)\n",
        "min_x = np.amin(player_x)\n",
        "print('min x:', min_x,'max x:',max_x)\n",
        "max_y = np.amax(player_y)\n",
        "min_y = np.amin(player_y)\n",
        "print('min y:', min_y,'max y:',max_y)\n",
        "\n",
        "unique = np.unique(X_obs, axis=0,return_counts=True)\n",
        "print(positions.shape)\n",
        "positions = unique[0]\n",
        "\n",
        "values = unique[1]\n",
        "\n",
        "total = values.sum()\n",
        "print('total:',total)\n",
        "#print(values[-1])\n",
        "ordered_values = np.sort(values)\n",
        "#print(ordered_values[-30]) \n",
        "largest_ordered_values = ordered_values[-10:-1] \n",
        "print(largest_ordered_values.sum() + ordered_values[-1])\n",
        "\n",
        "#max_value = np.amax(values)\n",
        "#print(max_value)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min x: 12 max x: 171\n",
            "min y: 2 max y: 158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRQ5rPkN2ocR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def same(inputs):\n",
        "    current = inputs[0]\n",
        "    #print('current current)\n",
        "    next = inputs[1]\n",
        "    #print(next)\n",
        "    if current[0] == next[0] and current[1] == next[1]:\n",
        "        output = True\n",
        "    else:\n",
        "        output = False\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlXY0TarncbB",
        "colab_type": "code",
        "outputId": "7e1fdf1e-9b53-455b-e4bb-45fafb05139b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(all_agent_positions,columns=['x', 'y'])\n",
        "no_dup_df = df.drop_duplicates()\n",
        "fully_ordered_list = no_dup_df.sort_values(by=['x','y'])\n",
        "print(fully_ordered_list[0:530])\n",
        "ordered_data = fully_ordered_list.to_numpy()\n",
        "#np.save('/content/gdrive/My Drive/MsPacman-data/possible_pacman_positions',ordered_data)\n",
        "array = np.array([120,80])\n",
        "print(array.shape)\n",
        "if array.tolist() in ordered_data.tolist():\n",
        "    print('present')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         x    y\n",
            "447369  58   24\n",
            "318877  58   25\n",
            "16592   58   26\n",
            "556     58   50\n",
            "1816    58   74\n",
            "54      58   98\n",
            "1629    58   99\n",
            "23      58  100\n",
            "1289    58  101\n",
            "1428    58  102\n",
            "1218    58  103\n",
            "29      58  104\n",
            "1606    58  105\n",
            "813     58  106\n",
            "771     58  107\n",
            "249     58  108\n",
            "554     58  109\n",
            "68      58  110\n",
            "3388    58  111\n",
            "98      58  112\n",
            "7820    58  113\n",
            "1036    58  114\n",
            "4557    58  115\n",
            "1318    58  116\n",
            "459     58  117\n",
            "4929    58  118\n",
            "1521    58  119\n",
            "337     58  120\n",
            "832     58  121\n",
            "536     58  122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctKPqTuqVEV8",
        "colab_type": "code",
        "outputId": "63a3aa49-286c-45ac-cd06-1861f5a8386f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "next_state = np.zeros((200,8))\n",
        "next_state[:,4] = 1\n",
        "next_state[:,5] = 1\n",
        "#print(next_state)\n",
        "#print(next_state)\n",
        "agent_position = np.array((next_state[:,4], next_state[:,5]))\n",
        "#agent_position = np.array(agent_position)\n",
        "#print('agent shape',agent_position.shape)\n",
        "sue_position = np.array((next_state[:,0], next_state[:,1]))\n",
        "blinky_position = np.array((next_state[:,2], next_state[:,3]))\n",
        "winky_position = np.array((next_state[:,6], next_state[:,7]))\n",
        "\n",
        "distance_sue = np.array([np.linalg.norm(agent_position-sue_position, axis=0)])\n",
        "distance_blinky = np.array([np.linalg.norm(agent_position-blinky_position, axis=0)])\n",
        "distance_winky = np.array([np.linalg.norm(agent_position- winky_position, axis=0)])\n",
        "#print(distance_winky.shape)\n",
        "overall_dist = np.concatenate((distance_sue, distance_blinky, distance_winky),axis = 0)\n",
        "overall_dist = np.transpose(overall_dist)\n",
        "print('overall dist',overall_dist.shape)\n",
        "minElement = np.amin(overall_dist, axis = 1)\n",
        "print('min element', minElement.shape)\n",
        "\n",
        "minElement[minElement<1.2] = 0\n",
        "minElement[minElement>4] = 1\n",
        "minElement = np.expand_dims(minElement,axis=1)\n",
        "print(minElement.shape)\n",
        "'''\n",
        "minElement = np.amin(distance,axis=0)\n",
        "print('min element',minElement.shape)\n",
        "minElement[minElement<1] = 0\n",
        "minElement[minElement>1] = 1\n",
        "minElement = np.expand_dims(minElement,axis = 1)\n",
        "print(minElement.shape)\n",
        "\n",
        "print('overall_shape',overall_dist.shape)\n",
        "'''\n",
        "#enemy_positions = (next_state[:,0], next_state[:,1]),(next_state[:,2], next_state[:,3]),(next_state[:6], next_state[:,7])\n",
        "#enemy_positions = np.array(enemy_positions)\n",
        "#print('enemy positions',enemy_positions.shape)\n",
        "\n",
        "'''\n",
        "distance = np.array([np.linalg.norm(agent_position-x,axis = 0) for x in enemy_positions])\n",
        "print('distance',distance.shape)\n",
        "minElement = np.amin(distance,axis=0)\n",
        "print('min element',minElement.shape)\n",
        "minElement[minElement<1] = 0\n",
        "minElement[minElement>1] = 1\n",
        "minElement = np.expand_dims(minElement,axis = 1)\n",
        "print(minElement.shape)\n",
        "'''     \n",
        "\n",
        "'''\n",
        "for x in enemy_positions:\n",
        "    dist = agent_position - x\n",
        "    distance = np.array(np.linalg.norm(dist,axis = 0))\n",
        "    print(distance.shape)\n",
        "'''\n",
        "    #print(dist.shape)\n",
        "#print(enemy_positions.shape)\n",
        "\n",
        "#distance = np.array(np.linalg.norm(agent_position -enemy_positions,axis = 1))\n",
        "#print(distance)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "overall dist (200, 3)\n",
            "min element (200,)\n",
            "(200, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor x in enemy_positions:\\n    dist = agent_position - x\\n    distance = np.array(np.linalg.norm(dist,axis = 0))\\n    print(distance.shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHvi6nQyX_Q7",
        "colab_type": "code",
        "outputId": "ffdf5d96-8339-4bb1-c4da-45e4c398023b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c = np.array([[ 1, 2, 3],[-1, 1, 4]])\n",
        "print(c.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a_4PgYYQ0PR",
        "colab_type": "code",
        "outputId": "ced0240f-10a8-40a9-e2c0-38d53a02dd14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next_state = np.zeros((3,10))\n",
        "#print(x.shape)\n",
        "next_state[:,0:7] = 1\n",
        "\n",
        "\n",
        "agent_position = [(next_state[:,8], next_state[:,9])]\n",
        "agent_position = np.array(agent_position)\n",
        "\n",
        "enemy_positions = [(next_state[:,0],next_state[:,4]),(next_state[:,1],next_state[:,5]),(next_state[:,2],next_state[:,6]),(next_state[:,3],next_state[:,7])]\n",
        "enemy_positions = np.array(enemy_positions)\n",
        "distance = [np.linalg.norm(agent_position[0]-x,axis = 1) for x in enemy_positions]\n",
        "min_distance = np.amin(distance)\n",
        "\n",
        "if min_distance < 1:\n",
        "    reward = 0\n",
        "else:\n",
        "    reward = 1\n",
        "\n",
        "#np.linalg.norm(axis =0)\n",
        "print(distance)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([1.73205081, 1.73205081]), array([1.73205081, 1.73205081]), array([1.73205081, 1.73205081]), array([1.73205081, 0.        ])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JabOS8s03oFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for var_name in env_optimizer.state_dict():\n",
        "    print(var_name)\n",
        "    print(env_optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "    #print(var_name, \"\\t\", env_optimizer.state_dict()[var_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J6b7FwBdp-U",
        "colab_type": "code",
        "outputId": "af0d17ea-92c8-4081-d3df-1322f29782a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import numpy as np\n",
        "agent_position = [(0,1)]\n",
        "agent_position = np.array(agent_position)\n",
        "centerList = [(1,2),(54, 2991), (1717, 2989), (1683, 2991), (1604, 2991), (114, 2991), (919,222), (930,233)]\n",
        "centerList = np.array(centerList)\n",
        "\n",
        "#print(centerList[0])\n",
        "#print(centerList[0])\n",
        "values = np.array([np.linalg.norm(agent_position[0]-x) for x in centerList])\n",
        "\n",
        "#print(centerList.shape)\n",
        "print(values)\n",
        "#print(new_values)\n",
        "agent = np.array([1,2])\n",
        "print(agent.shape)\n",
        "print(agent_position.shape)\n",
        "minElement = np.amin(values)\n",
        " \n",
        "print('Minimum element from Numpy Array : ', minElement)\n",
        "def getDistance(p0,p1):\n",
        "    return np.linalg.norm(p0-p1)\n",
        "\n",
        "#getDistance(centerList[0], centerList[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.41421356e+00 2.99048759e+03 3.44619109e+03 3.43112066e+03\n",
            " 3.39306882e+03 2.99217245e+03 9.45199450e+02 9.58500913e+02]\n",
            "(2,)\n",
            "(1, 2)\n",
            "Minimum element from Numpy Array :  1.4142135623730951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j8Z3es4UKRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5P-E36KnFoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = wandb.config\n",
        "config.batch_size = BATCH_SIZE          \n",
        "config.horizon_length = HORIZON_LENGTH\n",
        "config.num_action_seq = NUM_ACTIONS_SEQUENCES\n",
        "config.train_model_iter = TRAIN_ITER_MODEL \n",
        "config.num_rand_trajectories = NUM_RAND_TRAJECTORIES\n",
        "config.aggr_iter = AGGR_ITER\n",
        "config.steps_per_aggr = STEPS_PER_AGGR\n",
        "\n",
        "config.env_lr = ENV_LEARNING_RATE\n",
        "\n",
        "\n",
        "config.rew_oracle = rew_oracle\n",
        "     \n",
        "config.no_actions = n_actions\n",
        "config.load_data = load_data\n",
        "config.collect_data = collect_data\n",
        "config.single_life = single_life\n",
        "config.random_seed = random_seed\n",
        "\n",
        "if load_data:\n",
        "    config.loaded_trajectories = loaded_trajectories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YWFYWCoMhw",
        "colab_type": "code",
        "outputId": "1917d94e-ea3f-4db5-a38d-7ff5949dad1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "counter = 0\n",
        "#current = rand_dataset[1,0]\n",
        "#next = rand_dataset[2,1]\n",
        "\n",
        "#if current[0] == next[0] and current[1] == next[1]:\n",
        "#    print('same')\n",
        "#print(current)\n",
        "#print(next)\n",
        "\n",
        "def same(inputs):\n",
        "    current = inputs[0]\n",
        "    #print('current current)\n",
        "    next = inputs[1]\n",
        "    #print(next)\n",
        "    if current[0] == next[0] and current[1] == next[1]:\n",
        "        output = True\n",
        "    else:\n",
        "        output = False\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "print('rand',rand_dataset[0])\n",
        "some_list = np.array([x for x in rand_dataset if not same(x)])\n",
        "print('some list',some_list.shape)\n",
        "'''\n",
        "for i in range(len(rand_dataset)):\n",
        "    current = rand_dataset[i,0]\n",
        "    next = rand_dataset[i,1]\n",
        "    if current[0] == next[0] and current[1] == next[1]:\n",
        "        counter += 1 \n",
        "\n",
        "print(counter)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rand [array([88, 98], dtype=uint8) array([85, 98], dtype=uint8) 0.0 False\n",
            " array([0, 1, 0, 0, 0], dtype=uint8)]\n",
            "some list (301909, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i in range(len(rand_dataset)):\\n    current = rand_dataset[i,0]\\n    next = rand_dataset[i,1]\\n    if current[0] == next[0] and current[1] == next[1]:\\n        counter += 1 \\n\\nprint(counter)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42HEmPSnFpuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#unique = np.unique(all_agent_positions, axis=0)\n",
        "#ordered_values = np.sort(unique)\n",
        "#ordered_values.shape\n",
        "\n",
        "#all_agent_positions_x = all_agent_positions[:,1]\n",
        "#all_agent_positions_x =\n",
        "\n",
        "#new_array = [tuple(row) for row in all_agent_positions]\n",
        "#unique_tuples = np.unique(new_array)\n",
        "\n",
        "'''\n",
        "max_x = np.amax(all_agent_positions_x)\n",
        "min_x = np.amin(all_agent_positions_x)\n",
        "print('min x:', min_x,'max x:',max_x)\n",
        "\n",
        "a = np.array([[1, 1], [1, 0 ],[2,0],[3,0]])\n",
        "print(a.shape)\n",
        "unique_prac = np.unique(a,axis=0)\n",
        "print(unique_prac)\n",
        "\n",
        "list_of_values = []\n",
        "list_of_values.append([1,2])\n",
        "list_of_values.append([2,3])\n",
        "final_list = np.array(list_of_values)\n",
        "print(final_list)\n",
        "\n",
        "\n",
        "def manual_sorting(dataset):\n",
        "    final_list = []\n",
        "    upper_val = len(dataset)\n",
        "    x = 11\n",
        "    while x < 172:\n",
        "        y = 1\n",
        "        while y < 159:\n",
        "            if dataset[i] == [x,y]:\n",
        "                final_list.append([x,y])\n",
        "                y +=1\n",
        "                i = 0\n",
        "            else:\n",
        "                i +=1\n",
        "            \n",
        "            if i == upper_val - 1:\n",
        "                y += 1\n",
        "                i = 0\n",
        "            # if all y values have been seen, increase the x value\n",
        "            if y == 158:\n",
        "                x +=1\n",
        "                \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}