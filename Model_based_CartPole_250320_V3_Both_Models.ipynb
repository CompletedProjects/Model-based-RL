{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_based_CartPole_250320_V3_Both_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOfWGq+VS4m1mpbwy8cpyLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/Model_based_CartPole_250320_V3_Both_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFXEIGMvhbTt",
        "colab_type": "code",
        "outputId": "ad3eb4c9-f58a-4b2c-e485-d29b866b7a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/6f/f8485396e1e17c6b4257eecbcae1869d4d27ec960eb2bc06ea3fb829bc9a/wandb-0.8.30-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.7MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 8.8MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 16.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \n",
            "\u001b[?25hCollecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2019.11.28)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, gql, subprocess32, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=5895068459ed47662bcccfa9bb49fd266e2f21c19789a3c2de3d16a601514eba\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=be7976fb63e63def67bfa3cf44d7fbf579391f54054b69e862ced09308a9d0a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=10c7d093cf2009c3f5597746ef8f34743c88807cbc46527192a49e715372a208\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=91db70f36492c27fa1626cb72af308e62d2758c691fad4052e6cd7d48b075045\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=e921232fccc858e942392ac1e4506b2542ef0f518b1b25db89fc63d81660d3af\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog gql subprocess32 pathtools graphql-core\n",
            "Installing collected packages: pathtools, watchdog, shortuuid, smmap, gitdb, GitPython, sentry-sdk, graphql-core, gql, docker-pycreds, configparser, subprocess32, wandb\n",
            "Successfully installed GitPython-3.1.0 configparser-4.0.2 docker-pycreds-0.4.0 gitdb-4.0.2 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.1 subprocess32-3.5.4 wandb-0.8.30 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNZIoA0Fp9Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, BatchSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "import gym\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import wandb\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Imported required for the Model-based RL\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "4840f986-18eb-41d0-f0c0-1903b2d0f086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login 11d787a211e05ca01c50131c5724e375cd5d3364"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaBzmIUDnqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping(object):\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, verbose=False, wandb=None, name=\"\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = 1e11 # Nawid - Set a very high initial best loss\n",
        "        self.name = name\n",
        "        self.wandb = wandb\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score >= self.best_score: # Nawid - Inverse signs to take into minimising loss instead of maximising accuracy\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping for {self.name} counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(f'{self.name} has stopped')\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f'Validation loss decreased/improved for {self.name}  ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "\n",
        "        save_dir = self.wandb.run.dir\n",
        "        torch.save(model.state_dict(), save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.wandb.save(save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YkYkqY4igVj",
        "colab_type": "text"
      },
      "source": [
        "# Oracle models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT7ST5imbY3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.envs.classic_control import CartPoleEnv\n",
        "import math\n",
        "from gym import spaces, logger\n",
        "from gym.utils import seeding\n",
        "\n",
        "class CartPoleTask(CartPoleEnv):\n",
        "    def predict_reward(self, state, action, next_state):\n",
        "        x, theta = next_state[:, 0], next_state[:, 2]\n",
        "        done = ((x < -self.x_threshold) \\\n",
        "                | (x > self.x_threshold) \\\n",
        "                | (theta < -self.theta_threshold_radians) \\\n",
        "                | (theta > self.theta_threshold_radians))#.float()\n",
        "        cost = done - 1.0\n",
        "        reward = - cost # Nawid- Finds the reward\n",
        "        \n",
        "        reward = np.expand_dims(reward,axis=1)\n",
        "        return reward\n",
        "    \n",
        "    def get_reset_state(self, n): # Nawid - Resets the state\n",
        "        return torch.FloatTensor(n, 4).uniform_(-0.05, 0.05)\n",
        "\n",
        "    def set_new_state(self, state): # Nawid - Used to manually change the state if necessary\n",
        "        self.state = state \n",
        "\n",
        "    def step(self, action, *args, **kwargs):\n",
        "        next_state, reward, done, info = super().step(action, *args, **kwargs)\n",
        "        if done:\n",
        "            reward = 0\n",
        "        return np.float32(next_state), reward, done, info\n",
        "    \n",
        "    def predict_states(self, states, actions): # Nawid - Obtains the next state, this is obtained from the step function\n",
        "        x, x_dot, theta, theta_dot = states[:,0], states[:,1],states[:,2],states[:,3]\n",
        "        force = np.array([self.force_mag if action == 1 else -self.force_mag for action in actions])\n",
        "        #force = self.force_mag if action==1 else -self.force_mag\n",
        "        cos_vectorised = np.vectorize(math.cos) # Nawid-  Used to vectorise the math.cos function which usually takes one input\n",
        "        sin_vectorised = np.vectorize(math.sin)\n",
        "        costheta = cos_vectorised(theta)\n",
        "        sintheta = sin_vectorised(theta)\n",
        "        #costheta = math.cos(theta)\n",
        "        #sintheta = math.sin(theta)\n",
        "        \n",
        "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
        "        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "        if self.kinematics_integrator == 'euler':\n",
        "            x  = x + self.tau * x_dot\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "        else: # semi-implicit euler\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            x  = x + self.tau * x_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "        \n",
        "        \n",
        "        next_states = np.array((x,x_dot,theta,theta_dot)) # Nawid - Joins together the different (200,) arrays to get a (4,200) \n",
        "        next_states = np.transpose(next_states) # Nawid - Outputs 200,4 shape\n",
        "        \n",
        "        return next_states\n",
        "\n",
        "    def eval(self): # Nawid - Does nothing but is a function which is consistent with neural network models\n",
        "        pass\n",
        "    \n",
        "    def train(self): # Nawid - Does nothing but is a function which is consistent with the neural network models\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N20KY3wnoTwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(i):\n",
        "    a = np.zeros(n_actions, 'uint8')\n",
        "    a[i] = 1\n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI9-_LCU3oS2",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QifBilQL3fI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NNDynamicsModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the next state, given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, obs_output_dim):\n",
        "        super(NNDynamicsModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, obs_output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "class NNRewardModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the reward given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, reward_output_dim):\n",
        "        super(NNRewardModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, reward_output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "def model_MSEloss(y_pred,y_truth, device):\n",
        "    '''\n",
        "    Compute the MSE (Mean Squared Error)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))\n",
        "\n",
        "def model_CEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the CEloss\n",
        "    '''\n",
        "    y_truth = torch.Tensor(np.array(y_truth)).to(device)\n",
        "    return F.cross_entropy(y_pred, y_truth)\n",
        "\n",
        "def model_BCEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the BCE (Binary cross entropy)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.binary_cross_entropy(y_pred.view(-1).float(), y_truth.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaWRYpeo3sC9",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ-poadG-T7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_random_trajectories(num_traj,env_name):\n",
        "    '''\n",
        "    Run num_traj random trajectories to gather information about the next state and reward.\n",
        "    Data used to train the models in a supervised way.\n",
        "    '''\n",
        "    dataset_random = []\n",
        "    env = gym.make(env_name)\n",
        "    game_rewards = []\n",
        "    for n in range(num_traj):\n",
        "\n",
        "        obs = env.reset()\n",
        "        while True:\n",
        "            sampled_action = env.action_space.sample()\n",
        "            sampled_action_one_hot = one_hot(sampled_action)\n",
        "            new_obs, reward, done, _ = env.step(sampled_action)\n",
        "            \n",
        "\n",
        "            dataset_random.append([obs, new_obs, reward, done, sampled_action_one_hot])\n",
        "\n",
        "            obs = new_obs\n",
        "            game_rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    # print some stats\n",
        "    print('Mean R:',np.round(np.sum(game_rewards)/num_traj,2), 'Max R:', np.round(np.max(game_rewards),2), np.round(len(game_rewards)/num_traj))\n",
        "\n",
        "    return dataset_random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzpINzuiwC0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_data(random_dataset, rl_dataset,num_examples_added):\n",
        "    if len(rl_dataset) > 0:\n",
        "        '''\n",
        "        # To use only a fraction of the random dataset\n",
        "        rand = np.arange(len(random_dataset))\n",
        "        np.random.shuffle(rand)\n",
        "        rand = rand[:int(len(rl_dataset)*0.8)] # 80% of rl dataset\n",
        "        d_concat = np.concatenate([np.array(random_dataset)[rand], rl_dataset], axis=0)'''        \n",
        "        random_dataset = np.array(random_dataset)\n",
        "        rl_dataset = np.array(rl_dataset)\n",
        "        print('random_dataset', random_dataset.shape)\n",
        "        print('rl dataset', rl_dataset.shape)\n",
        "        \n",
        "        d_concat = np.concatenate([random_dataset, rl_dataset], axis=0)\n",
        "        print('d_concat',d_concat.shape)        \n",
        "    else:\n",
        "        d_concat = np.array(random_dataset)\n",
        "    \n",
        "    # Split the dataset into train(80%) and test(20%)\n",
        "    D_train = d_concat[:int(-num_examples_added*1/5)]\n",
        "    D_valid = d_concat[int(-num_examples_added*1/5):]\n",
        "\n",
        "    print(\"len(D):\", len(d_concat), 'len(Dtrain)', len(D_train))\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    sff = np.arange(len(D_train))\n",
        "    np.random.shuffle(sff)\n",
        "    D_train = D_train[sff]\n",
        "\n",
        "    # Create the input and output for the train\n",
        "    X_env_train = np.array([np.concatenate([obs,act]) for obs,_,_,_,act in D_train]) # Takes obs and action\n",
        "    # Env output\n",
        "    y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "    y_env_train = y_env_train - np.array([obs for obs,_,_,_,_ in D_train]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "    # Reward's output\n",
        "    \n",
        "    X_rew_train = np.array([np.concatenate([obs,act,no]) for obs,no,_,_,act in D_train]) # Takes obs and action and next state\n",
        "    y_rew_train = np.array([[D] for _,_,_,D,_ in D_train])\n",
        "    #y_rew_train = np.array([[rw] for _,_,rw,_,_ in D_train])\n",
        "    \n",
        "    # Create the input and output array for the validation\n",
        "    X_env_val = np.array([np.concatenate([obs,act]) for obs,_,_,_,act in D_valid]) # Takes obs and action\n",
        "    X_rew_val = np.array([np.concatenate([obs,act,no]) for obs,no,_,_,act in D_valid]) # Takes obs and action\n",
        "    \n",
        "    # Reward output\n",
        "    y_rew_val = np.array([[D] for _,_,_,D,_ in D_valid])\n",
        "    #y_rew_val = np.array([[rw] for _,_,rw,_,_ in D_valid])\n",
        "\n",
        "    # Next state output\n",
        "    y_env_val = np.array([no for _,no,_,_,_ in D_valid])\n",
        "    y_env_val = y_env_val - np.array([obs for obs,_,_,_,_ in D_valid]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "    env_train_data, env_val_data = (X_env_train, y_env_train), (X_env_val, y_env_val) \n",
        "    rew_train_data, rew_val_data = (X_rew_train, y_rew_train), (X_rew_val, y_rew_val) \n",
        "\n",
        "    return env_train_data, env_val_data, rew_train_data, rew_val_data "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAy-fXpVzsPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalise_data(env_train, env_val, rew_train,rew_val):\n",
        "    (X_env_train, y_env_train), (X_env_val, y_env_val) =  env_train, env_val\n",
        "    (X_rew_train, y_rew_train), (X_rew_val, y_rew_val) =  rew_train, rew_val\n",
        "    env_input_scaler = StandardScaler()\n",
        "    \n",
        "    # Standard the input features by removign the mean and scaling to unit variance\n",
        "    X_env_train = env_input_scaler.fit_transform(X_env_train)\n",
        "    X_env_val = env_input_scaler.transform(X_env_val)\n",
        "\n",
        "    # Standardize the outputs by removing the mean and scaling to unit variance\n",
        "    env_output_scaler = StandardScaler()\n",
        "    y_env_train = env_output_scaler.fit_transform(y_env_train)\n",
        "    y_env_val = env_output_scaler.transform(y_env_val)\n",
        "\n",
        "    # Standardise input to reward model\n",
        "    rew_input_scaler = StandardScaler()\n",
        "    X_rew_train = rew_input_scaler.fit_transform(X_rew_train)\n",
        "    X_rew_train = rew_input_scaler.transform(X_rew_train)\n",
        "\n",
        "    # Standardise output of reward model\n",
        "    rew_output_scaler = StandardScaler()\n",
        "    y_rew_train = rew_output_scaler.fit_transform(y_rew_train)\n",
        "    y_rew_val = rew_output_scaler.transform(y_rew_val)\n",
        "    norm = (env_input_scaler, env_output_scaler, rew_input_scaler, rew_output_scaler)\n",
        "\n",
        "    norm_env_train_data, norm_env_val_data = (X_env_train, y_env_train), (X_env_val, y_env_val) \n",
        "    norm_rew_train_data, norm_rew_val_data = (X_rew_train, y_rew_train), (X_rew_val, y_rew_val)\n",
        "\n",
        "    return norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oxh7fJcsMZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalise_v2(env_train,env_val,rew_train,rew_val,norm):\n",
        "    (X_env_train, y_env_train), (X_env_val, y_env_val) =  env_train, env_val\n",
        "    (X_rew_train, y_rew_train), (X_rew_val, y_rew_val) =  rew_train, rew_val\n",
        "    \n",
        "    env_input_scaler, env_output_scaler, rew_input_scaler, rew_output_scaler =norm\n",
        "\n",
        "    X_env_train = env_input_scaler.transform(X_env_train)\n",
        "    X_env_val = env_input_scaler.transform(X_env_val)\n",
        "\n",
        "    y_env_train = env_output_scaler.transform(y_env_train)\n",
        "    y_env_val = env_output_scaler.transform(y_env_val)\n",
        "\n",
        "    # Standardise input to reward model\n",
        "    \n",
        "    X_rew_train = rew_input_scaler.transform(X_rew_train)\n",
        "    X_rew_train = rew_input_scaler.transform(X_rew_train)\n",
        "\n",
        "    # Standardise output of reward model\n",
        "    y_rew_train = rew_output_scaler.transform(y_rew_train)\n",
        "    y_rew_val = rew_output_scaler.transform(y_rew_val)\n",
        "\n",
        "    norm_env_train_data, norm_env_val_data = (X_env_train, y_env_train), (X_env_val, y_env_val) \n",
        "    norm_rew_train_data, norm_rew_val_data = (X_rew_train, y_rew_train), (X_rew_val, y_rew_val)\n",
        "\n",
        "    return norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiO0iRrMnKPT",
        "colab_type": "text"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32TmxYsJuTPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_data,val_data,model,batch_size, max_model_iter, optimizer, device,early_stopper,desired_model='Env_model'):\n",
        "    ''' \n",
        "    General function to train either of the two models\n",
        "    '''\n",
        "\n",
        "    (X_train, y_train), (X_val, y_val) =  train_data, val_data\n",
        "    losses_env = []\n",
        "    if desired_model =='Env_model': # Nawid -  Decides which loss function to use\n",
        "        loss_function = model_MSEloss\n",
        "    else:\n",
        "        loss_function = model_BCEloss\n",
        "\n",
        "    # go through max_model iter supervised iterations\n",
        "    for it in tqdm(range(max_model_iter)):\n",
        "        # create mini batches of size batch_size\n",
        "        for mb in range(0,len(X_train), batch_size): # Nawid- Batch size is the step size\n",
        "            if len(X_train) > mb + BATCH_SIZE:\n",
        "                X_mb = X_train[mb:mb+BATCH_SIZE]\n",
        "                y_mb = y_train[mb:mb+BATCH_SIZE]\n",
        "                #X_mb += np.random.normal(loc = 0, scale = 0.001, size= X_mb.shape)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # forward pass of model to compute the output\n",
        "                pred_mb = model(torch.tensor(X_mb).to(device))\n",
        "                \n",
        "\n",
        "                # compute the MSE loss\n",
        "                \n",
        "                loss = loss_function(pred_mb,y_mb,device) #Nawid-  Uses Mse loss if dynamics model or uses BCE loss if reward/done model\n",
        "                wandb.log({'{} Training loss'.format(desired_model):loss.cpu().detach().numpy()})\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                # optimization step\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Nawid - Calculate the validation loss after each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_val = model(torch.tensor(X_val).to(device))\n",
        "            val_loss = loss_function(pred_val, y_val,device)\n",
        "            wandb.log({'{} Validation loss'.format(desired_model):val_loss})\n",
        "        model.train()\n",
        "\n",
        "        early_stopper(val_loss,model)\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfobpJ8430dy",
        "colab_type": "text"
      },
      "source": [
        "# Controller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY7CF1xBTVL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_model_based_control(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, norm,device):\n",
        "    '''\n",
        "    Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "    '''\n",
        "    best_reward = -1e9\n",
        "    best_next_action = []\n",
        "    \n",
        "    (env_input_scaler, env_output_scaler, rew_input_scaler, rew_output_scaler) =norm\n",
        "    m_obs = np.array([real_obs for _ in range(num_sequences)])\n",
        "    \n",
        "    # array that contains the rewards for all the sequence\n",
        "    unroll_rewards = np.zeros((num_sequences, 1)) \n",
        "    first_sampled_actions = []\n",
        "\n",
        "    env_model.eval()\n",
        "    rew_model.eval()\n",
        "\n",
        "    # Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon length' times\n",
        "    ## i.e roll a given number of trajectories in a single batch (to increase speed)\n",
        "    \n",
        "    for t in range(horizon_length):\n",
        "        # sample actions for each sequence\n",
        "        sampled_actions = np.array([sample_action() for _ in range(num_sequences)])\n",
        "        sampled_actions_one_hot = np.array([one_hot(action) for action in sampled_actions])\n",
        "\n",
        "        if isinstance(env_model,NNDynamicsModel): # Nawid-  If the env model is a neural net\n",
        "            env_model_input = env_input_scaler.transform(np.concatenate([m_obs, sampled_actions_one_hot], axis = 1))\n",
        "            # compute the next state for each sequence\n",
        "            pred_obs = env_model(torch.tensor(env_model_input).to(device))\n",
        "\n",
        "            # inverse scaler transformation\n",
        "            pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
        "            # add previous observation\n",
        "            next_obs = pred_obs + m_obs\n",
        "        else:            \n",
        "            next_obs = env_model.predict_states(m_obs,sampled_actions) # Nawid - Able to obtain the next state directly rather than predicting a change in states\n",
        "\n",
        "        if isinstance(rew_model, NNRewardModel):\n",
        "            rew_model_input = rew_input_scaler.transform(np.concatenate([m_obs, sampled_actions_one_hot, next_obs], axis = 1))\n",
        "            pred_rew = rew_model(torch.tensor(rew_model_input).to(device)) # Nawid -  I believe I do not need to rescale for a True of false situation\n",
        "            unroll_rewards +=  (1 - pred_rew.cpu().detach().numpy())\n",
        "        else:\n",
        "            pred_rew = rew_model.predict_reward(m_obs, sampled_actions,next_obs)\n",
        "            unroll_rewards += pred_rew\n",
        "\n",
        "        m_obs = next_obs # Nawid - Update the state after calculating the new state and calculating the rewards\n",
        "    \n",
        "        if t ==0:\n",
        "            first_sampled_actions = sampled_actions\n",
        "        \n",
        "    env_model.train()\n",
        "    rew_model.train()\n",
        "\n",
        "    # Best the position of the sequence with the higher reward\n",
        "    arg_best_reward = np.argmax(unroll_rewards)\n",
        "    best_sum_reward = unroll_rewards[arg_best_reward].squeeze()\n",
        "    # take the first action of this sequence\n",
        "    best_action = first_sampled_actions[arg_best_reward]\n",
        "    #best_action =  np.squeeze(best_action)\n",
        "    return best_action, best_sum_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAAsmK644FqA",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4waS5AN0_UFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Main loop hyperp\n",
        "AGGR_ITER = 10\n",
        "STEPS_PER_AGGR = 500\n",
        "\n",
        "# Random MB hyperp\n",
        "NUM_RAND_TRAJECTORIES = 1000\n",
        "\n",
        "# 'cuda' or 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Supervised Model Hyperp\n",
        "ENV_LEARNING_RATE = 1e-4\n",
        "REW_LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1024\n",
        "TRAIN_ITER_MODEL =  30\n",
        "\n",
        "# Controller Hyperp\n",
        "HORIZON_LENGTH = 5\n",
        "NUM_ACTIONS_SEQUENCES = 200\n",
        "\n",
        "n_actions = 2\n",
        "reward_dim = 1\n",
        "\n",
        "# Time and date information\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n",
        "\n",
        "# Decides whether to use a dynamics model or\n",
        "nn_dynamics = True\n",
        "nn_reward = True\n",
        "\n",
        "# Making the network deterministic - https://pytorch.org/docs/stable/notes/randomness.html\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgwXUSfqMQrY",
        "colab_type": "code",
        "outputId": "8792fbc6-2cdd-4136-f2ff-37873d6cf1e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "project_name = 'CartPole'\n",
        "wandb.init(entity=\"nerdk312\", project= project_name)\n",
        "\n",
        "config = wandb.config\n",
        "config.batch_size = BATCH_SIZE          \n",
        "config.horizon_length = HORIZON_LENGTH\n",
        "config.num_action_seq = NUM_ACTIONS_SEQUENCES\n",
        "config.train_model_iter = TRAIN_ITER_MODEL \n",
        "config.num_rand_trajectories = NUM_RAND_TRAJECTORIES\n",
        "config.aggr_iter = AGGR_ITER\n",
        "config.steps_per_aggr = STEPS_PER_AGGR\n",
        "config.env_lr = ENV_LEARNING_RATE\n",
        "config.rew_lr = REW_LEARNING_RATE\n",
        "config.no_actions = n_actions"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/CartPole\" target=\"_blank\">https://app.wandb.ai/nerdk312/CartPole</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/CartPole/runs/3aac6brn\" target=\"_blank\">https://app.wandb.ai/nerdk312/CartPole/runs/3aac6brn</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIqN1db037uY",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4HZZVf4_7yv",
        "colab_type": "text"
      },
      "source": [
        "Setting and training of dynamics model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS4DnaTJ_2cF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20ab9ea5-5fbe-41d9-918a-581be2bd72c7"
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "rand_dataset = gather_random_trajectories(NUM_RAND_TRAJECTORIES,ENV_NAME)\n",
        "rl_dataset = []\n",
        "num_examples_added = len(rand_dataset)\n",
        "env_train, env_val, rew_train, rew_val = collate_data(rand_dataset,rl_dataset,num_examples_added)\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, norm = normalise_data(env_train, env_val, rew_train,rew_val)\n",
        "\n",
        "# Initialize the models\n",
        "if nn_dynamics:\n",
        "    env_model = NNDynamicsModel(n_actions + env.observation_space.shape[0], env.observation_space.shape[0]).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "    env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "    wandb.watch(env_model, log=\"all\")\n",
        "    env_model_name = 'Env_model'+ '_' + date_time\n",
        "    early_stopping_env = EarlyStopping(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "\n",
        "    for n_iter in range(AGGR_ITER):\n",
        "        if early_stopping_env.early_stop:\n",
        "            print('Early stopping')\n",
        "            break\n",
        "        train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)\n",
        "    else:\n",
        "        env_model = CartPoleTask() # Nawid - Using the oracle for the state"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean R: 22.28 Max R: 1.0 22.0\n",
            "len(D): 22281 len(Dtrain) 17825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 2/30 [00:00<00:14,  1.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (100000000000.000000 --> 0.232162).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.232162 --> 0.017513).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 4/30 [00:01<00:08,  2.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.017513 --> 0.010677).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.010677 --> 0.005642).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 5/30 [00:01<00:07,  3.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.005642 --> 0.004633).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 6/30 [00:01<00:06,  3.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.004633 --> 0.003831).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 8/30 [00:01<00:05,  4.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.003831 --> 0.003169).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.003169 --> 0.002727).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 10/30 [00:02<00:04,  4.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.002727 --> 0.002398).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.002398 --> 0.002134).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 11/30 [00:02<00:04,  4.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.002134 --> 0.001931).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 12/30 [00:02<00:03,  4.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001931 --> 0.001764).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 13/30 [00:02<00:03,  4.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001764 --> 0.001624).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 14/30 [00:03<00:03,  4.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001624 --> 0.001508).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001508 --> 0.001408).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 16/30 [00:03<00:02,  4.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001408 --> 0.001322).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001322 --> 0.001248).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 19/30 [00:04<00:02,  5.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001248 --> 0.001186).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001186 --> 0.001133).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|███████   | 21/30 [00:04<00:01,  5.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001133 --> 0.001086).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001086 --> 0.001046).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 22/30 [00:04<00:01,  4.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001046 --> 0.001010).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|███████▋  | 23/30 [00:04<00:01,  4.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.001010 --> 0.000978).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 24/30 [00:05<00:01,  4.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000978 --> 0.000948).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 25/30 [00:05<00:01,  4.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000948 --> 0.000920).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 27/30 [00:05<00:00,  4.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000920 --> 0.000895).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000895 --> 0.000871).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 28/30 [00:06<00:00,  4.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000871 --> 0.000850).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 97%|█████████▋| 29/30 [00:06<00:00,  4.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000850 --> 0.000831).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [00:06<00:00,  4.65it/s]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000831 --> 0.000814).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000814 --> 0.000799).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 3/30 [00:00<00:05,  5.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000799 --> 0.000786).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000786 --> 0.000775).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 5/30 [00:00<00:04,  5.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000775 --> 0.000764).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000764 --> 0.000754).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 7/30 [00:01<00:04,  5.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000754 --> 0.000745).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000745 --> 0.000737).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 8/30 [00:01<00:04,  5.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000737 --> 0.000729).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 9/30 [00:01<00:04,  4.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000729 --> 0.000722).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000722 --> 0.000715).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 11/30 [00:02<00:03,  5.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000715 --> 0.000709).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 12/30 [00:02<00:03,  4.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000709 --> 0.000704).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 13/30 [00:02<00:03,  4.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000704 --> 0.000700).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 14/30 [00:02<00:03,  4.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000700 --> 0.000695).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 15/30 [00:03<00:03,  4.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000695 --> 0.000691).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000691 --> 0.000688).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 17/30 [00:03<00:02,  4.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000688 --> 0.000685).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 18/30 [00:03<00:02,  4.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000685 --> 0.000682).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000682 --> 0.000679).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|███████   | 21/30 [00:04<00:01,  4.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000679 --> 0.000676).  Saving model ...\n",
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000676 --> 0.000673).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 22/30 [00:04<00:01,  4.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000673 --> 0.000671).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|███████▋  | 23/30 [00:04<00:01,  4.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000671 --> 0.000668).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 24/30 [00:04<00:01,  4.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000668 --> 0.000667).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 25/30 [00:05<00:01,  4.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000667 --> 0.000666).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 26/30 [00:05<00:00,  4.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000666 --> 0.000666).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 27/30 [00:05<00:00,  4.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000666 --> 0.000665).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 28/30 [00:05<00:00,  4.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_25_18.33.35  (0.000665 --> 0.000665).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [00:06<00:00,  4.94it/s]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_25_18.33.35 counter: 1 out of 5\n",
            "EarlyStopping for Env_model_25_18.33.35 counter: 2 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 1/30 [00:00<00:05,  5.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_25_18.33.35 counter: 3 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 2/30 [00:00<00:05,  4.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_25_18.33.35 counter: 4 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 2/30 [00:00<00:08,  3.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_25_18.33.35 counter: 5 out of 5\n",
            "Env_model_25_18.33.35 has stopped\n",
            "Early stopping\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C627Ba3aAF4K",
        "colab_type": "text"
      },
      "source": [
        "Setting and Training of reward model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRqBYvwT_jMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65cd2450-92c7-47bf-9b27-a267b4e25943"
      },
      "source": [
        "if nn_reward:\n",
        "    rew_model = NNRewardModel(n_actions + (2*env.observation_space.shape[0]),reward_dim).to(device)\n",
        "    rew_optimizer = torch.optim.Adam(rew_model.parameters(), REW_LEARNING_RATE)\n",
        "    wandb.watch(rew_model, log=\"all\")\n",
        "    desired_model = 'Reward_model'\n",
        "    model_name = desired_model +'_'+ date_time\n",
        "    early_stopping_reward = EarlyStopping(patience=5, verbose=True, wandb=wandb, name=model_name)\n",
        "else:\n",
        "    rew_model = CartPoleTask()\n",
        "\n",
        "game_reward = 0\n",
        "\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_reward.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    \n",
        "    env_train, env_val, rew_train, rew_val = collate_data(rand_dataset,rl_dataset,num_examples_added)# Nawid- Updates the amount of data present\n",
        "    norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data = normalise_v2(env_train,env_val,rew_train,rew_val,norm)\n",
        "\n",
        "    #supervised training of the dataset (random and rl if it exists)\n",
        "    \n",
        "    train_model(rew_train, rew_val, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL,rew_optimizer,device,early_stopping_reward,desired_model = 'Rew')\n",
        "    obs = env.reset()\n",
        "\n",
        "    num_examples_added = 0\n",
        "    game_reward = 0\n",
        "    controller_pred_rews = []\n",
        "    rews = []\n",
        "    \n",
        "    while num_examples_added < STEPS_PER_AGGR:\n",
        "        while True:\n",
        "            tt = time.time()\n",
        "            # Execute the control to roll the sequences and pick the first action of the sequence with the higher reward\n",
        "            action, pred_rew = multi_model_based_control(env_model, rew_model, obs, NUM_ACTIONS_SEQUENCES, HORIZON_LENGTH, env.action_space.sample, norm, device)\n",
        "            action_one_hot = one_hot(action)\n",
        "            controller_pred_rews.append(pred_rew)\n",
        "\n",
        "            # one step in the environment with the action returned by the controller\n",
        "            new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "            # add the last step to the RL dataset\n",
        "            rl_dataset.append([obs, new_obs, reward, done, action_one_hot])\n",
        "            num_examples_added += 1\n",
        "            obs = new_obs\n",
        "            game_reward += reward\n",
        "\n",
        "            # if the environment is done, reset it and print some stats\n",
        "            if done:\n",
        "                obs = env.reset()\n",
        "                print('  >> R: {:.2f}, Mean sum:{:.2f}, {}'.format(game_reward, np.mean(controller_pred_rews), num_examples_added))\n",
        "\n",
        "                rews.append(game_reward)\n",
        "                game_reward = 0\n",
        "                controller_pred_rews = []\n",
        "                break\n",
        "\n",
        "    print('  >> Mean: {:.2f}', np.mean(rews))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "len(D): 22281 len(Dtrain) 17825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 1/30 [00:00<00:15,  1.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (100000000000.000000 --> 0.604639).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 2/30 [00:00<00:12,  2.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.604639 --> 0.435313).  Saving model ...\n",
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.435313 --> 0.328882).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 4/30 [00:01<00:08,  3.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.328882 --> 0.274078).  Saving model ...\n",
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.274078 --> 0.244181).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 6/30 [00:01<00:06,  3.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.244181 --> 0.222866).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 7/30 [00:01<00:05,  3.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.222866 --> 0.204105).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 8/30 [00:02<00:05,  4.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.204105 --> 0.187799).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 9/30 [00:02<00:05,  4.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.187799 --> 0.175449).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 10/30 [00:02<00:04,  4.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.175449 --> 0.163989).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 11/30 [00:02<00:04,  4.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.163989 --> 0.154075).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 12/30 [00:02<00:04,  4.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.154075 --> 0.145400).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 13/30 [00:03<00:04,  4.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.145400 --> 0.138569).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 14/30 [00:03<00:03,  4.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.138569 --> 0.131808).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 15/30 [00:03<00:03,  4.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.131808 --> 0.126133).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 16/30 [00:03<00:03,  4.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.126133 --> 0.121043).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 57%|█████▋    | 17/30 [00:04<00:02,  4.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.121043 --> 0.116886).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 18/30 [00:04<00:02,  4.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.116886 --> 0.111910).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 63%|██████▎   | 19/30 [00:04<00:02,  4.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.111910 --> 0.108094).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 20/30 [00:04<00:02,  4.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.108094 --> 0.104443).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 21/30 [00:05<00:02,  4.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.104443 --> 0.101016).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 22/30 [00:05<00:01,  4.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.101016 --> 0.098005).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|███████▋  | 23/30 [00:05<00:01,  4.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.098005 --> 0.095285).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 24/30 [00:05<00:01,  4.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.095285 --> 0.092762).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 25/30 [00:05<00:01,  4.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.092762 --> 0.090364).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 26/30 [00:06<00:00,  4.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.090364 --> 0.088095).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 27/30 [00:06<00:00,  4.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.088095 --> 0.085885).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 28/30 [00:06<00:00,  4.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.085885 --> 0.083912).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 97%|█████████▋| 29/30 [00:06<00:00,  4.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.083912 --> 0.081617).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [00:07<00:00,  4.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.081617 --> 0.079756).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  >> R: 200.00, Mean sum:4.58, 200\n",
            "  >> R: 191.00, Mean sum:4.23, 391\n",
            "  >> R: 200.00, Mean sum:4.74, 591\n",
            "  >> Mean: {:.2f} 197.0\n",
            "random_dataset (22281, 5)\n",
            "rl dataset (591, 5)\n",
            "d_concat (22872, 5)\n",
            "len(D): 22872 len(Dtrain) 22754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 1/30 [00:00<00:08,  3.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.079756 --> 0.053798).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 2/30 [00:00<00:08,  3.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.053798 --> 0.053115).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 3/30 [00:00<00:08,  3.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.053115 --> 0.052361).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 4/30 [00:01<00:07,  3.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.052361 --> 0.051952).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 5/30 [00:01<00:07,  3.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.051952 --> 0.051515).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 6/30 [00:01<00:07,  3.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.051515 --> 0.051293).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 7/30 [00:02<00:06,  3.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.051293 --> 0.051163).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 8/30 [00:02<00:06,  3.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.051163 --> 0.051109).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 9/30 [00:02<00:06,  3.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Reward_model_25_18.33.35  (0.051109 --> 0.050887).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 10/30 [00:02<00:05,  3.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_25_18.33.35 counter: 1 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 11/30 [00:03<00:05,  3.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_25_18.33.35 counter: 2 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 12/30 [00:03<00:05,  3.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_25_18.33.35 counter: 3 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 13/30 [00:03<00:04,  3.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_25_18.33.35 counter: 4 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 13/30 [00:04<00:05,  3.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_25_18.33.35 counter: 5 out of 5\n",
            "Reward_model_25_18.33.35 has stopped\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  >> R: 200.00, Mean sum:4.90, 200\n",
            "  >> R: 200.00, Mean sum:4.94, 400\n",
            "  >> R: 200.00, Mean sum:4.91, 600\n",
            "  >> Mean: {:.2f} 200.0\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY_2Eg5jyDrs",
        "colab_type": "text"
      },
      "source": [
        "Experimental notes\n",
        "\n",
        "*   Normalising the target values for the dones seems to make the loss worse, \n",
        "*   This is likely due to normalising causes the target values to shift from the 0-1 scale onto a different scale and this can lead to issues when using a binary cross entropy loss which expects values between 0 and 1 (and can lead to a negative loss value)"
      ]
    }
  ]
}