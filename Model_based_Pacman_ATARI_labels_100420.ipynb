{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Model_based_Pacman_ATARI_labels_100420.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PpGt1rdvsLFT"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMXZpFOi5mOsVwuCRa0Wcdh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/Model_based_Pacman_ATARI_labels_100420.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4jLAGgyjflE",
        "colab_type": "text"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "64b157b3-fd11-49cf-dd47-6c2a97dcf2b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/mila-iqia/atari-representation-learning.git\n",
            "  Cloning git://github.com/mila-iqia/atari-representation-learning.git to /tmp/pip-req-build-smqnc_3x\n",
            "  Running command git clone -q git://github.com/mila-iqia/atari-representation-learning.git /tmp/pip-req-build-smqnc_3x\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->atariari==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: atariari\n",
            "  Building wheel for atariari (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atariari: filename=atariari-0.0.1-cp36-none-any.whl size=46584 sha256=a6b5167e59896dfe51ad8edc56b594ddcd9e177e150631a5aafe57a0ad3b67b8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pxqb3gsl/wheels/3d/69/51/5e436e5ae566c5b4dec5c53e65396d516459877a42a11d7aa4\n",
            "Successfully built atariari\n",
            "Installing collected packages: atariari\n",
            "Successfully installed atariari-0.0.1\n",
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-5blo0z67\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-5blo0z67\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=7c009d7dfcaf0cdc1ecf02029b8d333fffb911ef60d3ce92b187b38a38699849\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u22ir9an/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-z4zwzyn2\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-z4zwzyn2\n",
            "Collecting gym<0.16.0,>=0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/01/8771e8f914a627022296dab694092a11a7d417b6c8364f0a44a8debca734/gym-0.15.7.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.38.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines, gym\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=43c3fb6cb46320d2502bf6bb5fd65db594d09aeced48b5cb691018ce2908f8ae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8usinpv2/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-cp36-none-any.whl size=1648840 sha256=b5319df44a21083847426f9ead356e99ffb02e483b89554ab284d3dac05b740b\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/60/6a/f9c27ae133abaf5a5687ed2fa8ed19627d7fac5d843a27572b\n",
            "Successfully built baselines gym\n",
            "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gym, baselines\n",
            "  Found existing installation: gym 0.17.1\n",
            "    Uninstalling gym-0.17.1:\n",
            "      Successfully uninstalled gym-0.17.1\n",
            "Successfully installed baselines-0.1.6 gym-0.15.7\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/18/ef5215832f523c29f6e0c19a5b87e0dd90fe40fb48ba38362f961be14e4f/wandb-0.8.31-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 48.7MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.8MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.1)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
            "\u001b[?25hCollecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: gql, subprocess32, watchdog, graphql-core, pathtools\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=f51ac29a75da65b5df5adaa8fbeed1234a8608d171d35b0ea45570644e3daf08\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=20d0d00af4cd375727fa3d2c010924ba5e42e3dd9d49b22e0633d71fbf570198\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=cef370dfe44d0ee06fbd96024cf0a03129732109cd9831256564de1e5b0f816a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=9fd91e6ebe8a7c6b1dd62785447643b7fc29abcb833b9962020132cc0352ea5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=3c1891ef18d6c646c29b3d5c5a8bc9667689e3532d0c9d52998cb617f309711f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built gql subprocess32 watchdog graphql-core pathtools\n",
            "Installing collected packages: docker-pycreds, graphql-core, gql, smmap, gitdb, GitPython, subprocess32, pathtools, watchdog, configparser, shortuuid, sentry-sdk, wandb\n",
            "Successfully installed GitPython-3.1.0 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.2 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.1 subprocess32-3.5.4 wandb-0.8.31 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZ9gMrxp5vU",
        "colab_type": "code",
        "outputId": "286f58df-0045-401d-e63d-07ad627ef38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI287_Yjxn3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, BatchSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from atariari.benchmark.envs import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from atariari.benchmark.wrapper import AtariARIWrapper\n",
        "\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Imported required for the Model-based RL\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "169a22d4-8ac4-4f75-ba09-262ffa6a5b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login ##########"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gdFb0bqHHN",
        "colab_type": "code",
        "outputId": "d473d91b-4789-4002-a72d-bc37f01736d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "wandb.init(entity=\"nerdk312\", project=\"ATARI_LABELS_MPC\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/zu69bddh\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/zu69bddh</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/zu69bddh"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnrvnbMcP6Xz",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9wLqaPmqqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ENV_NAME = 'MsPacmanNoFrameskip-v4'\n",
        "ENV_NAME = 'MsPacmanDeterministic-v4'\n",
        "feature_size = 10\n",
        "#feature_size = all_defaults['feature_size'] # Nawid- Dimensionality of the representation\n",
        "# workers = 8 # Nawid - Choosing the number of workers for the network\n",
        "\n",
        "# Main loop hyperp\n",
        "env_model_pretrain =  True\n",
        "rew_model_pretrain = True\n",
        "if env_model_pretrain:\n",
        "    pretrained_env_model = '/content/gdrive/My Drive/MsPacman-data/Env_model_4_9.47.41.pt'\n",
        "\n",
        "if rew_model_pretrain:\n",
        "    pretrained_rew_model = '/content/gdrive/My Drive/MsPacman-data/Rew_model_1000_randtraj_lr1e-4.pt'\n",
        "    AGGR_ITER = 10\n",
        "else:\n",
        "    AGGR_ITER = 10\n",
        "\n",
        "STEPS_PER_AGGR = 100\n",
        "\n",
        "# Random MB hyperp\n",
        "NUM_RAND_TRAJECTORIES = 1000\n",
        "\n",
        "# 'cuda' or 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Supervised Model Hyperp\n",
        "ENV_LEARNING_RATE = 1e-3\n",
        "REW_LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1024\n",
        "TRAIN_ITER_MODEL =  100\n",
        "\n",
        "# Controller Hyperp\n",
        "HORIZON_LENGTH = 5\n",
        "NUM_ACTIONS_SEQUENCES = 200\n",
        "\n",
        "load_data = True\n",
        "if load_data:\n",
        "    loaded_trajectories = '/content/gdrive/My Drive/MsPacman-data/pacman_rand1000.npy'\n",
        "\n",
        "collect_data = False\n",
        "\n",
        "observation_channels = 1\n",
        "action_dim = 1\n",
        "n_actions = 5 #9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "reward_dim = 1\n",
        "\n",
        "# Time and date information\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n",
        "\n",
        "# Making the network deterministic - https://pytorch.org/docs/stable/notes/randomness.html\n",
        "random_seed = 0\n",
        "\n",
        "if random_seed:    \n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    #np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKm_FKWGnPep",
        "colab_type": "text"
      },
      "source": [
        "# Saving config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5P-E36KnFoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = wandb.config\n",
        "config.batch_size = BATCH_SIZE          \n",
        "config.horizon_length = HORIZON_LENGTH\n",
        "config.num_action_seq = NUM_ACTIONS_SEQUENCES\n",
        "config.train_model_iter = TRAIN_ITER_MODEL \n",
        "config.num_rand_trajectories = NUM_RAND_TRAJECTORIES\n",
        "config.aggr_iter = AGGR_ITER\n",
        "config.steps_per_aggr = STEPS_PER_AGGR\n",
        "config.env_lr = ENV_LEARNING_RATE\n",
        "config.rew_lr = REW_LEARNING_RATE\n",
        "config.no_actions = n_actions\n",
        "config.load_data = load_data\n",
        "config.collect_data = collect_data\n",
        "config.env_model_pretrain = env_model_pretrain\n",
        "config.rew_model_pretrain = rew_model_pretrain\n",
        "config.random_seed = random_seed\n",
        "\n",
        "if load_data:\n",
        "    config.loaded_trajectories = loaded_trajectories\n",
        "\n",
        "if env_model_pretrain:\n",
        "    config.pretrained_env_model = pretrained_env_model\n",
        "if rew_model_pretrain:\n",
        "    config.pretrained_rew_model = pretrained_rew_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKVfcEFEz9-t",
        "colab_type": "text"
      },
      "source": [
        "# Model setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaBzmIUDnqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping_loss(object):\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, verbose=False, wandb=None, name=\"\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = 1e11 # Nawid - Set a very high initial best loss\n",
        "        self.name = name\n",
        "        self.wandb = wandb\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score >= self.best_score: # Nawid - Inverse signs to take into minimising loss instead of maximising accuracy\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping for {self.name} counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(f'{self.name} has stopped')\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f'Validation loss decreased/improved for {self.name}  ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "\n",
        "        save_dir = self.wandb.run.dir\n",
        "        torch.save(model.state_dict(), save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.wandb.save(save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGPEetVzTtOb",
        "colab": {}
      },
      "source": [
        "class NNDynamicsModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the next state, given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, obs_output_dim):\n",
        "        super(NNDynamicsModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, obs_output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "class NNRewardModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the reward given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, reward_output_dim):\n",
        "        super(NNRewardModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, reward_output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "def model_MSEloss(y_pred,y_truth, device):\n",
        "    '''\n",
        "    Compute the MSE (Mean Squared Error)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))\n",
        "\n",
        "def model_CEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the CEloss\n",
        "    '''\n",
        "    y_truth = torch.Tensor(np.array(y_truth)).to(device)\n",
        "    return F.cross_entropy(y_pred, y_truth)\n",
        "\n",
        "def model_BCEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the BCE (Binary cross entropy)\n",
        "    param y_pred: y_pred is a 2 dimensional n x 1 data tensor\n",
        "    param y_truth: y_truth is a 2D numpy array which later gets converted into a tensor\n",
        "    '''\n",
        "    n,c = y_pred.size()\n",
        "    weights = np.zeros((n,c))\n",
        "    pos = y_truth[y_truth==1].sum()\n",
        "    neg = n - pos\n",
        "    #nx = y_truth.cpu().data.numpy()\n",
        "    index = np.where(y_truth == 1)[0]\n",
        "    weights[:] = 1 #1/neg\n",
        "    weights[index] = neg* 1/pos\n",
        "    weights = torch.Tensor(weights).to(device)\n",
        "    #print('y_pred',y_pred.size())\n",
        "    #print('weights',weights.size())\n",
        "\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    #print('y_truth',y_truth.size())\n",
        "    return F.binary_cross_entropy(y_pred.view(-1).float(), y_truth.view(-1),weights.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRn5uXDhgmQs",
        "colab_type": "text"
      },
      "source": [
        "# General Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCHWWzZ_f5GV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(i):\n",
        "    a = np.zeros(n_actions, 'uint8')\n",
        "    a[i] = 1\n",
        "    return a\n",
        "\n",
        "def normalise(train_data, val_data, scaler = None): # Nawid - Used to normalise each dimension individually\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        train_data = scaler.fit_transform(train_data)\n",
        "    else: \n",
        "        train_data = scaler.transform(train_data)\n",
        "        \n",
        "    val_data = scaler.transform(val_data)\n",
        "    return train_data, val_data, scaler\n",
        "\n",
        "def normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data,X_env_obs_scaler = None,y_env_scaler=None,X_rew_obs_scaler= None):\n",
        "    # Unpack data\n",
        "    (X_env_train_obs, X_env_train_act, y_env_train), (X_env_val_obs, X_env_val_act, y_env_val) = env_train_data, env_val_data\n",
        "    (X_rew_train_obs, X_rew_train_act, y_rew_train), (X_rew_val_obs, X_rew_val_act, y_rew_val) = rew_train_data, rew_val_data\n",
        "    \n",
        "    # Normalise training and validation data\n",
        "    X_env_train_obs,X_env_val_obs, X_env_obs_scaler =  normalise(X_env_train_obs, X_env_val_obs, X_env_obs_scaler)\n",
        "    y_env_train, y_env_val, y_env_scaler = normalise(y_env_train, y_env_val, y_env_scaler)\n",
        "    X_rew_train_obs, X_rew_val_obs, X_rew_obs_scaler = normalise(X_rew_train_obs, X_rew_val_obs, X_rew_obs_scaler)\n",
        "    \n",
        "    # Concatentates the normalised states with the one hot vector for the actions\n",
        "    X_env_train = np.concatenate((X_env_train_obs,X_env_train_act),axis=1)\n",
        "    X_env_val = np.concatenate((X_env_val_obs,X_env_val_act),axis=1)\n",
        "    X_rew_train = np.concatenate((X_rew_train_obs,X_rew_train_act),axis=1)\n",
        "    X_rew_val = np.concatenate((X_rew_val_obs,X_rew_val_act),axis=1)\n",
        "\n",
        "    # Pack data tuples\n",
        "    env_train_data, env_val_data = (X_env_train, y_env_train),(X_env_val, y_env_val) \n",
        "    rew_train_data, rew_val_data = (X_rew_train, y_rew_train),(X_rew_val, y_rew_val)\n",
        "\n",
        "    return env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_obs_scaler, y_env_scaler, X_rew_obs_scaler\n",
        "\n",
        "#normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJgI07xT7eo",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzpOMJFWs9MH",
        "colab_type": "code",
        "outputId": "744a6055-6163-4fe8-927d-ff5f6b55b82e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "class Data_collection():\n",
        "    def __init__(self,ENV_NAME,feature_size,state_mode = 'ATARIARI', encoder = None):\n",
        "        self.ENV_NAME = ENV_NAME\n",
        "        self.state_mode = state_mode\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = encoder\n",
        "        self.repeated_initial = True # Nawid - set repeated initial as true initially\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def gather_random_trajectories(self,num_traj):\n",
        "        dataset_random = []\n",
        "        #Env name could either be the RAM case or the generic case\n",
        "        env = AtariARIWrapper(gym.make(self.ENV_NAME)) \n",
        "        i = 0\n",
        "        for n in range(num_traj):\n",
        "            if n % 10 ==0:\n",
        "                print('trajectory number :',n)\n",
        "            # Initial set up\n",
        "            obs = env.reset()\n",
        "            env.seed(0)\n",
        "\n",
        "            self.repeated_initial = True # Nawid- Used to represent the initial state\n",
        "            initial_info_labels = env.labels()\n",
        "            info_labels = env.labels() # Nawid -  Used to get the current state\n",
        "            #print('trajectory number',n)\n",
        "            \n",
        "            while True:\n",
        "                # Choosing action and env step\n",
        "                \n",
        "                sampled_action = np.random.randint(0,n_actions)\n",
        "                sampled_action_one_hot = one_hot(sampled_action)\n",
        "                next_obs, reward, done, next_info = env.step(sampled_action)\n",
        "                #print('done after env step', done)\n",
        "                next_info_labels = next_info['labels']\n",
        "                # self.repeated initial is set to true at first and it is is turned to true in the reward_collection class when done is true\n",
        "\n",
        "                if self.repeated_initial: # If the initial state is repeating\n",
        "                    if initial_info_labels == next_info_labels: # Current state is still the same as the initial state\n",
        "                        pass\n",
        "                    else:\n",
        "                        # New state achieved, so save data\n",
        "                        self.repeated_initial = False\n",
        "                    \n",
        "                        state, next_state = self.state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                        reward, pacman_done = self.reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                        #print('pacman_done', pacman_done)\n",
        "                        dataset_random.append([state, next_state,reward,pacman_done,sampled_action_one_hot])\n",
        "                    \n",
        "                        obs = next_obs\n",
        "                        info_labels = next_info_labels\n",
        "\n",
        "                else:\n",
        "                    # Save data as this is when the initial state and the next state are not identical\n",
        "                    state, next_state = self.state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                    reward, pacman_done = self.reward_collection(reward, done, info_labels, next_info_labels) # Nawid - Changed name to pacman done as the variable done is used to exit the loop\n",
        "                    dataset_random.append([state, next_state,reward,pacman_done,sampled_action_one_hot])\n",
        "                \n",
        "                    obs = next_obs\n",
        "                    info_labels = next_info_labels\n",
        "                    # Used to test whether the done behaviour was correct\n",
        "                    \n",
        "                    '''if pacman_done:\n",
        "                        i +=1 \n",
        "                        print('pacman done', i)'''\n",
        "                    \n",
        "\n",
        "                if done:\n",
        "                    break \n",
        "        '''               \n",
        "        #dataset_random = np.array(dataset_random)\n",
        "        first_dataset = np.array(dataset_random[0])\n",
        "        print(first_dataset[0])\n",
        "        print('zero',dataset_random[0])\n",
        "        print('One',dataset_random[1])\n",
        "        print('Two',dataset_random[2])\n",
        "        print('Three',dataset_random[3])\n",
        "        print('Four',dataset_random[4])\n",
        "        #print(dataset_random.shape)\n",
        "        '''\n",
        "        return dataset_random\n",
        "    \n",
        "\n",
        "    def state_collection(self,obs, next_obs, info_labels, next_info_labels):\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            state = []\n",
        "            next_state = []\n",
        "            i = 0 \n",
        "            #print(info_labels)\n",
        "            '''\n",
        "            for key in info_labels:\n",
        "                if key =='player_x' or key == 'player_y' or key == 'player_direction':\n",
        "                    state.append(info_labels[key])\n",
        "                    next_state.append(next_info_labels[key])\n",
        "                \n",
        "            state = np.array(state)\n",
        "            next_state = np.array(next_state)\n",
        "            return state, next_state\n",
        "            '''\n",
        "            \n",
        "            for key in info_labels :\n",
        "                if i < self.feature_size: # Nawid - Only the first 14 info are crucial I believe\n",
        "                    state.append(info_labels[key])\n",
        "                    next_state.append(next_info_labels[key])\n",
        "                    i +=1\n",
        "                else:\n",
        "                    state = np.array(state)\n",
        "                    next_state = np.array(next_state)\n",
        "                    return state, next_state\n",
        "            \n",
        "    \n",
        "        elif self.state_mode == 'STDIM': # Encoding is the state\n",
        "            assert self.encoder is not None\n",
        "            with torch.no_grad():\n",
        "                self.encoder.eval()\n",
        "                state = self.encoder(obs.float().to(self.device) / 255)\n",
        "                next_state = self.encoder(next_obs.float().to(self.device) / 255)\n",
        "                return state.cpu().detach().numpy(), next_state.cpu().detach().numpy()\n",
        "\n",
        "        else: # Image observations are the state or the RAM labels\n",
        "            return obs, next_obs\n",
        "\n",
        "    def reward_collection(self,reward,done,info_labels,next_info_labels):\n",
        "        # checks whether there has been a change in lives- change done to 1, otherwise the done should be fine regardless\n",
        "        if not info_labels['num_lives'] == next_info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "            #print('change in lives')\n",
        "            done = True \n",
        "            self.repeated_initial = True\n",
        "        return reward, done\n",
        "\n",
        "    def unison_shuffled_copies(self,*args): # Nawid- Randomises all the different values, using *args to use a variable number of parameters so i can shuffle many different values at once\n",
        "        p = np.random.permutation(len(args[0]))\n",
        "        shuffled = [i[p] for i in args]\n",
        "        return shuffled\n",
        "    \n",
        "    def collate_data(self,random_dataset, rl_dataset):\n",
        "        rand_data = np.array(random_dataset)\n",
        "        num_rand_examples = len(rand_data)\n",
        "        D_train = rand_data[:int(-num_rand_examples*1/5)] \n",
        "        D_valid = rand_data[int(-num_rand_examples*1/5):]\n",
        "        print(\"number random examples:\",num_rand_examples, 'len(D_train_rand)', len(D_train),'len(D_valid_rand)', len(D_valid))\n",
        "        if len(rl_dataset) > 0:\n",
        "            # Adds the rl dataset to the random dataset if there is any present\n",
        "            rl_data = np.array(rl_dataset)\n",
        "            num_rl_examples = len(rl_data)\n",
        "            D_rl_train = rl_data[:int(-num_rl_examples*1/5)] \n",
        "            D_rl_valid = rl_data[int(-num_rl_examples*1/5):]\n",
        "                        \n",
        "            D_train = np.concatenate([D_train, D_rl_train], axis = 0)\n",
        "            D_valid = np.concatenate([D_valid, D_rl_valid], axis = 0)\n",
        "            print(\"number rl examples:\",num_rl_examples, 'len(D_rl_train)', len(D_rl_train),'len(D_valid_rand)', len(D_rl_valid))\n",
        "            \n",
        "        #print(\"len(D_train):\", len(D_train), 'len(D_valid)', len(D_valid))\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        \n",
        "        sff = np.arange(len(D_train))\n",
        "        np.random.shuffle(sff)\n",
        "        D_train = D_train[sff]\n",
        "        #print('D_train shape',D_train.shape)\n",
        "\n",
        "\n",
        "        # Create the input and output for the train\n",
        "        X_train_obs = np.array([obs for obs,_,_,_,_ in D_train]) # Takes obs and action\n",
        "        X_train_obs = X_train_obs.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        X_train_act = np.array([act for _,_,_,_,act in D_train])\n",
        "        \n",
        "\n",
        "        # Env output\n",
        "        y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "        y_env_train = y_env_train.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        y_env_train = y_env_train - X_train_obs \n",
        "        #y_env_train = y_env_train - np.array([obs for obs,_,_,_,_ in D_train]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        # Reward's output\n",
        "        y_rew_train = np.array([[D] for _,_,_,D,_ in D_train])\n",
        "    \n",
        "        # Next state output\n",
        "        X_val_obs = np.array([obs for obs,_,_,_,_ in D_valid]) # Takes obs and action\n",
        "        X_val_obs = X_val_obs.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)        \n",
        "        X_val_act = np.array([act for _,_,_,_,act in D_valid])\n",
        "\n",
        "        y_env_val = np.array([no for _,no,_,_,_ in D_valid])\n",
        "        y_env_val = y_env_val.astype(np.int16)\n",
        "        y_env_val = y_env_val - X_val_obs \n",
        "        #y_env_val = y_env_val - np.array([obs for obs,_,_,_,_ in D_valid]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        # Reward output\n",
        "        y_rew_val = np.array([[D] for _,_,_,D,_ in D_valid])\n",
        "    \n",
        "        env_train_data, env_val_data = (X_train_obs, X_train_act, y_env_train), (X_val_obs, X_val_act, y_env_val)\n",
        "        rew_train_data, rew_val_data = (X_train_obs, X_train_act, y_rew_train), (X_val_obs, X_val_act, y_rew_val)\n",
        "\n",
        "        return env_train_data, env_val_data, rew_train_data, rew_val_data \n",
        "    \n",
        "    '''\n",
        "    def collate_data_check(self,random_dataset, rl_dataset):\n",
        "        rand_data = np.array(random_dataset)\n",
        "        num_rand_examples = len(rand_data)\n",
        "        D_train = rand_data[:int(-num_rand_examples*1/5)] \n",
        "        D_valid = rand_data[int(-num_rand_examples*1/5):]\n",
        "        print(\"number random examples:\",num_rand_examples, 'len(D_train_rand)', len(D_train),'len(D_valid_rand)', len(D_valid))\n",
        "        if len(rl_dataset) > 0:\n",
        "            # Adds the rl dataset to the random dataset if there is any present\n",
        "            rl_data = np.array(rl_dataset)\n",
        "            num_rl_examples = len(rl_data)\n",
        "            D_rl_train = rl_data[:int(-num_rl_examples*1/5)] \n",
        "            D_rl_valid = rl_data[int(-num_rl_examples*1/5):]\n",
        "                        \n",
        "            D_train = np.concatenate([D_train, D_rl_train], axis = 0)\n",
        "            D_valid = np.concatenate([D_valid, D_rl_valid], axis = 0)\n",
        "            print(\"number rl examples:\",num_rl_examples, 'len(D_rl_train)', len(D_rl_train),'len(D_valid_rand)', len(D_rl_valid))\n",
        "            \n",
        "        #print(\"len(D_train):\", len(D_train), 'len(D_valid)', len(D_valid))\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        \n",
        "        sff = np.arange(len(D_train))\n",
        "        np.random.shuffle(sff)\n",
        "        D_train = D_train[sff]\n",
        "        \n",
        "        #print('D_train shape',D_train.shape)\n",
        "\n",
        "\n",
        "        # Create the input and output for the train\n",
        "        X_train_obs = np.array([obs for obs,_,_,_,_ in D_train]) # Takes obs and action\n",
        "        X_train_act = np.array([act for _,_,_,_,act in D_train])\n",
        "        X_train_nobs = np.array([nobs for _,nobs,_,_,_ in D_train]) # Takes obs and action\n",
        "        \n",
        "        X_train_obs = X_train_obs.astype(np.int16)\n",
        "        X_train_nobs = X_train_nobs.astype(np.int16)\n",
        "        # Env output\n",
        "        #y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "        y_env_train = X_train_nobs - X_train_obs #np.array([obs for obs,_,_,_,_ in D_train]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "    \n",
        "        return X_train_obs, X_train_act, X_train_nobs, y_env_train\n",
        "    '''\n",
        "\n",
        "    \n",
        "#data_collector = Data_collection('MsPacmanNoFrameskip-v4',10)\n",
        "data_collector = Data_collection('MsPacmanDeterministic-v4',10)\n",
        "random_data = data_collector.gather_random_trajectories(1)\n",
        "rl_dataset = []\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data =  data_collector.collate_data(random_data, rl_dataset) \n",
        "#X_train_obs,X_train_act, X_train_nobs, y_env = data_collector.collate_data_check(random_data, rl_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trajectory number : 0\n",
            "number random examples: 493 len(D_train_rand) 395 len(D_valid_rand) 98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDvh7CTnT34G",
        "colab_type": "code",
        "outputId": "0c146501-1b26-4af7-f512-3517ecea1659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "X_train_obs, X_train_act, y_env_train = env_val_data\n",
        "print(y_env_train[0:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  0  2  1  0 -2  0  0  0]\n",
            " [ 0  2  0  3  3  0 -4  0  0  0]\n",
            " [ 0  3 -1  2  3  0 -2  0 -3  0]\n",
            " [ 0  2 -4  3  3  0  0  0 -3  0]\n",
            " [ 0  2 -1  3  3  0 -2  0 -3  0]\n",
            " [ 0  1  0  3  3  1 -4  0 -3  0]\n",
            " [ 0  0  0  1  3  4 -3 -2 -3  0]\n",
            " [ 0  0  0  0  3  3 -4 -4 -3  0]\n",
            " [ 1  0  0  0  0  2 -2 -2  3  0]\n",
            " [ 2  0  3  0  0  3 -1 -4  3  0]\n",
            " [ 2  0  2  0  0  4  0 -3  3  0]\n",
            " [ 2  0  3  0  0  3  0 -4  3  0]\n",
            " [ 2  0  2  0  0  3  0 -4  3  0]\n",
            " [ 2  0  3 -2  0  2  0 -1  3  0]\n",
            " [ 2  0  2 -2  0  4  0  0 -3  0]\n",
            " [ 2  0  3 -3  0  3  0  0 -3  0]\n",
            " [ 1  0  1 -2  0  2  0  0  3  0]\n",
            " [ 2  0  3 -3  0  3  0  0  3  0]\n",
            " [ 2  0  2 -2  0  4  0  0 -3  0]\n",
            " [ 2  0  3 -3  0  3  0  0 -3  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ahvG8lhSVp",
        "colab_type": "text"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3T5LHY6hRW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_data,val_data,model,batch_size, max_model_iter, optimizer, device,early_stopper,desired_model='Env_model'):\n",
        "    ''' \n",
        "    General function to train either of the two models\n",
        "    '''\n",
        "    # Unpack data\n",
        "    (X_train, y_train), (X_val, y_val) =  train_data, val_data\n",
        "    losses_env = []\n",
        "\n",
        "    # Choose loss function based on what type of model is training\n",
        "    if desired_model =='Env_model': # Nawid -  Decides which loss function to use\n",
        "        loss_function = model_MSEloss\n",
        "    else:\n",
        "        loss_function = model_BCEloss\n",
        "        #print('BCE being used')\n",
        "\n",
        "    # go through max_model iter supervised iterations\n",
        "    for it in tqdm(range(max_model_iter)):\n",
        "        # create mini batches of size batch_size\n",
        "        for mb in range(0,len(X_train), batch_size): # Nawid- Batch size is the step size\n",
        "            if len(X_train) > mb + BATCH_SIZE:\n",
        "                X_mb = X_train[mb:mb+BATCH_SIZE]\n",
        "                y_mb = y_train[mb:mb+BATCH_SIZE]\n",
        "                #X_mb += np.random.normal(loc = 0, scale = 0.001, size= X_mb.shape)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # forward pass of model to compute the output\n",
        "                pred_mb = model(torch.tensor(X_mb).to(device))\n",
        "                \n",
        "                # compute the loss\n",
        "                loss = loss_function(pred_mb,y_mb,device) #Nawid-  Uses Mse loss if dynamics model or uses BCE loss if reward/done model\n",
        "                wandb.log({'{} Training loss'.format(desired_model):loss.cpu().detach().numpy()})\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                # optimization step\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Nawid - Calculate the validation loss after each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_val = model(torch.tensor(X_val).to(device))\n",
        "            val_loss = loss_function(pred_val, y_val,device)\n",
        "            wandb.log({'{} Validation loss'.format(desired_model):val_loss})\n",
        "        \n",
        "\n",
        "        # Checks whether to early stop after each epoch\n",
        "        early_stopper(val_loss,model)\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jEdA4yE3GfE",
        "colab_type": "text"
      },
      "source": [
        "# Controller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOzwWiaiRxQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class multi_model_based_control():\n",
        "    \n",
        "    def __init__(self,ENV_NAME, env_model, rew_model,num_sequences, horizon_length, n_actions, norm, num_features = feature_size, state_mode ='ATARIARI'):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.state_mode = state_mode\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            self.env = AtariARIWrapper(gym.make(ENV_NAME))\n",
        "            self.initial_info_labels = self.env.labels()\n",
        "        \n",
        "        self.env_model = env_model\n",
        "        self.rew_model = rew_model\n",
        "        self.horizon_length = horizon_length\n",
        "        self.num_sequences = num_sequences\n",
        "        self.repeated_initial = True # Nawid - set repeated initial as true initially\n",
        "        self.env_input_scaler, self.env_output_scaler, self.rew_input_scaler = norm\n",
        "        self.n_actions = n_actions\n",
        "        self.num_features = num_features\n",
        "        self.state_mode = state_mode\n",
        "        \n",
        "    def random_sampling_shooting(self,real_obs):\n",
        "        '''\n",
        "        Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "        '''\n",
        "        best_reward = -1e9\n",
        "        best_next_action = []\n",
        "        m_obs = np.array([real_obs for _ in range(self.num_sequences)])\n",
        "\n",
        "        # array that contains the rewards for all the sequence\n",
        "        unroll_rewards = np.zeros((self.num_sequences, 1)) \n",
        "        first_sampled_actions = []\n",
        "\n",
        "        self.env_model.eval()\n",
        "        self.rew_model.eval()\n",
        "\n",
        "        # Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon length' times\n",
        "        ## i.e roll a given number of trajectories in a single batch (to increase speed)\n",
        "        for t in range(self.horizon_length):\n",
        "            # sample actions for each sequence\n",
        "            sampled_actions = np.array([np.random.randint(0,self.n_actions) for _ in range(self.num_sequences)])\n",
        "            sampled_actions_one_hot = np.array([one_hot(action) for action in sampled_actions])\n",
        "            if isinstance(self.env_model,NNDynamicsModel): # Nawid-  If the env model is a neural net\n",
        "                #print('using dynamics model')\n",
        "                m_obs_env_scaled = self.env_input_scaler.transform(m_obs)\n",
        "                env_model_input = np.concatenate([m_obs_env_scaled, sampled_actions_one_hot], axis = 1)\n",
        "                # compute the next state for each sequence\n",
        "                pred_obs = self.env_model(torch.tensor(env_model_input).to(self.device))\n",
        "\n",
        "                # inverse scaler transformation\n",
        "                pred_obs = self.env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
        "                # add previous observation\n",
        "                next_obs = pred_obs + m_obs\n",
        "            else:\n",
        "                #print('using oracle state')            \n",
        "                next_obs = self.env_model.predict_states(m_obs,sampled_actions) # Nawid - Able to obtain the next state directly rather than predicting a change in states\n",
        "\n",
        "            if isinstance(self.rew_model, NNRewardModel):\n",
        "                m_obs_rew_scaled = self.rew_input_scaler.transform(m_obs)\n",
        "                rew_model_input = np.concatenate([m_obs_rew_scaled, sampled_actions_one_hot], axis = 1)\n",
        "                pred_rew = self.rew_model(torch.tensor(rew_model_input).to(self.device)) # Nawid -  I believe I do not need to rescale for a True of false situation\n",
        "                unroll_rewards += (1 - pred_rew.cpu().detach().numpy())\n",
        "            else:\n",
        "                #print('using oracle reward')\n",
        "                pred_rew = self.rew_model.predict_reward(m_obs, sampled_actions,next_obs)\n",
        "                unroll_rewards += pred_rew\n",
        "        \n",
        "            m_obs = next_obs # Nawid - Update the state after calculating the new state and calculating the rewards\n",
        "\n",
        "            if t ==0:\n",
        "                first_sampled_actions = sampled_actions\n",
        "        \n",
        "        self.env_model.train()\n",
        "        self.rew_model.train()\n",
        "\n",
        "        # Best the position of the sequence with the higher reward\n",
        "        arg_best_reward = np.argmax(unroll_rewards)\n",
        "        best_sum_reward = unroll_rewards[arg_best_reward].squeeze()\n",
        "        # take the first action of this sequence\n",
        "        best_action = first_sampled_actions[arg_best_reward]\n",
        "        #best_action =  np.squeeze(best_action)\n",
        "        return best_action, best_sum_reward\n",
        "    \n",
        "    def check_initial(self, next_info_labels):\n",
        "        # Checks if there has been a change from the initial info labels to show that the initial lag period is over \n",
        "        if self.initial_info_labels == next_info_labels:\n",
        "            pass\n",
        "        else:\n",
        "            self.repeated_initial = False\n",
        "    \n",
        "    def state_conversion(self,obs, info_labels):\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            state = []\n",
        "            i = 0 \n",
        "            for key in info_labels :\n",
        "                if i < self.num_features: # Nawid - Only the first 14 info are crucial I believe\n",
        "                    state.append(info_labels[key])\n",
        "                    i +=1\n",
        "                else:\n",
        "                    state = np.array(state)\n",
        "                    return state\n",
        "    \n",
        "        elif self.state_mode == 'STDIM': # Encoding is the state\n",
        "            assert self.encoder is not None\n",
        "            with torch.no_grad():\n",
        "                self.encoder.eval()\n",
        "                state = self.encoder(obs.float().to(self.device) / 255)\n",
        "                return state.cpu().detach().numpy()\n",
        "        else: # Image observations are the state or the RAM labels\n",
        "            return obs\n",
        "    \n",
        "    def reward_conversion(self,reward,done,info_labels,next_info_labels):\n",
        "        # checks whether there has been a change in lives- change done to 1, otherwise the done should be fine regardless\n",
        "        if not info_labels['num_lives'] == next_info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "            done = True \n",
        "            self.repeated_initial = True\n",
        "            self.initial_info_labels  = next_info_labels # Sets the new initial labels.\n",
        "        return reward, done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIBf1pknbf_",
        "colab_type": "text"
      },
      "source": [
        "# Main - Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk1xCx6x7NZ2",
        "colab_type": "text"
      },
      "source": [
        "Data collection and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVutEvgUib8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "data_collector = Data_collection(ENV_NAME,feature_size)\n",
        "#rand_dataset = data_collector.gather_random_trajectories(NUM_RAND_TRAJECTORIES)\n",
        "if load_data:\n",
        "    rand_dataset = np.load(loaded_trajectories,allow_pickle=True)\n",
        "else:\n",
        "    rand_dataset = data_collector.gather_random_trajectories(1000)\n",
        "\n",
        "rl_dataset = []\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = data_collector.collate_data(rand_dataset,rl_dataset)\n",
        "\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "norm = (X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJi8pwC27Q5N",
        "colab_type": "text"
      },
      "source": [
        "Dynamics model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h62P5907GbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "\n",
        "if env_model_pretrain:\n",
        "    env_model.load_state_dict(torch.load(pretrained_env_model))\n",
        "    env_model.eval()\n",
        "    \n",
        "else:\n",
        "    env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "    wandb.watch(env_model, log=\"all\")\n",
        "    env_model_name = 'Env_model'+ '_' + date_time\n",
        "    early_stopping_env = EarlyStopping_loss(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "    \n",
        "    for n_iter in range(AGGR_ITER):\n",
        "        if early_stopping_env.early_stop:\n",
        "            print('Early stopping')\n",
        "            break\n",
        "        train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTOm9XOX7kOL",
        "colab_type": "text"
      },
      "source": [
        "# Training the reward function and MPC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9DeBR47Lpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "rew_model = NNRewardModel(n_actions + feature_size, reward_dim).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "\n",
        "if rew_model_pretrain:\n",
        "    rew_model.load_state_dict(torch.load(pretrained_rew_model))\n",
        "\n",
        "rew_optimizer = torch.optim.Adam(rew_model.parameters(),REW_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "wandb.watch(rew_model, log=\"all\")\n",
        "Rew_model_name = 'Rew_model'+ '_' + date_time\n",
        "early_stopping_rew = EarlyStopping_loss(patience=5, verbose=True, wandb=wandb, name=Rew_model_name)\n",
        "\n",
        "#env = AtariARIWrapper(gym.make(ENV_NAME))  # Instantiate environment\n",
        "Controller = multi_model_based_control(ENV_NAME,env_model, rew_model, NUM_ACTIONS_SEQUENCES, HORIZON_LENGTH, n_actions, norm)\n",
        "global_step = 0\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_rew.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "\n",
        "    if not rew_model_pretrain:\n",
        "        train_model(norm_rew_train_data, norm_rew_val_data, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL,rew_optimizer,device,early_stopping_rew,desired_model='Rew_model')\n",
        "\n",
        "    \n",
        "    if collect_data:\n",
        "        env_train_data, env_val_data, rew_train_data, rew_val_data = data_collector.collate_data(rand_dataset,rl_dataset)\n",
        "        norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "\n",
        "    obs = Controller.env.reset()\n",
        "    Controller.env.seed(0) # Set the random seed of the environment\n",
        "    #obs = env.reset()\n",
        "    #initial_info_labels = env.labels()\n",
        "\n",
        "    num_examples_added = 0\n",
        "    game_reward = 0\n",
        "    # records how long the agent survives for\n",
        "    timesteps = 0\n",
        "    controller_pred_rews = []\n",
        "    rews = []\n",
        "    # records how many simulations have occurred\n",
        "     \n",
        "    #i = 0 \n",
        "    while num_examples_added < STEPS_PER_AGGR:\n",
        "    #while num_examples_added < STEPS_PER_AGGR:\n",
        "        while True:\n",
        "            tt = time.time()\n",
        "            \n",
        "            if Controller.repeated_initial:\n",
        "                #i += 1\n",
        "                #print(i)\n",
        "                obs,_,_,info = Controller.env.step(0) # Any action taken\n",
        "                info_labels = info['labels']\n",
        "                # Checks if the initial set is being repeated and if it isnt, it sets the value off\n",
        "                Controller.check_initial(info_labels) \n",
        "            else:\n",
        "                # new state achieved\n",
        "                state = Controller.state_conversion(obs,info_labels)\n",
        "                action, pred_rew = Controller.random_sampling_shooting(state)\n",
        "                action_one_hot = one_hot(action)\n",
        "                controller_pred_rews.append(pred_rew)\n",
        "\n",
        "                # one step in the environment with the action returned by\n",
        "                next_obs, reward,done, next_info = Controller.env.step(action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                \n",
        "                next_state = Controller.state_conversion(next_obs, next_info_labels)\n",
        "                # Obtains the reward, done and sets whether repeated initial should be true or not\n",
        "                reward, pacman_done = Controller.reward_conversion(reward,done,info_labels, next_info_labels) \n",
        "\n",
        "                # add to the RL dataset                \n",
        "                rl_dataset.append([state, next_state, reward, pacman_done, action_one_hot])\n",
        "\n",
        "                num_examples_added += 1\n",
        "                timesteps +=1 \n",
        "                obs = next_obs\n",
        "                info_labels = next_info_labels\n",
        "\n",
        "                game_reward += reward \n",
        "                if done:\n",
        "                    global_step += 1\n",
        "                    obs = Controller.env.reset()\n",
        "                    Controller.env.seed(0) # Need to set the random seed after the environment is done\n",
        "                    wandb.log({'game reward':game_reward, 'pred_rew': np.mean(controller_pred_rews),'survival time':timesteps,'global step':global_step })\n",
        "                    print('  >> R: {:.2f}, Mean sum:{:.2f},Survival time: {},Num examples:{}'.format(game_reward, np.mean(controller_pred_rews),timesteps,num_examples_added))\n",
        "                    rews.append(game_reward)\n",
        "                    game_reward = 0\n",
        "                    timesteps = 0\n",
        "                    controller_pred_rews = []\n",
        "                    break\n",
        "\n",
        "    print('  >> Mean: {:.2f}', np.mean(rews))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpGt1rdvsLFT",
        "colab_type": "text"
      },
      "source": [
        "# OLD CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqE-9CLYK6jF",
        "colab_type": "code",
        "outputId": "e1201452-eabb-4faf-9408-b915bd48d326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "\n",
        "encoder = NatureCNN(observation_channels,all_defaults)\n",
        "encoder.load_state_dict(torch.load('/content/MsPacmanNoFrameskip-v4_55.pt'))\n",
        "encoder.to(device)\n",
        "encoder.eval()\n",
        "rand_dataset = gather_random_trajectories(10000,'MsPacmanNoFrameskip-v4',encoder)\n",
        "rl_dataset = []\n",
        "num_examples_added = len(rand_dataset)\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(rand_dataset,rl_dataset,num_examples_added)\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "norm = (X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "\n",
        "env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "wandb.watch(env_model, log=\"all\")\n",
        "env_model_name = 'Env_model'+ '_' + date_time\n",
        "early_stopping_env = EarlyStopping(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_env.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▎         | 2/55 [00:00<00:18,  2.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy increased for Env_model_27_13.53.44  (0.000000 --> 1.118334).  Saving model ...\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 1 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 4/55 [00:00<00:11,  4.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_27_13.53.44 counter: 2 out of 5\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 3 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 5/55 [00:01<00:10,  4.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_27_13.53.44 counter: 4 out of 5\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 5 out of 5\n",
            "Env_model_27_13.53.44 has stopped\n",
            "Early stopping\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YKalYAl2PyE",
        "colab_type": "code",
        "outputId": "7866ec8e-930e-4157-f378-6e801ddef191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "practice_dones = torch.zeros((64,1))\n",
        "practice_dones[16:40,:] = 1\n",
        "#print(practice_dones)\n",
        "n,c = practice_dones.size()\n",
        "\n",
        "weights = np.zeros((n,c))\n",
        "\n",
        "pos = practice_dones[practice_dones==1].sum()\n",
        "neg = n - pos\n",
        "#print(practice_dones[practice_dones==1])\n",
        "\n",
        "# need to get indices,for when the dones is positive\n",
        "# weights = 1/neg\n",
        "# weights[p] = 1/pos # change the other weights to positive value\n",
        "'''\n",
        "weights[]\n",
        "p = np.random.permutation(len(args[0]))\n",
        "        shuffled = [i[p] for i in args]\n",
        "        return shuffled\n",
        "'''\n",
        "\n",
        "nx = practice_dones.numpy()\n",
        "index = np.where(nx == 1)[0]\n",
        "weights[:] = 1#1/neg\n",
        "weights[index] = neg* 1/pos\n",
        "print(weights)\n",
        "\n",
        "'''\n",
        "for i in range(n):\n",
        "    t = practice_dones[i,:].cpu().data.numpy()\n",
        "    pos = (t==1).sum()\n",
        "    neg = (t==0).sum()\n",
        "        \n",
        "    weights[i,t==1] = 1/pos\n",
        "    weights[i,t==0] = 1/neg\n",
        "\n",
        "weights = torch.Tensor(weights)\n",
        "print(weights)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i in range(n):\\n    t = practice_dones[i,:].cpu().data.numpy()\\n    pos = (t==1).sum()\\n    neg = (t==0).sum()\\n        \\n    weights[i,t==1] = 1/pos\\n    weights[i,t==0] = 1/neg\\n\\nweights = torch.Tensor(weights)\\nprint(weights)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIN4MDclKmCC",
        "colab_type": "code",
        "outputId": "f3440231-afd1-4491-c019-be48ec49cecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "env = AtariARIWrapper(gym.make(ENV_NAME))\n",
        "obs = env.reset()\n",
        "env.seed(1)\n",
        "for i in range(5):\n",
        "    _,_,_,info = env.step(1)\n",
        "    info_labels = info['labels']\n",
        "    print(info_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 0, 'player_score': 0, 'num_lives': 2}\n",
            "{'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 0, 'player_score': 0, 'num_lives': 2}\n",
            "{'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 0, 'player_score': 0, 'num_lives': 2}\n",
            "{'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 0, 'player_score': 0, 'num_lives': 2}\n",
            "{'enemy_sue_x': 88, 'enemy_inky_x': 88, 'enemy_pinky_x': 88, 'enemy_blinky_x': 88, 'enemy_sue_y': 80, 'enemy_inky_y': 80, 'enemy_pinky_y': 80, 'enemy_blinky_y': 50, 'player_x': 88, 'player_y': 98, 'fruit_x': 0, 'fruit_y': 0, 'ghosts_count': 3, 'player_direction': 3, 'dots_eaten_count': 0, 'player_score': 0, 'num_lives': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnWi4fVx_XdR",
        "colab_type": "code",
        "outputId": "fe0c8f00-f6aa-46b3-aa92-3df33ae5a0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "'''\n",
        "X_obs_rew, X_act_rew, y_rew =rew_train_data\n",
        "#print(X_obs_rew[0])\n",
        "norm_X_rew_train, norm_y_rew_train = norm_rew_train_data\n",
        "\n",
        "X_obs_env, X_act_env, y_env = env_train_data\n",
        "print(y_env[18])\n",
        "norm_X_env_train, norm_y_env_train = norm_env_train_data\n",
        "'''\n",
        "#print(X_rew_train[1])\n",
        "env = AtariARIWrapper(gym.make(ENV_NAME))\n",
        "obs = env.reset()\n",
        "env.seed(1)\n",
        "for i in range(5):\n",
        "    _,_,_,info = env.step(1)\n",
        "    info_labels = info['labels']\n",
        "    \n",
        "env.seed(1) # Fix random seed\n",
        "_,_,_,info = env.step(0)\n",
        "info_labels = info['labels']\n",
        "\n",
        "state = []\n",
        "i = 0 \n",
        "for key in info_labels :\n",
        "    if i < 10: # Nawid - Only the first 14 info are crucial I believe\n",
        "        state.append(info_labels[key])\n",
        "        print(key)\n",
        "        i +=1\n",
        "    else:\n",
        "        state = np.array(state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enemy_sue_x\n",
            "enemy_inky_x\n",
            "enemy_pinky_x\n",
            "enemy_blinky_x\n",
            "enemy_sue_y\n",
            "enemy_inky_y\n",
            "enemy_pinky_y\n",
            "enemy_blinky_y\n",
            "player_x\n",
            "player_y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tJp1ofFz0x7",
        "colab_type": "code",
        "outputId": "fe36102e-bbf6-4d8b-e591-7f7cb8572239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "#X_train_obs, X_train_nobs = X_train_obs.astype(np.int16), X_train_nobs.astype(np.int16)\n",
        "#print('difference',X_train_nobs[0]- X_train_obs[0])\n",
        "\n",
        "# 75 - 85 was informative\n",
        "print('X_train obs',X_train_obs[40:50])\n",
        "\n",
        "print('X_train act',X_train_act[40:50])\n",
        "#print('X_train_nobs',X_train_nobs[0:10])\n",
        "print('y_env',y_env[40:50])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train obs [[133  85  94  18  50  80  79   3  66  26]\n",
            " [134  88  94  20  48  80  76   2  66  26]\n",
            " [134  92  94  22  45  80  74   2  66  26]\n",
            " [134  94  94  25  42  79  71   2  66  26]\n",
            " [134  94  94  27  41  77  70   2  66  26]\n",
            " [134  94  94  30  38  74  67   2  66  26]\n",
            " [134  94  93  32  35  70  66   2  66  26]\n",
            " [134  94  90  35  32  67  66   2  66  26]\n",
            " [134  92  88  38  29  66  66   2  66  26]\n",
            " [134  89  84  41  26  66  66   2  66  26]]\n",
            "X_train act [[0 1 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 0 0 0]]\n",
            "y_env [[ 1  3  0  2 -2  0 -3 -1  0  0]\n",
            " [ 0  4  0  2 -3  0 -2  0  0  0]\n",
            " [ 0  2  0  3 -3 -1 -3  0  0  0]\n",
            " [ 0  0  0  2 -1 -2 -1  0  0  0]\n",
            " [ 0  0  0  3 -3 -3 -3  0  0  0]\n",
            " [ 0  0 -1  2 -3 -4 -1  0  0  0]\n",
            " [ 0  0 -3  3 -3 -3  0  0  0  0]\n",
            " [ 0 -2 -2  3 -3 -1  0  0  0  0]\n",
            " [ 0 -3 -4  3 -3  0  0  0  0  0]\n",
            " [ 2 -1 -2  1  0 -3  1  2  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}