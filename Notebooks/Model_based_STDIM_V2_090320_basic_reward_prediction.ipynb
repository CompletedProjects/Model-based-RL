{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_based_STDIM_V2_090320_basic_reward_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNV6iiUhQAmYsCYE6kudW+w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/Model_based_STDIM_V2_090320_basic_reward_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "9f17fde3-64ce-41f6-fe49-7d1e7b59fb6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/mila-iqia/atari-representation-learning.git\n",
            "  Cloning git://github.com/mila-iqia/atari-representation-learning.git to /tmp/pip-req-build-xmt_cu6x\n",
            "  Running command git clone -q git://github.com/mila-iqia/atari-representation-learning.git /tmp/pip-req-build-xmt_cu6x\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (0.15.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (4.1.2.30)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.10)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->atariari==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: atariari\n",
            "  Building wheel for atariari (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atariari: filename=atariari-0.0.1-cp36-none-any.whl size=46584 sha256=9b3e84eca6929788e74ebe4471e17b8bdc3a6fb3bcc53545a12a674b4265e311\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j5_7vhzh/wheels/3d/69/51/5e436e5ae566c5b4dec5c53e65396d516459877a42a11d7aa4\n",
            "Successfully built atariari\n",
            "Installing collected packages: atariari\n",
            "Successfully installed atariari-0.0.1\n",
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-__flgwp6\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-__flgwp6\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.15.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.1.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.10)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.6.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->a2c-ppo-acktr==0.0.1) (45.2.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=709c71be639746d3efbee12fb9dadf450c872f0b98e0db43443affa83101aade\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k5krt7ke/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-uncxhox4\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-uncxhox4\n",
            "Requirement already satisfied: gym<0.16.0,>=0.15.4 in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.15.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.28.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.4.10)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.17.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=50c4728cc3d62237403f94cb85dd9ac9ff9c9f0d8173c7a08e00e603fc4d2917\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sdv3bjj5/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "Successfully built baselines\n",
            "Installing collected packages: baselines\n",
            "Successfully installed baselines-0.1.6\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/05/a0bf45b2f4909c3ffb1729deb19355a067a8cf8d56eebd3159d702321b68/wandb-0.8.29-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 47.2MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.6MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/e6/058c2dc4723b3647a8cf61385c61f284bfbd0d0657bc5623c22e9ef45a3c/sentry_sdk-0.14.2-py2.py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.8MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.6MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[?25hCollecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, gql, watchdog, graphql-core, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=2b771e50b93c3e1576203961932a271b5f2e65bd2cd50f298590566a327e517c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=ad9873275ffcb1cb590b869993309566aca59fa96401ea8b2297a9bda0070dce\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=d0a6b6434052423da1be14863d888d48ff13ba5b5de6a8d76602d8fa9e1b325b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=32482f937589313b3c2000da3904998d3bfb69212a79c8193ad3bdaa752ffa7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=624ed0456e019ca8c2cd017ca652c9fd63b19252e9dcfc08ed4899718934d8a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 gql watchdog graphql-core pathtools\n",
            "Installing collected packages: docker-pycreds, smmap, gitdb, GitPython, subprocess32, configparser, graphql-core, gql, sentry-sdk, shortuuid, pathtools, watchdog, wandb\n",
            "Successfully installed GitPython-3.1.0 configparser-4.0.2 docker-pycreds-0.4.0 gitdb-4.0.2 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.2 shortuuid-1.0.1 smmap-3.0.1 subprocess32-3.5.4 wandb-0.8.29 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZ9gMrxp5vU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "a6183c13-2bef-4c61-9b9f-435c2d96f5da"
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNZIoA0Fp9Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, BatchSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from atariari.methods.utils import calculate_accuracy, Cutout, EarlyStopping\n",
        "from atariari.methods.trainer import Trainer\n",
        "from atariari.benchmark.episodes import get_episodes\n",
        "from atariari.benchmark.envs import *\n",
        "from atariari.methods.utils import get_argparser\n",
        "from torch.autograd import Variable\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Imported required for the Model-based RL\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6e6d28e3-fc05-4771-9022-b29f743627c4"
      },
      "source": [
        "!wandb login ###################"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gdFb0bqHHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2eb3fb13-71b2-46fd-f941-f428c3945e6b"
      },
      "source": [
        "parser = get_argparser()\n",
        "args = parser.parse_args(\"\")\n",
        "all_defaults = {}\n",
        "for key in vars(args):\n",
        "    all_defaults[key] = parser.get_default(key)\n",
        "\n",
        "all_defaults['env_name'] = 'MsPacmanNoFrameskip-v4'\n",
        "\n",
        "wandb.init(entity=\"nerdk312\", project=\"STDIM\",config=all_defaults)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/STDIM\" target=\"_blank\">https://app.wandb.ai/nerdk312/STDIM</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/STDIM/runs/1k30i8rr\" target=\"_blank\">https://app.wandb.ai/nerdk312/STDIM/runs/1k30i8rr</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/nerdk312/STDIM/runs/1k30i8rr"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G5adriWecft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "outputId": "c5eb2a3a-56bf-405e-970f-dacaf3f42cf4"
      },
      "source": [
        "all_defaults"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'beta': 1.0,\n",
              " 'checkpoint_index': -1,\n",
              " 'collect_mode': 'random_agent',\n",
              " 'color': False,\n",
              " 'cuda_id': 0,\n",
              " 'encoder_type': 'Nature',\n",
              " 'end_with_relu': False,\n",
              " 'entropy_threshold': 0.6,\n",
              " 'env_name': 'MsPacmanNoFrameskip-v4',\n",
              " 'epochs': 100,\n",
              " 'feature_size': 256,\n",
              " 'gru_layers': 2,\n",
              " 'gru_size': 256,\n",
              " 'linear': True,\n",
              " 'lr': 0.0003,\n",
              " 'method': 'infonce-stdim',\n",
              " 'naff_fc_size': 2048,\n",
              " 'no_downsample': True,\n",
              " 'num_frame_stack': 1,\n",
              " 'num_processes': 8,\n",
              " 'num_rew_evals': 10,\n",
              " 'num_runs': 1,\n",
              " 'patience': 15,\n",
              " 'pred_offset': 1,\n",
              " 'pretraining_steps': 100000,\n",
              " 'probe_collect_mode': 'random_agent',\n",
              " 'probe_lr': 0.0003,\n",
              " 'probe_steps': 50000,\n",
              " 'seed': 42,\n",
              " 'sequence_length': 100,\n",
              " 'steps_end': 99,\n",
              " 'steps_start': 0,\n",
              " 'steps_step': 4,\n",
              " 'train_encoder': True,\n",
              " 'use_multiple_predictors': False,\n",
              " 'wandb_entity': None,\n",
              " 'wandb_proj': 'atari-reps',\n",
              " 'weights_path': 'None'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aFJO7vdqOY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from a2c_ppo_acktr.utils import init\n",
        "import time\n",
        "from atariari.benchmark.utils import download_run\n",
        "from atariari.benchmark.episodes import checkpointed_steps_full_sorted\n",
        "import os\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class Conv2dSame(torch.nn.Module): # Nawid - Performs convolution in the same way as 'same' tensorflow format I assume\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, bias=True, padding_layer=nn.ReflectionPad2d):\n",
        "        super().__init__()\n",
        "        ka = kernel_size // 2\n",
        "        kb = ka - 1 if kernel_size % 2 == 0 else ka\n",
        "        self.net = torch.nn.Sequential(\n",
        "            padding_layer((ka, kb, ka, kb)),\n",
        "            torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            Conv2dSame(in_channels, out_channels, 3),\n",
        "            nn.ReLU(),\n",
        "            Conv2dSame(in_channels, out_channels, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.block(x)\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImpalaCNN(nn.Module): # Nawid -  CNN architecture in the Impala paper\n",
        "    def __init__(self, input_channels, args):\n",
        "        super(ImpalaCNN, self).__init__()\n",
        "        self.hidden_size = args['feature_size']\n",
        "        self.depths = [16, 32, 32, 32]\n",
        "        self.downsample = not args['no_downsample']\n",
        "        self.layer1 = self._make_layer(input_channels, self.depths[0])\n",
        "        self.layer2 = self._make_layer(self.depths[0], self.depths[1])\n",
        "        self.layer3 = self._make_layer(self.depths[1], self.depths[2])\n",
        "        self.layer4 = self._make_layer(self.depths[2], self.depths[3])\n",
        "        if self.downsample:\n",
        "            self.final_conv_size = 32 * 9 * 9\n",
        "        else:\n",
        "            self.final_conv_size = 32 * 12 * 9\n",
        "        self.final_linear = nn.Linear(self.final_conv_size, self.hidden_size)\n",
        "        self.flatten = Flatten()\n",
        "        self.train()\n",
        "\n",
        "    def _make_layer(self, in_channels, depth): # Nawid-  Used to make a layer\n",
        "        return nn.Sequential(\n",
        "            Conv2dSame(in_channels, depth, 3),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(depth, depth),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(depth, depth)\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def local_layer_depth(self):\n",
        "        return self.depths[-2]\n",
        "\n",
        "    def forward(self, inputs, fmaps=False):\n",
        "        #print(inputs.size())\n",
        "        f5 = self.layer3(self.layer2(self.layer1(inputs))) # Nawid -Uses the output of the third layer and then sees whether we want to downsample or not\n",
        "        \n",
        "        if not self.downsample:\n",
        "            out = self.layer4(f5)\n",
        "            \n",
        "        else:\n",
        "            out = f5\n",
        "            \n",
        "        \n",
        "        #print('before out size',out.size())\n",
        "        out = F.relu(self.final_linear(self.flatten(out))) # Nawid- global feature vector\n",
        "        #print('after out size', out.size())\n",
        "\n",
        "        if fmaps:\n",
        "            return {\n",
        "                'f5': f5.permute(0, 2, 3, 1), # Nawid - Make the channels in the last dimension\n",
        "                'out': out\n",
        "            }\n",
        "\n",
        "        return out\n",
        "\n",
        "class NatureCNN(nn.Module): # Nawid - Nature CNN\n",
        "\n",
        "    def __init__(self, input_channels, args):\n",
        "        super().__init__()\n",
        "        self.feature_size = args['feature_size']\n",
        "        self.hidden_size = self.feature_size\n",
        "        self.downsample = not args['no_downsample']\n",
        "        self.input_channels = input_channels\n",
        "        self.end_with_relu = args['end_with_relu']\n",
        "        self.args = args\n",
        "        init_ = lambda m: init(m,\n",
        "                               nn.init.orthogonal_,\n",
        "                               lambda x: nn.init.constant_(x, 0),\n",
        "                               nn.init.calculate_gain('relu'))\n",
        "        self.flatten = Flatten()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.final_conv_size = 32 * 7 * 7\n",
        "            self.final_conv_shape = (32, 7, 7)\n",
        "            self.main = nn.Sequential(\n",
        "                init_(nn.Conv2d(input_channels, 32, 8, stride=4)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(32, 64, 4, stride=2)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(64, 32, 3, stride=1)),\n",
        "                nn.ReLU(),\n",
        "                Flatten(),\n",
        "                init_(nn.Linear(self.final_conv_size, self.feature_size)),\n",
        "                #nn.ReLU()\n",
        "            )\n",
        "        else:\n",
        "            self.final_conv_size = 64 * 9 * 6\n",
        "            self.final_conv_shape = (64, 9, 6)\n",
        "            self.main = nn.Sequential(\n",
        "                init_(nn.Conv2d(input_channels, 32, 8, stride=4)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(32, 64, 4, stride=2)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(64, 128, 4, stride=2)),\n",
        "                nn.ReLU(),\n",
        "                init_(nn.Conv2d(128, 64, 3, stride=1)),\n",
        "                nn.ReLU(),\n",
        "                Flatten(),\n",
        "                init_(nn.Linear(self.final_conv_size, self.feature_size)),\n",
        "                #nn.ReLU()\n",
        "            )\n",
        "        self.train()\n",
        "\n",
        "    @property\n",
        "    def local_layer_depth(self):\n",
        "        return self.main[4].out_channels\n",
        "\n",
        "    def forward(self, inputs, fmaps=False):\n",
        "        f5 = self.main[:6](inputs)\n",
        "        f7 = self.main[6:8](f5)\n",
        "        out = self.main[8:](f7)\n",
        "        if self.end_with_relu:\n",
        "            assert self.args.method != \"vae\", \"can't end with relu and use vae!\"\n",
        "            out = F.relu(out)\n",
        "        if fmaps: # Nawid - obtains the different feature maps as well as global feature vector\n",
        "            return {\n",
        "                'f5': f5.permute(0, 2, 3, 1),\n",
        "                'f7': f7.permute(0, 2, 3, 1),\n",
        "                'out': out\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class PPOEncoder(nn.Module):\n",
        "    def __init__(self, env_name, checkpoint_index):\n",
        "        super().__init__()\n",
        "        checkpoint_step = checkpointed_steps_full_sorted[checkpoint_index]\n",
        "        filepath = download_run(env_name, checkpoint_step)\n",
        "        while not os.path.exists(filepath):\n",
        "            time.sleep(5)\n",
        "\n",
        "        self.masks = torch.zeros(1, 1)\n",
        "        self.ppo_model, ob_rms = torch.load(filepath, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, _, _, feature_vectors, _ = self.ppo_model.act(x,\n",
        "                                                            None,\n",
        "                                                            self.masks,\n",
        "                                                            deterministic=False)\n",
        "        return feature_vectors\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePMZh0WfleFC",
        "colab_type": "text"
      },
      "source": [
        "# Model-based RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUzSa9NikYHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NNDynamicModel(nn.Module):\n",
        "    '''\n",
        "    Model that predicts the next state, given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, obs_output_dim):\n",
        "        super(NNDynamicModel,self).__init__()\n",
        "        self.Linear1 = nn.Linear(input_dim,512)\n",
        "        self.BN1 = nn.BatchNorm1d(num_features = 512)\n",
        "        self.Linear2 = nn.Linear(512,256)\n",
        "        self.BN2 = nn.BatchNorm1d(num_features=256)\n",
        "        self.Linear3 = nn.Linear(256, obs_output_dim) \n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        print('input dimensions',x.size())\n",
        "        x = self.Linear1(x.float())\n",
        "        print('after linear1',x.size())\n",
        "        x = self.BN1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Linear2(x)\n",
        "        print('after linear2',x.size())\n",
        "        x = self.BN2(x)\n",
        "        x = F.relu(x)\n",
        "        output = self.Linear3(x)\n",
        "        print('after linear3',output.size())        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mHFTQIJlts2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NNRewardModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the reward given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, reward_output_dim):\n",
        "        super(NNRewardModel, self).__init__()\n",
        "        \n",
        "        self.Linear1 = nn.Linear(input_dim,512)\n",
        "        self.BN1 = nn.BatchNorm1d(num_features = 512)\n",
        "        self.Linear2 = nn.Linear(512,256)\n",
        "        self.BN2 = nn.BatchNorm1d(num_features=256)\n",
        "        self.Linear3 = nn.Linear(256, reward_output_dim) \n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.Linear1(x.float())\n",
        "        #print('first linear x size',x.size())\n",
        "        x = self.BN1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Linear2(x)\n",
        "        #print('second linear x size',x.size())\n",
        "        x = self.BN2(x)\n",
        "        x = F.relu(x)\n",
        "        output = self.Linear3(x)\n",
        "        #print('third linear x size',output.size())        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR-jezFVFWm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_random_trajectories(num_traj, env_name,encoder):\n",
        "    '''\n",
        "    Run num_traj random trajectories to gather information about the next state and reward.\n",
        "    Data used to train the models in a supervised way.\n",
        "    '''\n",
        "\n",
        "    dataset_random = []\n",
        "    game_rewards = []\n",
        "    with torch.no_grad():\n",
        "        for n in range(num_traj):\n",
        "            print(n)        \n",
        "            env = make_vec_envs(ENV_NAME,1,workers,downsample=False) # Nawid- Makes several different vectorised environment\n",
        "            obs = env.reset()\n",
        "            #print('does observation work',obs.shape)\n",
        "        \n",
        "            state = encoder(obs.float().to(device) / 255)\n",
        "        \n",
        "            while True:\n",
        "                sampled_action = torch.tensor([env.action_space.sample() for i in range(workers)]).unsqueeze(dim=1) # Nawid - This needs to be used for the case where the environment is vectorised\n",
        "                print('action type used for the environment',sampled_action,sampled_action.size())\n",
        "                new_obs, reward, done, _ = env.step(sampled_action)\n",
        "            \n",
        "                new_state = encoder(new_obs.float().to(device) / 255)\n",
        "                #print('new_state',new_state.size())\n",
        "                #print('reward', reward.size())\n",
        "                #print('sampled_action', sampled_action.size())\n",
        "                dataset_random.append([state.cpu(), new_state.cpu(), reward, done,sampled_action.float()]) # Nawid - Appends the state instead of the observation, changed action to float in order to concatenate with the observation tensor to use as training data, and need to move the state to cpu in order to change it to numpy\n",
        "                #print('dataset_random',dataset_random.shape) \n",
        "                #dataset_random.append([obs, new_obs, reward, done,sampled_action]) # Nawid - Appends the state instead of the observation\n",
        "\n",
        "                obs = new_obs\n",
        "                game_rewards.append(reward)\n",
        "\n",
        "                if done:\n",
        "                    env.close()\n",
        "                    break\n",
        "\n",
        "        # print some stats\n",
        "        print('Mean R:',np.round(np.sum(game_rewards)/num_traj,2), 'Max R:', np.round(np.max(game_rewards),2), np.round(len(game_rewards)/num_traj))\n",
        "\n",
        "    return dataset_random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM-D95cFl3J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_MSEloss(y_truth, y_pred, device):\n",
        "    '''\n",
        "    Compute the MSE (Mean Squared Error)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_7VEcz2mBJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_dyna_model(random_dataset, rl_dataset, env_model, rew_model, batch_size, max_model_iter, num_examples_added, ENV_LEARNING_RATE, REW_LEARNING_RATE, device):\n",
        "    '''\n",
        "    Train the two models that predict the next state and the expected reward\n",
        "    '''\n",
        "\n",
        "    env_optimizer = optim.Adam(env_model.parameters(), lr=ENV_LEARNING_RATE)\n",
        "    rew_optimizer = optim.Adam(rew_model.parameters(), lr=REW_LEARNING_RATE)\n",
        "\n",
        "    if len(rl_dataset) > 0:\n",
        "        '''\n",
        "        # To use only a fraction of the random dataset\n",
        "        rand = np.arange(len(random_dataset))\n",
        "        np.random.shuffle(rand)\n",
        "        rand = rand[:int(len(rl_dataset)*0.8)] # 80% of rl dataset\n",
        "        d_concat = np.concatenate([np.array(random_dataset)[rand], rl_dataset], axis=0)'''\n",
        "\n",
        "        # Concatenate the random dataset with the RL dataset. Used only in the aggregation iterations\n",
        "        d_concat = np.concatenate([random_dataset, rl_dataset], axis=0)\n",
        "    else:\n",
        "        d_concat = np.array(random_dataset)\n",
        "\n",
        "    # Split the dataset into train(80%) and test(20%)\n",
        "    D_train = d_concat[:int(-num_examples_added*1/5)]\n",
        "    D_valid = d_concat[int(-num_examples_added*1/5):]\n",
        "\n",
        "    print(\"len(D):\", len(d_concat), 'len(Dtrain)', len(D_train))\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    sff = np.arange(len(D_train))\n",
        "    np.random.shuffle(sff)\n",
        "    D_train = D_train[sff]\n",
        "\n",
        "    print('does it work at this point')\n",
        "    # Create the input and output for the train\n",
        "    #for obs,_,_,_,act in D_train:\n",
        "    #    concatenated_array =  np.concatenate([obs,act],axis = 1)\n",
        "    \n",
        "    #obs_train = D_train[:,0] \n",
        "    #n_obs_train = D_train[:,1]\n",
        "    #reward_train = D_train[:,2]\n",
        "    #act_train = D_train[:,3]\n",
        " \n",
        "  \n",
        "    X_train = torch.empty(size=(len(D_train),feature_size + 1)) # env.action_space.n))\n",
        "    y_env_train = torch.empty(size=(len(D_train),feature_size))\n",
        "    y_rew_train = torch.empty(size=(len(D_train),1))\n",
        "    \n",
        "\n",
        "    for i, (obs,n_obs,rew,_,act) in enumerate(D_train):\n",
        "        X_train[i] = torch.cat((obs,act),axis=1)\n",
        "        y_env_train[i] = n_obs - obs # Nawid - Change of state is the label for the model training # y(state) = s(t+1) - s(t)\n",
        "        y_rew_train[i] = rew\n",
        "\n",
        "    print('X_train', X_train.size())\n",
        "    print('y_env_train', y_env_train.size())\n",
        "    print('y_rew_train',y_rew_train.size())\n",
        "    \n",
        "    X_valid = torch.empty(size=(len(D_train),feature_size + 1)) # env.action_space.n))\n",
        "    y_env_valid = torch.empty(size=(len(D_train),feature_size))\n",
        "    y_rew_valid = torch.empty(size=(len(D_train),1))\n",
        "\n",
        "    for i, (obs,n_obs,rew,_,act) in enumerate(D_valid):\n",
        "        X_valid[i] = torch.cat((obs,act),axis=1)\n",
        "        y_env_valid[i] = n_obs - obs # Nawid - Change of state is the label for the model training # y(state) = s(t+1) - s(t)\n",
        "        y_rew_valid[i] = rew\n",
        "\n",
        "\n",
        "    # Standardize the input features by removing the mean and scaling to unit variance\n",
        "    input_scaler = StandardScaler()\n",
        "    X_train = input_scaler.fit_transform(X_train)\n",
        "    X_valid = input_scaler.transform(X_valid)\n",
        "\n",
        "    # Standardize the outputs by removing the mean and scaling to unit variance\n",
        "\n",
        "    env_output_scaler = StandardScaler()\n",
        "    y_env_train = env_output_scaler.fit_transform(y_env_train)\n",
        "    y_env_valid = env_output_scaler.transform(y_env_valid)\n",
        "\n",
        "    rew_output_scaler = StandardScaler()\n",
        "    y_rew_train = rew_output_scaler.fit_transform(y_rew_train)\n",
        "    y_rew_valid = rew_output_scaler.transform(y_rew_valid)\n",
        "    print('does it work after setting up the scaling')\n",
        "    # store all the scalers in a variable to later uses\n",
        "    norm = (input_scaler, env_output_scaler, rew_output_scaler)\n",
        "\n",
        "    losses_env = []\n",
        "    losses_rew = []\n",
        "\n",
        "    # go through max_model_iter supervised iterations\n",
        "    for it in tqdm(range(max_model_iter)):\n",
        "        #X_train += Variable(ins.data.new(ins.size()).normal_(0, 0.001))\n",
        "\n",
        "        #print(X_train.shape)\n",
        "        #X_train += Variable(torch.FloatTensor(X_train.size()).normal_()) # Nawid-  This is for the case when it is a torch tensor.\n",
        "        \n",
        "        X_train += np.random.normal(loc=0, scale=0.001, size=X_train.shape)\n",
        "        print('Random noise is added')\n",
        "        ## Optimisation of the 'env_model' neural net\n",
        "        env_optimizer.zero_grad()\n",
        "        print('X_train',X_train.shape)\n",
        "\n",
        "        pred_state = env_model(torch.tensor(X_train).to(device))\n",
        "        print('Predictions can be made')\n",
        "        loss = model_MSEloss(y_env_train, pred_state, device)\n",
        "        if it == (max_model_iter - 1):\n",
        "            losses_env.append(loss.cpu().detach().numpy())\n",
        "        \n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # optimization step\n",
        "        env_optimizer.step()\n",
        "\n",
        "\n",
        "        ## Optimization of the 'rew_model' neural net\n",
        "        rew_optimizer.zero_grad()\n",
        "        # forward pass of the model to compute the output\n",
        "        print('Does it work before reward prediction')\n",
        "        pred_rew = rew_model(torch.tensor(X_train).to(device))\n",
        "        print('Does it work after reward prediction')\n",
        "        # compute the MSE loss\n",
        "        loss = model_MSEloss(y_rew_train, pred_rew, device)\n",
        "\n",
        "        if it == (max_model_iter - 1):\n",
        "            losses_rew.append(loss.cpu().detach().numpy())\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # optimization step\n",
        "        rew_optimizer.step()\n",
        "\n",
        "        print('Does it work till this point')\n",
        "\n",
        "    # Evalute the models every 10 iterations and print the losses\n",
        "    if it % 10 == 0:\n",
        "        env_model.eval()\n",
        "        rew_model.eval()\n",
        "\n",
        "        pred_state = env_model(torch.tensor(X_valid).to(device))\n",
        "        pred_rew = rew_model(torch.tensor(X_valid).to(device))\n",
        "        env_model.train(True)\n",
        "        rew_model.train(True)\n",
        "\n",
        "        valid_env_loss = model_MSEloss(y_env_valid, pred_state, device)\n",
        "        valid_rew_loss = model_MSEloss(y_rew_valid, pred_rew, device)\n",
        "\n",
        "        print('..', it, valid_env_loss.cpu().detach().numpy(), valid_rew_loss.cpu().detach().numpy())\n",
        "\n",
        "    ## Evaluate the MSE losses\n",
        "\n",
        "    env_model.eval()\n",
        "    rew_model.eval()\n",
        "\n",
        "    pred_state = env_model(torch.tensor(X_valid).to(device))\n",
        "    pred_rew = rew_model(torch.tensor(X_valid).to(device))\n",
        "    env_model.train(True)\n",
        "    rew_model.train(True)\n",
        "\n",
        "    valid_env_loss = model_MSEloss(y_env_valid, pred_state, device)\n",
        "    valid_rew_loss = model_MSEloss(y_rew_valid, pred_rew, device)\n",
        "\n",
        "    return np.mean(losses_env), np.mean(losses_rew), valid_env_loss.cpu().detach().numpy(), valid_rew_loss.cpu().detach().numpy(), norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEUgQwuWmMBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_model_based_control(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, norm, device):\n",
        "    '''\n",
        "    Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "    '''\n",
        "    best_reward = -1e9\n",
        "    best_next_action = []\n",
        "\n",
        "    input_scaler, env_output_scaler, rew_output_scaler = norm\n",
        "    m_obs = real_obs.repeat(num_sequences,1) # Nawid-  Repeats the tensor along the batch dimension for the number of different trajectories\n",
        "    print('real obs',real_obs.size())\n",
        "    print('m_obs size',m_obs.size())\n",
        "    \n",
        "\n",
        "    #m_obs = np.array([real_obs for _ in range(num_sequences)]) # Nawid - Takes a single observation and makes an array of the different possible trajectories\n",
        "\n",
        "    # array that contains the rewards for all the sequence\n",
        "    unroll_rewards = np.zeros((num_sequences, 1))\n",
        "    first_sampled_actions = []\n",
        "\n",
        "    env_model.eval()\n",
        "    rew_model.eval()\n",
        "\n",
        "    ## Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon_length' times.\n",
        "    ## i.e. roll a given number of trajectories in a single batch (to increase speed)\n",
        "\n",
        "    for t in range(horizon_length):\n",
        "\n",
        "        # sampled actions for each sequence\n",
        "        \n",
        "        #print('sample action',sample_action())\n",
        "        #sampled_actions = sample_action().repeat(num_sequences,1)\n",
        "        #print('single action', sample_action().size())\n",
        "        #print('sampled actions', sampled_actions.size())\n",
        "        # scale the input\n",
        "        sampled_actions = [sample_action() for _ in range(num_sequences)]\n",
        "        sampled_actions = np.array(sampled_actions, dtype=np.float32)\n",
        "\n",
        "        #sampled_actions = sampled_actions.astype(float)\n",
        "        sampled_actions = np.expand_dims(sampled_actions,axis=1)\n",
        "\n",
        "        \n",
        "        print('sampled actions', sampled_actions.shape)\n",
        "        print('working before models_input')\n",
        "\n",
        "        models_input = torch.cat((m_obs,torch.tensor(sampled_actions).to(device)),axis=1)\n",
        "\n",
        "        #models_input = input_scaler.transform(np.concatenate([m_obs, sampled_actions], axis=1))\n",
        "        print('working after model_input')\n",
        "\n",
        "        # compute the next state for each sequence\n",
        "        pred_obs = env_model(torch.tensor(models_input).to(device))\n",
        "        #print('predicted observation',pred_obs.size()) # Nawid - Torch tensor\n",
        "        # and the reward\n",
        "        pred_rew = rew_model(torch.tensor(models_input).to(device))\n",
        "\n",
        "        # inverse scaler transofrmation\n",
        "        pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
        "        print('predicted observation',pred_obs.shape) # Nawid - Torch tensor\n",
        "\n",
        "        # and add previous observation\n",
        "        m_obs = torch.tensor(pred_obs).to(device) + m_obs\n",
        "\n",
        "        assert(pred_rew.cpu().detach().numpy().shape == unroll_rewards.shape)\n",
        "\n",
        "        # sum of the expected rewards\n",
        "        unroll_rewards += pred_rew.cpu().detach().numpy()\n",
        "\n",
        "        if t == 0:\n",
        "            first_sampled_actions = sampled_actions\n",
        "\n",
        "    env_model.train(True)\n",
        "    rew_model.train(True)\n",
        "\n",
        "    print('Does it work till this point')\n",
        "\n",
        "    # Best the position of the sequence with the higher reward\n",
        "    arg_best_reward = np.argmax(unroll_rewards)\n",
        "    print('arg best reward done', arg_best_reward)\n",
        "    best_sum_reward = unroll_rewards[arg_best_reward].squeeze()\n",
        "    print('best sum reward one', best_sum_reward)\n",
        "    # take the first action of this sequence\n",
        "    best_action = first_sampled_actions[arg_best_reward]#.squeeze()\n",
        "    print('best action done', best_action)\n",
        "\n",
        "    return best_action, best_sum_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n-tAu7bmTpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'MsPacmanNoFrameskip-v4'\n",
        "\n",
        "feature_size = all_defaults['feature_size'] # Nawid- Dimensionality of the representation\n",
        "workers = 1 # Nawid - Choosing the number of workers for the network\n",
        "\n",
        "# Main loop hyperp\n",
        "AGGR_ITER = 3\n",
        "STEPS_PER_AGGR = 20000\n",
        "\n",
        "# Random MB Hyperp\n",
        "NUM_RAND_TRAJECTORIES = 2 #1000\n",
        "\n",
        "# cuda or cpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu' )\n",
        "\n",
        "# Supervised Model Hyperp\n",
        "ENV_LEARNING_RATE = 1e-3\n",
        "REW_LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 512\n",
        "TRAIN_ITER_MODEL = 55\n",
        "\n",
        "# Controller Hyper\n",
        "HORIZON_LENGTH = 10\n",
        "NUM_ACTIONS_SEQUENCES = 20000\n",
        "\n",
        "observation_channels = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OTGL9WVmb55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1962d2d-9bb2-4e4d-ce09-1ca343d44720"
      },
      "source": [
        "def main():\n",
        "\n",
        "    torch.cuda.empty_cache() # Nawid - I am not sure if this helps - hopefully it does    \n",
        "    encoder = ImpalaCNN(observation_channels,all_defaults)\n",
        "    encoder.load_state_dict(torch.load('/content/MsPacmanNoFrameskip-v4_95.pt'))#,map_location=torch.device('cpu')))\n",
        "    for param in encoder.parameters():\n",
        "        param.requires_grad = False \n",
        "    #pytorch_total_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
        "    #print(pytorch_total_params)\n",
        "\n",
        "\n",
        "    encoder.eval()\n",
        "    \n",
        "    encoder.to(device)\n",
        "    #print('does it work here')\n",
        "    \n",
        "    # gather the dataset of random sequences\n",
        "    #rand_dataset = gather_random_trajectories(NUM_RAND_TRAJECTORIES, ENV_NAME)\n",
        "    rand_dataset = gather_random_trajectories(NUM_RAND_TRAJECTORIES, ENV_NAME, encoder)\n",
        "\n",
        "    rl_dataset = []\n",
        "    env = make_vec_envs(ENV_NAME,1,workers,downsample=False) # Nawid- Makes several different vectorised environment\n",
        "    \n",
        "    # Initialize the models\n",
        "    print('env action space',env.action_space.n)\n",
        "    env_model = NNDynamicModel(1 + feature_size, feature_size).to(device) # Nawid - Models need to be initialised with the represention size, used +1 as this is the dimensionality of the action space\n",
        "    #pytorch_total_params = sum(p.numel() for p in env_model.parameters() if p.requires_grad)\n",
        "    #print('env_model parameters',pytorch_total_params)\n",
        "\n",
        "    rew_model = NNRewardModel(1 + feature_size, 1).to(device) # Nawid- Used +1 as this is the dimensionality of the action space\n",
        "    #reward_total_params = sum(p.numel() for p in rew_model.parameters() if p.requires_grad)\n",
        "    #print('reward_model parameters',reward_total_params)\n",
        "    \n",
        "    \n",
        "    game_reward = 0\n",
        "    num_examples_added = len(rand_dataset)\n",
        "\n",
        "    for n_iter in range(AGGR_ITER):\n",
        "\n",
        "        # supervised training of the dataset (random and rl if it exists)\n",
        "        train_env_loss, train_rew_loss, valid_env_loss, valid_rew_loss, norm = train_dyna_model(rand_dataset, rl_dataset, env_model, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL, num_examples_added, ENV_LEARNING_RATE, REW_LEARNING_RATE, device)\n",
        "        print('{} >> Eloss:{:.4f} EV loss:{:.4f} -- Rloss:{:.4f} RV loss:{:.4f}'.format(n_iter, train_env_loss, valid_env_loss, train_rew_loss, valid_rew_loss))\n",
        "\n",
        "        obs = env.reset()\n",
        "        with torch.no_grad():\n",
        "            state = encoder(obs.float().to(device) / 255)\n",
        "            print('state',state.size())\n",
        "\n",
        "        num_examples_added = 0\n",
        "        game_reward = 0\n",
        "        game_pred_rews = []\n",
        "        rews = []\n",
        "        print('Does it work before the model-based control')\n",
        "\n",
        "        while num_examples_added < STEPS_PER_AGGR:\n",
        "            while True:\n",
        "\n",
        "                tt = time.time()\n",
        "                # Execute the control to roll the sequences and pick the first action of the sequence with the higher reward\n",
        "                action, pred_rew = multi_model_based_control(env_model, rew_model, state, NUM_ACTIONS_SEQUENCES, HORIZON_LENGTH, env.action_space.sample, norm, device)\n",
        "                game_pred_rews.append(pred_rew)\n",
        "                print('is an action chosen',action)\n",
        "                action = torch.tensor(action)\n",
        "                action = action.int() # Nawid - Need to change to an int in order to use the code\n",
        "                #print('action chosen',action,action.size(),action.dtype)\n",
        "\n",
        "                # one step in the environment with the action returned by the controller\n",
        "                new_obs, reward, done, _ = env.step(action)\n",
        "                print('is an action executed')\n",
        "\n",
        "                input_scaler, env_output_scaler, rew_output_scaler = norm\n",
        "\n",
        "                ## Compute the reward and print some stats\n",
        "                print('observation',obs.size())\n",
        "                print('action',action.size())\n",
        "                \n",
        "\n",
        "                \n",
        "\n",
        "                action = np.expand_dims(action,axis=0) # Nawid-  Need to use this to make the dimensions the same during the concatenation   \n",
        "\n",
        "                with torch.no_grad():\n",
        "                    state = encoder(obs.float().to(device) / 255)\n",
        "                    print('state',state.size())\n",
        "\n",
        "\n",
        "\n",
        "                models_input = input_scaler.transform([np.concatenate([obs, action])])\n",
        "                rew_model.eval()\n",
        "                p_rew = rew_model(torch.tensor(models_input).to(device))\n",
        "                rew_model.train(True)\n",
        "                unnorm_rew = rew_output_scaler.inverse_transform([float(p_rew.cpu().data[0])]).squeeze()\n",
        "                print('  >> ',len(game_pred_rews), 'gt:',np.round(reward,3), 'pred:',np.round(unnorm_rew, 3),\n",
        "                      'sum:', np.round(pred_rew,3), '|', game_reward, np.round(time.time()-tt, 4), HORIZON_LENGTH)\n",
        "\n",
        "                # add the last step to the RL dataset\n",
        "                rl_dataset.append([obs, new_obs, reward, done, action])\n",
        "\n",
        "                num_examples_added += 1\n",
        "                obs = new_obs\n",
        "                game_reward += reward\n",
        "                # if the environment is done, reset it and print some stats\n",
        "                if done:\n",
        "                    obs = env.reset()\n",
        "                    print('  >> R: {:.2f}, Mean sum:{:.2f}, {}'.format(game_reward, np.mean(game_pred_rews), num_examples_added))\n",
        "\n",
        "                    rews.append(game_reward)\n",
        "                    game_reward = 0\n",
        "                    game_pred_rews = []\n",
        "                    break\n",
        "\n",
        "        print('  >> Mean: {:.2f}', np.mean(rews))\n",
        "main()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "1\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[7]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[3]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[0]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[5]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[2]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[1]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[6]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[4]]) torch.Size([1, 1])\n",
            "action type used for the environment tensor([[8]]) torch.Size([1, 1])\n",
            "Mean R: 13.5 Max R: 1.0 175.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 4/55 [00:00<00:01, 37.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "env action space 9\n",
            "len(D): 350 len(Dtrain) 280\n",
            "does it work at this point\n",
            "X_train torch.Size([280, 257])\n",
            "y_env_train torch.Size([280, 256])\n",
            "y_rew_train torch.Size([280, 1])\n",
            "does it work after setting up the scaling\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 36%|███▋      | 20/55 [00:00<00:00, 49.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 36/55 [00:00<00:00, 59.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 51/55 [00:00<00:00, 64.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 55/55 [00:00<00:00, 67.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "Random noise is added\n",
            "X_train (280, 257)\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "Predictions can be made\n",
            "Does it work before reward prediction\n",
            "Does it work after reward prediction\n",
            "Does it work till this point\n",
            "input dimensions torch.Size([280, 257])\n",
            "after linear1 torch.Size([280, 512])\n",
            "after linear2 torch.Size([280, 256])\n",
            "after linear3 torch.Size([280, 256])\n",
            "0 >> Eloss:0.1016 EV loss:34.3577 -- Rloss:0.0311 RV loss:nan\n",
            "state torch.Size([1, 256])\n",
            "Does it work before the model-based control\n",
            "real obs torch.Size([1, 256])\n",
            "m_obs size torch.Size([20000, 256])\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "sampled actions (20000, 1)\n",
            "working before models_input\n",
            "working after model_input\n",
            "input dimensions torch.Size([20000, 257])\n",
            "after linear1 torch.Size([20000, 512])\n",
            "after linear2 torch.Size([20000, 256])\n",
            "after linear3 torch.Size([20000, 256])\n",
            "predicted observation (20000, 256)\n",
            "Does it work till this point\n",
            "arg best reward done 15040\n",
            "best sum reward one 583133891.7041156\n",
            "best action done [8.]\n",
            "is an action chosen [8.]\n",
            "is an action executed\n",
            "observation torch.Size([1, 1, 210, 160])\n",
            "action torch.Size([1])\n",
            "state torch.Size([1, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9443c7b7ac49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  >> Mean: {:.2f}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-9443c7b7ac49>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mmodels_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0mrew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mp_rew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrew_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 2 dimension(s)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3LNXv4bkECn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}