{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Model_based_Pacman_ATARI_labels_020420.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eKVfcEFEz9-t",
        "taJgI07xT7eo",
        "l0ahvG8lhSVp",
        "0jEdA4yE3GfE",
        "PpGt1rdvsLFT"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNQg21htiykC29uSsFCQ1LA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/Model_based_Pacman_ATARI_labels_020420.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4jLAGgyjflE",
        "colab_type": "text"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "97bb11f4-af04-44d0-94e3-05922aa989b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/mila-iqia/atari-representation-learning.git\n",
            "  Cloning git://github.com/mila-iqia/atari-representation-learning.git to /tmp/pip-req-build-9yettvlo\n",
            "  Running command git clone -q git://github.com/mila-iqia/atari-representation-learning.git /tmp/pip-req-build-9yettvlo\n",
            "Requirement already satisfied (use --upgrade to upgrade): atariari==0.0.1 from git+git://github.com/mila-iqia/atari-representation-learning.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (0.15.7)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.18.2)\n",
            "Collecting cloudpickle~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->atariari==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: atariari\n",
            "  Building wheel for atariari (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atariari: filename=atariari-0.0.1-cp36-none-any.whl size=46584 sha256=089ab6083c51c11fb18cec74c18eb74ced962f6c67343fac510e46ed79666cd9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ikhdrfvi/wheels/3d/69/51/5e436e5ae566c5b4dec5c53e65396d516459877a42a11d7aa4\n",
            "Successfully built atariari\n",
            "Installing collected packages: cloudpickle\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "Successfully installed cloudpickle-1.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-3m8wzb2v\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-3m8wzb2v\n",
            "Requirement already satisfied (use --upgrade to upgrade): a2c-ppo-acktr==0.0.1 from git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.15.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=9040f6252ea719f594f6221358c400ed409e6a9fa2c22f95207433d7f4c4152a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6hz2_h5_/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-z5cfvg0c\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-z5cfvg0c\n",
            "Requirement already satisfied (use --upgrade to upgrade): baselines==0.1.6 from git+git://github.com/openai/baselines in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: gym<0.16.0,>=0.15.4 in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.15.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.38.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=c673bed5b643b21dc2b317c8e3d802004acafdd9580cff07e84bb0bf0be21013\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tg84iqlh/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "Successfully built baselines\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.8.31)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.14.3)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.1)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.2.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (1.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZ9gMrxp5vU",
        "colab_type": "code",
        "outputId": "ecdf658a-910a-4b86-dbd4-1ca4f3addb50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI287_Yjxn3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, BatchSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from atariari.benchmark.envs import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from atariari.benchmark.wrapper import AtariARIWrapper\n",
        "\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Imported required for the Model-based RL\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "784fc808-0996-4d9d-cf16-473ae9a78fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login #########################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gdFb0bqHHN",
        "colab_type": "code",
        "outputId": "da04282e-a7b1-425f-a32f-801e56942dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "wandb.init(entity=\"nerdk312\", project=\"ATARI_LABELS_MPC\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/2vxgmasp\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/2vxgmasp</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/2vxgmasp"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnrvnbMcP6Xz",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9wLqaPmqqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'MsPacmanNoFrameskip-v4'\n",
        "\n",
        "feature_size = 14\n",
        "#feature_size = all_defaults['feature_size'] # Nawid- Dimensionality of the representation\n",
        "# workers = 8 # Nawid - Choosing the number of workers for the network\n",
        "\n",
        "# Main loop hyperp\n",
        "env_model_pretrain =  True\n",
        "rew_model_pretrain = False\n",
        "if env_model_pretrain:\n",
        "    pretrained_env_model = '/content/gdrive/My Drive/MsPacman-data/Env_model_4_9.47.41.pt'\n",
        "\n",
        "if rew_model_pretrain:\n",
        "    pretrained_rew_model = '/content/gdrive/My Drive/MsPacman-data/Rew_model_1000_randtraj_lr1e-4.pt'\n",
        "    AGGR_ITER = 1\n",
        "else:\n",
        "    AGGR_ITER = 10\n",
        "\n",
        "STEPS_PER_AGGR = 100000\n",
        "\n",
        "# Random MB hyperp\n",
        "NUM_RAND_TRAJECTORIES = 1000\n",
        "\n",
        "# 'cuda' or 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Supervised Model Hyperp\n",
        "ENV_LEARNING_RATE = 1e-3\n",
        "REW_LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1024\n",
        "TRAIN_ITER_MODEL =  100\n",
        "\n",
        "# Controller Hyperp\n",
        "HORIZON_LENGTH = 5\n",
        "NUM_ACTIONS_SEQUENCES = 200\n",
        "\n",
        "load_data = True\n",
        "if load_data:\n",
        "    loaded_trajectories = '/content/gdrive/My Drive/MsPacman-data/pacman_rand1000.npy'\n",
        "\n",
        "collect_data = False\n",
        "\n",
        "observation_channels = 1\n",
        "action_dim = 1\n",
        "n_actions = 5 #9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "reward_dim = 1\n",
        "\n",
        "# Time and date information\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n",
        "\n",
        "# Making the network deterministic - https://pytorch.org/docs/stable/notes/randomness.html\n",
        "random_seed = 0\n",
        "if random_seed:\n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    #np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKm_FKWGnPep",
        "colab_type": "text"
      },
      "source": [
        "# Saving config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5P-E36KnFoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = wandb.config\n",
        "config.batch_size = BATCH_SIZE          \n",
        "config.horizon_length = HORIZON_LENGTH\n",
        "config.num_action_seq = NUM_ACTIONS_SEQUENCES\n",
        "config.train_model_iter = TRAIN_ITER_MODEL \n",
        "config.num_rand_trajectories = NUM_RAND_TRAJECTORIES\n",
        "config.aggr_iter = AGGR_ITER\n",
        "config.steps_per_aggr = STEPS_PER_AGGR\n",
        "config.env_lr = ENV_LEARNING_RATE\n",
        "config.rew_lr = REW_LEARNING_RATE\n",
        "config.no_actions = n_actions\n",
        "config.load_data = load_data\n",
        "config.collect_data = collect_data\n",
        "config.env_model_pretrain = env_model_pretrain\n",
        "config.rew_model_pretrain = rew_model_pretrain\n",
        "config.random_seed = random_seed\n",
        "\n",
        "if load_data:\n",
        "    config.loaded_trajectories = loaded_trajectories\n",
        "\n",
        "if env_model_pretrain:\n",
        "    config.pretrained_env_model = pretrained_env_model\n",
        "if rew_model_pretrain:\n",
        "    config.pretrained_rew_model = pretrained_rew_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKVfcEFEz9-t",
        "colab_type": "text"
      },
      "source": [
        "# Model setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaBzmIUDnqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping_loss(object):\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, verbose=False, wandb=None, name=\"\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = 1e11 # Nawid - Set a very high initial best loss\n",
        "        self.name = name\n",
        "        self.wandb = wandb\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score >= self.best_score: # Nawid - Inverse signs to take into minimising loss instead of maximising accuracy\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping for {self.name} counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(f'{self.name} has stopped')\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f'Validation loss decreased/improved for {self.name}  ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "\n",
        "        save_dir = self.wandb.run.dir\n",
        "        torch.save(model.state_dict(), save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.wandb.save(save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGPEetVzTtOb",
        "colab": {}
      },
      "source": [
        "class NNDynamicsModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the next state, given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, obs_output_dim):\n",
        "        super(NNDynamicsModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, obs_output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "class NNRewardModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the reward given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, reward_output_dim):\n",
        "        super(NNRewardModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, reward_output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "def model_MSEloss(y_pred,y_truth, device):\n",
        "    '''\n",
        "    Compute the MSE (Mean Squared Error)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))\n",
        "\n",
        "def model_CEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the CEloss\n",
        "    '''\n",
        "    y_truth = torch.Tensor(np.array(y_truth)).to(device)\n",
        "    return F.cross_entropy(y_pred, y_truth)\n",
        "\n",
        "def model_BCEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the BCE (Binary cross entropy)\n",
        "    param y_pred: y_pred is a 2 dimensional n x 1 data tensor\n",
        "    param y_truth: y_truth is a 2D numpy array which later gets converted into a tensor\n",
        "    '''\n",
        "    n,c = y_pred.size()\n",
        "    weights = np.zeros((n,c))\n",
        "    pos = y_truth[y_truth==1].sum()\n",
        "    neg = n - pos\n",
        "    #nx = y_truth.cpu().data.numpy()\n",
        "    index = np.where(y_truth == 1)[0]\n",
        "    weights[:] = 1 #1/neg\n",
        "    weights[index] = neg* 1/pos\n",
        "    weights = torch.Tensor(weights).to(device)\n",
        "    #print('y_pred',y_pred.size())\n",
        "    #print('weights',weights.size())\n",
        "\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    #print('y_truth',y_truth.size())\n",
        "    return F.binary_cross_entropy(y_pred.view(-1).float(), y_truth.view(-1),weights.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRn5uXDhgmQs",
        "colab_type": "text"
      },
      "source": [
        "# General Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCHWWzZ_f5GV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(i):\n",
        "    a = np.zeros(n_actions, 'uint8')\n",
        "    a[i] = 1\n",
        "    return a\n",
        "\n",
        "def normalise(train_data, val_data, scaler = None): # Nawid - Used to normalise each dimension individually\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        train_data = scaler.fit_transform(train_data)\n",
        "    else: \n",
        "        train_data = scaler.transform(train_data)\n",
        "        \n",
        "    val_data = scaler.transform(val_data)\n",
        "    return train_data, val_data, scaler\n",
        "\n",
        "def normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data,X_env_obs_scaler = None,y_env_scaler=None,X_rew_obs_scaler= None):\n",
        "    # Unpack data\n",
        "    (X_env_train_obs, X_env_train_act, y_env_train), (X_env_val_obs, X_env_val_act, y_env_val) = env_train_data, env_val_data\n",
        "    (X_rew_train_obs, X_rew_train_act, y_rew_train), (X_rew_val_obs, X_rew_val_act, y_rew_val) = rew_train_data, rew_val_data\n",
        "    \n",
        "    # Normalise training and validation data\n",
        "    X_env_train_obs,X_env_val_obs, X_env_obs_scaler =  normalise(X_env_train_obs, X_env_val_obs, X_env_obs_scaler)\n",
        "    y_env_train, y_env_val, y_env_scaler = normalise(y_env_train, y_env_val, y_env_scaler)\n",
        "    X_rew_train_obs, X_rew_val_obs, X_rew_obs_scaler = normalise(X_rew_train_obs, X_rew_val_obs, X_rew_obs_scaler)\n",
        "    \n",
        "    # Concatentates the normalised states with the one hot vector for the actions\n",
        "    X_env_train = np.concatenate((X_env_train_obs,X_env_train_act),axis=1)\n",
        "    X_env_val = np.concatenate((X_env_val_obs,X_env_val_act),axis=1)\n",
        "    X_rew_train = np.concatenate((X_rew_train_obs,X_rew_train_act),axis=1)\n",
        "    X_rew_val = np.concatenate((X_rew_val_obs,X_rew_val_act),axis=1)\n",
        "\n",
        "    # Pack data tuples\n",
        "    env_train_data, env_val_data = (X_env_train, y_env_train),(X_env_val, y_env_val) \n",
        "    rew_train_data, rew_val_data = (X_rew_train, y_rew_train),(X_rew_val, y_rew_val)\n",
        "\n",
        "    return env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_obs_scaler, y_env_scaler, X_rew_obs_scaler\n",
        "\n",
        "#normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJgI07xT7eo",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzpOMJFWs9MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data_collection():\n",
        "    def __init__(self,ENV_NAME,feature_size,state_mode = 'ATARIARI', encoder = None):\n",
        "        self.ENV_NAME = ENV_NAME\n",
        "        self.state_mode = state_mode\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = encoder\n",
        "        self.repeated_initial = True # Nawid - set repeated initial as true initially\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def gather_random_trajectories(self,num_traj):\n",
        "\n",
        "        dataset_random = []\n",
        "        #Env name could either be the RAM case or the generic case\n",
        "        env = AtariARIWrapper(gym.make(self.ENV_NAME)) \n",
        "        i = 0\n",
        "        for n in range(num_traj):\n",
        "            if n % 10 ==0:\n",
        "                print('trajectory number :',n)\n",
        "            # Initial set up\n",
        "            obs = env.reset()\n",
        "            self.repeated_initial = True # Nawid- Used to represent the initial state\n",
        "            initial_info_labels = env.labels()\n",
        "            info_labels = env.labels() # Nawid -  Used to get the current state\n",
        "            #print('trajectory number',n)\n",
        "            \n",
        "            while True:\n",
        "                # Choosing action and env step\n",
        "                sampled_action = np.random.randint(0,n_actions)\n",
        "                sampled_action_one_hot = one_hot(sampled_action)\n",
        "                next_obs, reward, done, next_info = env.step(sampled_action)\n",
        "                #print('done after env step', done)\n",
        "                next_info_labels = next_info['labels']\n",
        "                # self.repeated initial is set to true at first and it is is turned to true in the reward_collection class when done is true\n",
        "\n",
        "                if self.repeated_initial: # If the initial state is repeating\n",
        "                    if initial_info_labels == next_info_labels: # Current state is still the same as the initial state\n",
        "                        pass\n",
        "                    else:\n",
        "                        # New state achieved, so save data\n",
        "                        self.repeated_initial = False\n",
        "                    \n",
        "                        state, next_state = self.state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                        reward, pacman_done = self.reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                        #print('pacman_done', pacman_done)\n",
        "                        dataset_random.append([state, next_state,reward,pacman_done,sampled_action_one_hot])\n",
        "                    \n",
        "                        obs = next_obs\n",
        "                        info_labels = next_info_labels\n",
        "\n",
        "                else:\n",
        "                    # Save data as this is when the initial state and the next state are not identical\n",
        "                    state, next_state = self.state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                    reward, pacman_done = self.reward_collection(reward, done, info_labels, next_info_labels) # Nawid - Changed name to pacman done as the variable done is used to exit the loop\n",
        "                    dataset_random.append([state, next_state,reward,pacman_done,sampled_action_one_hot])\n",
        "                \n",
        "                    obs = next_obs\n",
        "                    info_labels = next_info_labels\n",
        "                    # Used to test whether the done behaviour was correct\n",
        "                    \n",
        "                    '''if pacman_done:\n",
        "                        i +=1 \n",
        "                        print('pacman done', i)'''\n",
        "                    \n",
        "\n",
        "                if done:\n",
        "                    break \n",
        "        '''               \n",
        "        #dataset_random = np.array(dataset_random)\n",
        "        first_dataset = np.array(dataset_random[0])\n",
        "        print(first_dataset[0])\n",
        "        print('zero',dataset_random[0])\n",
        "        print('One',dataset_random[1])\n",
        "        print('Two',dataset_random[2])\n",
        "        print('Three',dataset_random[3])\n",
        "        print('Four',dataset_random[4])\n",
        "        #print(dataset_random.shape)\n",
        "        '''\n",
        "        return dataset_random\n",
        "    \n",
        "\n",
        "    def state_collection(self,obs, next_obs, info_labels, next_info_labels):\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            state = []\n",
        "            next_state = []\n",
        "            i = 0 \n",
        "            for key in info_labels :\n",
        "                if i < self.feature_size: # Nawid - Only the first 14 info are crucial I believe\n",
        "                    state.append(info_labels[key])\n",
        "                    next_state.append(next_info_labels[key])\n",
        "                    i +=1\n",
        "                else:\n",
        "                    state = np.array(state)\n",
        "                    next_state = np.array(next_state)\n",
        "                    return state, next_state\n",
        "    \n",
        "        elif self.state_mode == 'STDIM': # Encoding is the state\n",
        "            assert self.encoder is not None\n",
        "            with torch.no_grad():\n",
        "                self.encoder.eval()\n",
        "                state = self.encoder(obs.float().to(self.device) / 255)\n",
        "                next_state = self.encoder(next_obs.float().to(self.device) / 255)\n",
        "                return state.cpu().detach().numpy(), next_state.cpu().detach().numpy()\n",
        "\n",
        "        else: # Image observations are the state or the RAM labels\n",
        "            return obs, next_obs\n",
        "\n",
        "    def reward_collection(self,reward,done,info_labels,next_info_labels):\n",
        "        # checks whether there has been a change in lives- change done to 1, otherwise the done should be fine regardless\n",
        "        if not info_labels['num_lives'] == next_info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "            #print('change in lives')\n",
        "            done = True \n",
        "            self.repeated_initial = True\n",
        "        return reward, done\n",
        "\n",
        "    def unison_shuffled_copies(self,*args): # Nawid- Randomises all the different values, using *args to use a variable number of parameters so i can shuffle many different values at once\n",
        "        p = np.random.permutation(len(args[0]))\n",
        "        shuffled = [i[p] for i in args]\n",
        "        return shuffled\n",
        "    \n",
        "    def collate_data(self,random_dataset, rl_dataset):\n",
        "        if len(rl_dataset) > 0:\n",
        "                \n",
        "            random_dataset = np.array(random_dataset)\n",
        "            rl_dataset = np.array(rl_dataset)\n",
        "            print('random_dataset', random_dataset.shape)\n",
        "            print('rl dataset', rl_dataset.shape)\n",
        "        \n",
        "            d_concat = np.concatenate([random_dataset, rl_dataset], axis=0)\n",
        "            print('d_concat',d_concat.shape)        \n",
        "        else:\n",
        "            d_concat = np.array(random_dataset)\n",
        "        \n",
        "        num_examples_added = len(d_concat)\n",
        "\n",
        "        # Split the dataset into train(80%) and test(20%)\n",
        "        D_train = d_concat[:int(-num_examples_added*1/5)]\n",
        "        D_valid = d_concat[int(-num_examples_added*1/5):]\n",
        "\n",
        "        print(\"len(D):\", len(d_concat), 'len(Dtrain)', len(D_train))\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        sff = np.arange(len(D_train))\n",
        "        np.random.shuffle(sff)\n",
        "        D_train = D_train[sff]\n",
        "\n",
        "        # Create the input and output for the train\n",
        "        X_train_obs = np.array([obs for obs,_,_,_,_ in D_train]) # Takes obs and action\n",
        "        X_train_act = np.array([act for _,_,_,_,act in D_train])\n",
        "\n",
        "        # Env output\n",
        "        y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "        y_env_train = y_env_train - np.array([obs for obs,_,_,_,_ in D_train]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        # Reward's output\n",
        "        y_rew_train = np.array([[D] for _,_,_,D,_ in D_train])\n",
        "    \n",
        "        # Next state output\n",
        "        X_val_obs = np.array([obs for obs,_,_,_,_ in D_valid]) # Takes obs and action\n",
        "        X_val_act = np.array([act for _,_,_,_,act in D_valid])\n",
        "\n",
        "        y_env_val = np.array([no for _,no,_,_,_ in D_valid])\n",
        "        y_env_val = y_env_val - np.array([obs for obs,_,_,_,_ in D_valid]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        # Reward output\n",
        "        y_rew_val = np.array([[D] for _,_,_,D,_ in D_valid])\n",
        "    \n",
        "        env_train_data, env_val_data = (X_train_obs, X_train_act, y_env_train), (X_val_obs, X_val_act, y_env_val)\n",
        "        rew_train_data, rew_val_data = (X_train_obs, X_train_act, y_rew_train), (X_val_obs, X_val_act, y_rew_val)\n",
        "\n",
        "        return env_train_data, env_val_data, rew_train_data, rew_val_data \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#data_collector = Data_collection('MsPacmanNoFrameskip-v4',feature_size)\n",
        "#random_data = data_collector.gather_random_trajectories(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ahvG8lhSVp",
        "colab_type": "text"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3T5LHY6hRW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_data,val_data,model,batch_size, max_model_iter, optimizer, device,early_stopper,desired_model='Env_model'):\n",
        "    ''' \n",
        "    General function to train either of the two models\n",
        "    '''\n",
        "    # Unpack data\n",
        "    (X_train, y_train), (X_val, y_val) =  train_data, val_data\n",
        "    losses_env = []\n",
        "\n",
        "    # Choose loss function based on what type of model is training\n",
        "    if desired_model =='Env_model': # Nawid -  Decides which loss function to use\n",
        "        loss_function = model_MSEloss\n",
        "    else:\n",
        "        loss_function = model_BCEloss\n",
        "        #print('BCE being used')\n",
        "\n",
        "    # go through max_model iter supervised iterations\n",
        "    for it in tqdm(range(max_model_iter)):\n",
        "        # create mini batches of size batch_size\n",
        "        for mb in range(0,len(X_train), batch_size): # Nawid- Batch size is the step size\n",
        "            if len(X_train) > mb + BATCH_SIZE:\n",
        "                X_mb = X_train[mb:mb+BATCH_SIZE]\n",
        "                y_mb = y_train[mb:mb+BATCH_SIZE]\n",
        "                #X_mb += np.random.normal(loc = 0, scale = 0.001, size= X_mb.shape)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # forward pass of model to compute the output\n",
        "                pred_mb = model(torch.tensor(X_mb).to(device))\n",
        "                \n",
        "                # compute the loss\n",
        "                loss = loss_function(pred_mb,y_mb,device) #Nawid-  Uses Mse loss if dynamics model or uses BCE loss if reward/done model\n",
        "                wandb.log({'{} Training loss'.format(desired_model):loss.cpu().detach().numpy()})\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                # optimization step\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Nawid - Calculate the validation loss after each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_val = model(torch.tensor(X_val).to(device))\n",
        "            val_loss = loss_function(pred_val, y_val,device)\n",
        "            wandb.log({'{} Validation loss'.format(desired_model):val_loss})\n",
        "        \n",
        "\n",
        "        # Checks whether to early stop after each epoch\n",
        "        early_stopper(val_loss,model)\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jEdA4yE3GfE",
        "colab_type": "text"
      },
      "source": [
        "# Controller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOzwWiaiRxQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class multi_model_based_control():\n",
        "    \n",
        "    def __init__(self,ENV_NAME, env_model, rew_model,num_sequences, horizon_length, n_actions, norm, num_features = feature_size, state_mode ='ATARIARI'):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.state_mode = state_mode\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            self.env = AtariARIWrapper(gym.make(ENV_NAME))\n",
        "            self.initial_info_labels = self.env.labels()\n",
        "        \n",
        "        self.env_model = env_model\n",
        "        self.rew_model = rew_model\n",
        "        self.horizon_length = horizon_length\n",
        "        self.num_sequences = num_sequences\n",
        "        self.repeated_initial = True # Nawid - set repeated initial as true initially\n",
        "        self.env_input_scaler, self.env_output_scaler, self.rew_input_scaler = norm\n",
        "        self.n_actions = n_actions\n",
        "        self.num_features = num_features\n",
        "        self.state_mode = state_mode\n",
        "        \n",
        "    def random_sampling_shooting(self,real_obs):\n",
        "        '''\n",
        "        Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "        '''\n",
        "        best_reward = -1e9\n",
        "        best_next_action = []\n",
        "        m_obs = np.array([real_obs for _ in range(self.num_sequences)])\n",
        "\n",
        "        # array that contains the rewards for all the sequence\n",
        "        unroll_rewards = np.zeros((self.num_sequences, 1)) \n",
        "        first_sampled_actions = []\n",
        "\n",
        "        self.env_model.eval()\n",
        "        self.rew_model.eval()\n",
        "\n",
        "        # Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon length' times\n",
        "        ## i.e roll a given number of trajectories in a single batch (to increase speed)\n",
        "        for t in range(self.horizon_length):\n",
        "            # sample actions for each sequence\n",
        "            sampled_actions = np.array([np.random.randint(0,self.n_actions) for _ in range(self.num_sequences)])\n",
        "            sampled_actions_one_hot = np.array([one_hot(action) for action in sampled_actions])\n",
        "            if isinstance(self.env_model,NNDynamicsModel): # Nawid-  If the env model is a neural net\n",
        "                #print('using dynamics model')\n",
        "                m_obs_env_scaled = self.env_input_scaler.transform(m_obs)\n",
        "                env_model_input = np.concatenate([m_obs_env_scaled, sampled_actions_one_hot], axis = 1)\n",
        "                # compute the next state for each sequence\n",
        "                pred_obs = self.env_model(torch.tensor(env_model_input).to(self.device))\n",
        "\n",
        "                # inverse scaler transformation\n",
        "                pred_obs = self.env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
        "                # add previous observation\n",
        "                next_obs = pred_obs + m_obs\n",
        "            else:\n",
        "                #print('using oracle state')            \n",
        "                next_obs = self.env_model.predict_states(m_obs,sampled_actions) # Nawid - Able to obtain the next state directly rather than predicting a change in states\n",
        "\n",
        "            if isinstance(self.rew_model, NNRewardModel):\n",
        "                m_obs_rew_scaled = self.rew_input_scaler.transform(m_obs)\n",
        "                rew_model_input = np.concatenate([m_obs_rew_scaled, sampled_actions_one_hot], axis = 1)\n",
        "                pred_rew = self.rew_model(torch.tensor(rew_model_input).to(self.device)) # Nawid -  I believe I do not need to rescale for a True of false situation\n",
        "                unroll_rewards += (1 - pred_rew.cpu().detach().numpy())\n",
        "            else:\n",
        "                #print('using oracle reward')\n",
        "                pred_rew = self.rew_model.predict_reward(m_obs, sampled_actions,next_obs)\n",
        "                unroll_rewards += pred_rew\n",
        "        \n",
        "            m_obs = next_obs # Nawid - Update the state after calculating the new state and calculating the rewards\n",
        "\n",
        "            if t ==0:\n",
        "                first_sampled_actions = sampled_actions\n",
        "        \n",
        "        self.env_model.train()\n",
        "        self.rew_model.train()\n",
        "\n",
        "        # Best the position of the sequence with the higher reward\n",
        "        arg_best_reward = np.argmax(unroll_rewards)\n",
        "        best_sum_reward = unroll_rewards[arg_best_reward].squeeze()\n",
        "        # take the first action of this sequence\n",
        "        best_action = first_sampled_actions[arg_best_reward]\n",
        "        #best_action =  np.squeeze(best_action)\n",
        "        return best_action, best_sum_reward\n",
        "    \n",
        "    def check_initial(self, next_info_labels):\n",
        "        # Checks if there has been a change from the initial info labels to show that the initial lag period is over \n",
        "        if self.initial_info_labels == next_info_labels:\n",
        "            pass\n",
        "        else:\n",
        "            self.repeated_initial = False\n",
        "    \n",
        "    def state_conversion(self,obs, info_labels):\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            state = []\n",
        "            i = 0 \n",
        "            for key in info_labels :\n",
        "                if i < self.num_features: # Nawid - Only the first 14 info are crucial I believe\n",
        "                    state.append(info_labels[key])\n",
        "                    i +=1\n",
        "                else:\n",
        "                    state = np.array(state)\n",
        "                    return state\n",
        "    \n",
        "        elif self.state_mode == 'STDIM': # Encoding is the state\n",
        "            assert self.encoder is not None\n",
        "            with torch.no_grad():\n",
        "                self.encoder.eval()\n",
        "                state = self.encoder(obs.float().to(self.device) / 255)\n",
        "                return state.cpu().detach().numpy()\n",
        "        else: # Image observations are the state or the RAM labels\n",
        "            return obs\n",
        "    \n",
        "    def reward_conversion(self,reward,done,info_labels,next_info_labels):\n",
        "        # checks whether there has been a change in lives- change done to 1, otherwise the done should be fine regardless\n",
        "        if not info_labels['num_lives'] == next_info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "            done = True \n",
        "            self.repeated_initial = True\n",
        "            self.initial_info_labels  = next_info_labels # Sets the new initial labels.\n",
        "        return reward, done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIBf1pknbf_",
        "colab_type": "text"
      },
      "source": [
        "# Main - Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk1xCx6x7NZ2",
        "colab_type": "text"
      },
      "source": [
        "Data collection and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVutEvgUib8y",
        "colab_type": "code",
        "outputId": "f65d5a40-13a5-45cf-a69d-af4082199db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "data_collector = Data_collection(ENV_NAME,feature_size)\n",
        "#rand_dataset = data_collector.gather_random_trajectories(NUM_RAND_TRAJECTORIES)\n",
        "if load_data:\n",
        "    rand_dataset = np.load(loaded_trajectories,allow_pickle=True)\n",
        "else:\n",
        "    rand_dataset = data_collector.gather_random_trajectories(1000)\n",
        "\n",
        "rl_dataset = []\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = data_collector.collate_data(rand_dataset,rl_dataset)\n",
        "\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "norm = (X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(D): 1653880 len(Dtrain) 1323104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJi8pwC27Q5N",
        "colab_type": "text"
      },
      "source": [
        "Dynamics model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h62P5907GbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "\n",
        "\n",
        "if env_model_pretrain:\n",
        "    env_model.load_state_dict(torch.load(pretrained_env_model))\n",
        "    env_model.eval()\n",
        "    \n",
        "else:\n",
        "    env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "    wandb.watch(env_model, log=\"all\")\n",
        "    env_model_name = 'Env_model'+ '_' + date_time\n",
        "    early_stopping_env = EarlyStopping_loss(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "    \n",
        "    for n_iter in range(AGGR_ITER):\n",
        "        if early_stopping_env.early_stop:\n",
        "            print('Early stopping')\n",
        "            break\n",
        "        train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTOm9XOX7kOL",
        "colab_type": "text"
      },
      "source": [
        "# Training the reward function and MPC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9DeBR47Lpo",
        "colab_type": "code",
        "outputId": "a05ae865-1de8-4fcb-e2d7-f6dbb42a79aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "collect_data = False\n",
        "rew_model = NNRewardModel(n_actions + feature_size, reward_dim).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "\n",
        "if rew_model_pretrain:\n",
        "    rew_model.load_state_dict(torch.load(pretrained_rew_model))\n",
        "\n",
        "rew_optimizer = torch.optim.Adam(rew_model.parameters(),REW_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "wandb.watch(rew_model, log=\"all\")\n",
        "Rew_model_name = 'Rew_model'+ '_' + date_time\n",
        "early_stopping_rew = EarlyStopping_loss(patience=5, verbose=True, wandb=wandb, name=Rew_model_name)\n",
        "\n",
        "#env = AtariARIWrapper(gym.make(ENV_NAME))  # Instantiate environment\n",
        "Controller = multi_model_based_control(ENV_NAME,env_model, rew_model, NUM_ACTIONS_SEQUENCES, HORIZON_LENGTH, n_actions, norm)\n",
        "global_step = 0\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_rew.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "\n",
        "    if not rew_model_pretrain:\n",
        "        train_model(norm_rew_train_data, norm_rew_val_data, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL,rew_optimizer,device,early_stopping_rew,desired_model='Rew_model')\n",
        "\n",
        "    \n",
        "    if collect_data:\n",
        "        env_train_data, env_val_data, rew_train_data, rew_val_data = data_collector.collate_data(rand_dataset,rl_dataset)\n",
        "        norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "\n",
        "    obs = Controller.env.reset()\n",
        "    #Controller.env.seed(0) # Set the random seed of the environment\n",
        "    #obs = env.reset()\n",
        "    #initial_info_labels = env.labels()\n",
        "\n",
        "    num_examples_added = 0\n",
        "    game_reward = 0\n",
        "    # records how long the agent survives for\n",
        "    timesteps = 0\n",
        "    controller_pred_rews = []\n",
        "    rews = []\n",
        "    # records how many simulations have occurred\n",
        "     \n",
        "    #i = 0 \n",
        "    while num_examples_added < STEPS_PER_AGGR:\n",
        "    #while num_examples_added < STEPS_PER_AGGR:\n",
        "        while True:\n",
        "            tt = time.time()\n",
        "            \n",
        "            if Controller.repeated_initial:\n",
        "                #i += 1\n",
        "                #print(i)\n",
        "                obs,_,_,info = Controller.env.step(0) # Any action taken\n",
        "                info_labels = info['labels']\n",
        "                # Checks if the initial set is being repeated and if it isnt, it sets the value off\n",
        "                Controller.check_initial(info_labels) \n",
        "            else:\n",
        "                # new state achieved\n",
        "                state = Controller.state_conversion(obs,info_labels)\n",
        "                action, pred_rew = Controller.random_sampling_shooting(state)\n",
        "                action_one_hot = one_hot(action)\n",
        "                controller_pred_rews.append(pred_rew)\n",
        "\n",
        "                # one step in the environment with the action returned by\n",
        "                next_obs, reward,done, next_info = Controller.env.step(action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                \n",
        "                next_state = Controller.state_conversion(next_obs, next_info_labels)\n",
        "                # Obtains the reward, done and sets whether repeated initial should be true or not\n",
        "                reward, pacman_done = Controller.reward_conversion(reward,done,info_labels, next_info_labels) \n",
        "\n",
        "                # add to the RL dataset                \n",
        "                rl_dataset.append([state, next_state, reward, pacman_done, action_one_hot])\n",
        "\n",
        "                num_examples_added += 1\n",
        "                timesteps +=1 \n",
        "                obs = next_obs\n",
        "                info_labels = next_info_labels\n",
        "\n",
        "                game_reward += reward \n",
        "                if done:\n",
        "                    global_step += 1\n",
        "                    obs = Controller.env.reset()\n",
        "                    #Controller.env.seed(0) # Need to set the random seed after the environment is done\n",
        "                    wandb.log({'game reward':game_reward, 'pred_rew': np.mean(controller_pred_rews),'survival time':timesteps,'global step':global_step })\n",
        "                    print('  >> R: {:.2f}, Mean sum:{:.2f},Survival time: {},Num examples:{}'.format(game_reward, np.mean(controller_pred_rews),timesteps,num_examples_added))\n",
        "                    rews.append(game_reward)\n",
        "                    game_reward = 0\n",
        "                    timesteps = 0\n",
        "                    controller_pred_rews = []\n",
        "                    break\n",
        "\n",
        "    print('  >> Mean: {:.2f}', np.mean(rews))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: RuntimeWarning: divide by zero encountered in long_scalars\n",
            "  1%|          | 1/100 [00:14<24:41, 14.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (100000000000.000000 --> 0.910245).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 2/100 [00:29<24:21, 14.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.910245 --> 0.860144).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 3/100 [00:44<24:08, 14.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.860144 --> 0.827346).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 4/100 [00:59<23:53, 14.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.827346 --> 0.811336).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 5/100 [01:14<23:24, 14.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.811336 --> 0.795112).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 6/100 [01:28<22:52, 14.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.795112 --> 0.782842).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 7/100 [01:43<22:41, 14.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.782842 --> 0.779994).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 8/100 [01:58<22:42, 14.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.779994 --> 0.773215).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 9/100 [02:12<22:15, 14.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.773215 --> 0.772186).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 10/100 [02:27<22:00, 14.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.772186 --> 0.769486).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 11/100 [02:42<21:50, 14.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.769486 --> 0.768720).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 12/100 [02:56<21:36, 14.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_7_20.12.3  (0.768720 --> 0.766038).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 13/100 [03:11<21:24, 14.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_7_20.12.3 counter: 1 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 14/100 [03:26<21:15, 14.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_7_20.12.3 counter: 2 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 15/100 [03:41<20:56, 14.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_7_20.12.3 counter: 3 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 16/100 [03:55<20:35, 14.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_7_20.12.3 counter: 4 out of 5\n",
            "EarlyStopping for Rew_model_7_20.12.3 counter: 5 out of 5\n",
            "Rew_model_7_20.12.3 has stopped\n",
            "Early stopping\n",
            "  >> R: 450.00, Mean sum:4.42,Survival time: 1794,Num examples:1794\n",
            "  >> R: 880.00, Mean sum:4.58,Survival time: 2195,Num examples:3989\n",
            "  >> R: 450.00, Mean sum:4.42,Survival time: 1795,Num examples:5784\n",
            "  >> R: 450.00, Mean sum:4.42,Survival time: 1795,Num examples:7579\n",
            "  >> R: 450.00, Mean sum:4.42,Survival time: 1795,Num examples:9374\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:10985\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:12596\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:14207\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:15818\n",
            "  >> R: 450.00, Mean sum:4.42,Survival time: 1795,Num examples:17613\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:19224\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:20835\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:22446\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:24057\n",
            "  >> R: 450.00, Mean sum:4.42,Survival time: 1795,Num examples:25852\n",
            "  >> R: 510.00, Mean sum:4.34,Survival time: 2019,Num examples:27871\n",
            "  >> R: 450.00, Mean sum:4.38,Survival time: 1795,Num examples:29666\n",
            "  >> R: 900.00, Mean sum:4.56,Survival time: 2371,Num examples:32037\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:33648\n",
            "  >> R: 450.00, Mean sum:4.42,Survival time: 1795,Num examples:35443\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:37054\n",
            "  >> R: 260.00, Mean sum:4.39,Survival time: 1627,Num examples:38681\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:40292\n",
            "  >> R: 260.00, Mean sum:4.39,Survival time: 1627,Num examples:41919\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:43530\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:45141\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:46752\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:48363\n",
            "  >> R: 880.00, Mean sum:4.49,Survival time: 2187,Num examples:50550\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:52161\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:53772\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:55383\n",
            "  >> R: 210.00, Mean sum:4.46,Survival time: 1195,Num examples:56578\n",
            "  >> R: 260.00, Mean sum:4.39,Survival time: 1627,Num examples:58205\n",
            "  >> R: 260.00, Mean sum:4.39,Survival time: 1627,Num examples:59832\n",
            "  >> R: 450.00, Mean sum:4.41,Survival time: 1795,Num examples:61627\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:63238\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:64849\n",
            "  >> R: 210.00, Mean sum:4.46,Survival time: 1195,Num examples:66044\n",
            "  >> R: 880.00, Mean sum:4.58,Survival time: 2195,Num examples:68239\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:69850\n",
            "  >> R: 910.00, Mean sum:4.45,Survival time: 2411,Num examples:72261\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:73872\n",
            "  >> R: 920.00, Mean sum:4.65,Survival time: 2091,Num examples:75963\n",
            "  >> R: 450.00, Mean sum:4.48,Survival time: 1795,Num examples:77758\n",
            "  >> R: 260.00, Mean sum:4.39,Survival time: 1627,Num examples:79385\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:80996\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:82607\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:84218\n",
            "  >> R: 910.00, Mean sum:4.45,Survival time: 2411,Num examples:86629\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:88240\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:89851\n",
            "  >> R: 150.00, Mean sum:4.61,Survival time: 747,Num examples:90598\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:92209\n",
            "  >> R: 440.00, Mean sum:4.46,Survival time: 1963,Num examples:94172\n",
            "  >> R: 910.00, Mean sum:4.45,Survival time: 2411,Num examples:96583\n",
            "  >> R: 450.00, Mean sum:4.42,Survival time: 1795,Num examples:98378\n",
            "  >> R: 250.00, Mean sum:4.58,Survival time: 1611,Num examples:99989\n",
            "  >> R: 320.00, Mean sum:4.48,Survival time: 2379,Num examples:102368\n",
            "  >> Mean: {:.2f} 381.864406779661\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpGt1rdvsLFT",
        "colab_type": "text"
      },
      "source": [
        "# OLD CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqE-9CLYK6jF",
        "colab_type": "code",
        "outputId": "e1201452-eabb-4faf-9408-b915bd48d326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "\n",
        "encoder = NatureCNN(observation_channels,all_defaults)\n",
        "encoder.load_state_dict(torch.load('/content/MsPacmanNoFrameskip-v4_55.pt'))\n",
        "encoder.to(device)\n",
        "encoder.eval()\n",
        "rand_dataset = gather_random_trajectories(10000,'MsPacmanNoFrameskip-v4',encoder)\n",
        "rl_dataset = []\n",
        "num_examples_added = len(rand_dataset)\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(rand_dataset,rl_dataset,num_examples_added)\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "norm = (X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "\n",
        "env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "wandb.watch(env_model, log=\"all\")\n",
        "env_model_name = 'Env_model'+ '_' + date_time\n",
        "early_stopping_env = EarlyStopping(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_env.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▎         | 2/55 [00:00<00:18,  2.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy increased for Env_model_27_13.53.44  (0.000000 --> 1.118334).  Saving model ...\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 1 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 4/55 [00:00<00:11,  4.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_27_13.53.44 counter: 2 out of 5\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 3 out of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 5/55 [00:01<00:10,  4.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Env_model_27_13.53.44 counter: 4 out of 5\n",
            "EarlyStopping for Env_model_27_13.53.44 counter: 5 out of 5\n",
            "Env_model_27_13.53.44 has stopped\n",
            "Early stopping\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGXOh2oVWT8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_random_trajectories(num_traj,ENV_NAME,mode = 'ATARIARI', encoder = None):\n",
        "    '''\n",
        "    states_list = []\n",
        "    next_states_list = []\n",
        "    rewards_list = []\n",
        "    dones_list = []\n",
        "    actions_list = []\n",
        "    '''\n",
        "    dataset_random = []\n",
        "    #Env name could either be the RAM case or the generic case\n",
        "    env = AtariARIWrapper(gym.make(ENV_NAME)) \n",
        "\n",
        "    for n in range(num_traj):\n",
        "        obs = env.reset()\n",
        "        repeated_initial = True # Nawid- Used to represent the initial state\n",
        "        initial_info_labels = env.labels()\n",
        "        info_labels = env.labels() # Nawid -  Used to get the current state\n",
        "        while True:\n",
        "            sampled_action = np.random.randint(0,n_actions)\n",
        "            sampled_action_one_hot = one_hot(sampled_action)\n",
        "            next_obs, reward, done, next_info = env.step(sampled_action)\n",
        "            next_info_labels = next_info['labels']\n",
        "\n",
        "            if repeated_initial: # If the initial state is repeating\n",
        "                if initial_info_labels == next_info_labels: # Current state is still the same as the initial state\n",
        "                    pass\n",
        "                else:\n",
        "                    repeated_initial = False\n",
        "                    \n",
        "                    state, next_state = state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                    reward, done = reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                    dataset_random.append([state, next_state,reward,done,sampled_action_one_hot])\n",
        "                    \n",
        "                    obs = next_obs\n",
        "                    info_labels = next_info_labels\n",
        "\n",
        "                    # Go into a new state and from the previous state and the current state as this is a transition between the states\n",
        "\n",
        "            else:\n",
        "                # Save data as this is when the initial state and the next state are not identical\n",
        "                state, next_state = state_collection(obs, next_obs,info_labels, next_info_labels)\n",
        "                reward, done = reward_collection(reward, done, info_labels, next_info_labels)\n",
        "                dataset_random.append([state, next_state,reward,done,sampled_action_one_hot])\n",
        "                \n",
        "                obs = next_obs\n",
        "                info_labels = next_info_labels\n",
        "\n",
        "                if not initial_info_labels['num_lives'] == info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "                    print('change in lives')\n",
        "                    repeated_initial = True\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWnL4S9xsedw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def state_collection(obs, next_obs, info_labels, next_info_labels,mode='ATARIARI', encoder = None):\n",
        "    if mode == 'ATARIARI':\n",
        "        state = []\n",
        "        next_state = []\n",
        "        i = 0 \n",
        "        for key in info_labels :\n",
        "            if i < 14: # Nawid - Only the first 14 info are crucial I believe\n",
        "                state.append(info_labels[key])\n",
        "                next_state.append(info_labels[key])\n",
        "                i +=1\n",
        "            else:\n",
        "                state = np.array(state)\n",
        "                next_state = np.array(next_state)\n",
        "                return state, next_state\n",
        "    \n",
        "    elif mode == 'STDIM': # Encoding is the state\n",
        "        assert encoder is not None\n",
        "        with torch.no_grad():\n",
        "            encoder.eval()\n",
        "            states = encoder(obs.float().to(device) / 255)\n",
        "            next_states = encoder(next_obs.float().to(device) / 255)\n",
        "            return states, next_states\n",
        "\n",
        "    else: # Image observations are the state or the RAM labels\n",
        "        return obs, next_obs\n",
        "\n",
        "'''\n",
        "env = AtariARIWrapper(gym.make('MsPacmanNoFrameskip-v4')) \n",
        "info_labels = env.labels()\n",
        "#print(info_labels)\n",
        "state = []\n",
        "print(info_labels)\n",
        "for key in info_labels:\n",
        "    state.append(info_labels[key])\n",
        "\n",
        "state = np.array(state)\n",
        "compact_state = state[0:14]\n",
        "print(compact_state)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGP9vBlbnMQ9",
        "colab_type": "code",
        "outputId": "78040184-3d95-4815-c719-e05061a4bc2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Instantiate reward model\n",
        "env = gym.make('MsPacmanNoFrameskip-v4')\n",
        "encoder = ImpalaCNN(observation_channels,all_defaults)\n",
        "encoder.load_state_dict(torch.load('/content/MsPacmanNoFrameskip-v4_95.pt'))\n",
        "encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "rew_model = NNRewardModel(n_actions + feature_size,reward_dim).to(device)\n",
        "rew_optimizer = torch.optim.Adam(rew_model.parameters(), REW_LEARNING_RATE)\n",
        "wandb.watch(rew_model, log=\"all\")\n",
        "desired_model = 'Reward_model'\n",
        "model_name = desired_model +'_'+ date_time\n",
        "early_stopping_rew = EarlyStopping(patience=7, verbose=True, wandb=wandb, name=model_name)\n",
        "\n",
        "game_reward = 0\n",
        "rand_dataset = gather_random_trajectories(10000,'MsPacmanNoFrameskip-v4',encoder)\n",
        "print('Does it work past here')\n",
        "rl_dataset = []\n",
        "num_examples_added = len(rand_dataset)\n",
        "\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(rand_dataset, rl_dataset, num_examples_added)\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "print('Is data normalised?')\n",
        "#env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(rand_dataset,rl_dataset,num_examples_added)\n",
        "\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_reward.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    #norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "    print('Does the model train')\n",
        "    train_model(norm_rew_train_data, norm_rew_val_data, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL,rew_optimizer,device,early_stopping_rew)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rewards list (10000, 1)\n",
            "dones list (10000, 1)\n",
            "Does it work past here\n",
            "Is data normalised?\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 3/55 [00:00<00:29,  1.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy increased for Reward_model_2_9.26.46  (0.000000 --> 0.230905).  Saving model ...\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 1 out of 7\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 2 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 5/55 [00:01<00:21,  2.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 3 out of 7\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 4 out of 7\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 5 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 7/55 [00:01<00:09,  4.91it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 6 out of 7\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 7 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 8 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 9 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 10 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 11 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 12 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 13 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 14 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Reward_model_2_9.26.46 counter: 15 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n",
            "Does the model train\n",
            "EarlyStopping for Reward_model_2_9.26.46 counter: 16 out of 7\n",
            "Reward_model_2_9.26.46 has stopped\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yadzOFSrhEWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "\n",
        "encoder = NatureCNN(observation_channels,all_defaults)\n",
        "encoder.load_state_dict(torch.load('/content/MsPacmanNoFrameskip-v4_55.pt'))\n",
        "encoder.to(device)\n",
        "encoder.eval()\n",
        "rand_dataset = gather_random_trajectories(10000,'MsPacmanNoFrameskip-v4',encoder)\n",
        "rl_dataset = []\n",
        "num_examples_added = len(rand_dataset)\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = collate_data(rand_dataset,rl_dataset,num_examples_added)\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "norm = (X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "\n",
        "env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "wandb.watch(env_model, log=\"all\")\n",
        "env_model_name = 'Env_model'+ '_' + date_time\n",
        "early_stopping_env = EarlyStopping(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_env.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9ZlTpnR7HaS",
        "colab_type": "code",
        "outputId": "a4e1dfc1-8205-4592-f80c-b69860d69bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "rew_model = NNRewardModel(n_actions + feature_size, reward_dim).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "rew_optimizer = torch.optim.Adam(rew_model.parameters(),REW_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "wandb.watch(rew_model, log=\"all\")\n",
        "Rew_model_name = 'Rew_model'+ '_' + date_time\n",
        "early_stopping_rew = EarlyStopping_loss(patience=7, verbose=True, wandb=wandb, name=Rew_model_name)\n",
        "\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_rew.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    train_model(norm_rew_train_data, norm_rew_val_data, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL,rew_optimizer,device,early_stopping_rew,desired_model='Rew_model')\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/55 [00:16<14:33, 16.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (100000000000.000000 --> 0.020023).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 2/55 [00:31<14:02, 15.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.020023 --> 0.011873).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 3/55 [00:46<13:32, 15.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011873 --> 0.011708).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 4/55 [01:01<13:05, 15.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011708 --> 0.011604).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 5/55 [01:17<13:04, 15.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011604 --> 0.011513).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 6/55 [01:32<12:38, 15.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011513 --> 0.011444).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 7/55 [01:47<12:17, 15.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011444 --> 0.011378).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 15%|█▍        | 8/55 [02:02<11:55, 15.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011378 --> 0.011330).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▋        | 9/55 [02:17<11:35, 15.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011330 --> 0.011288).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 10/55 [02:32<11:17, 15.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011288 --> 0.011258).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 11/55 [02:47<10:57, 14.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011258 --> 0.011223).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 12/55 [03:01<10:40, 14.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011223 --> 0.011186).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 24%|██▎       | 13/55 [03:16<10:26, 14.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011186 --> 0.011173).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 14/55 [03:32<10:17, 15.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011173 --> 0.011156).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 15/55 [03:47<09:58, 14.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011156 --> 0.011146).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 29%|██▉       | 16/55 [04:01<09:38, 14.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011146 --> 0.011140).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 17/55 [04:16<09:21, 14.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Rew_model_4_19.8.50  (0.011140 --> 0.011134).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 18/55 [04:31<09:09, 14.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_4_19.8.50 counter: 1 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 35%|███▍      | 19/55 [04:46<08:55, 14.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_4_19.8.50 counter: 2 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▋      | 20/55 [05:00<08:39, 14.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_4_19.8.50 counter: 3 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 21/55 [05:15<08:23, 14.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_4_19.8.50 counter: 4 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 22/55 [05:30<08:07, 14.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_4_19.8.50 counter: 5 out of 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 23/55 [05:44<07:52, 14.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EarlyStopping for Rew_model_4_19.8.50 counter: 6 out of 7\n",
            "EarlyStopping for Rew_model_4_19.8.50 counter: 7 out of 7\n",
            "Rew_model_4_19.8.50 has stopped\n",
            "Early stopping\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YKalYAl2PyE",
        "colab_type": "code",
        "outputId": "7866ec8e-930e-4157-f378-6e801ddef191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "practice_dones = torch.zeros((64,1))\n",
        "practice_dones[16:40,:] = 1\n",
        "#print(practice_dones)\n",
        "n,c = practice_dones.size()\n",
        "\n",
        "weights = np.zeros((n,c))\n",
        "\n",
        "pos = practice_dones[practice_dones==1].sum()\n",
        "neg = n - pos\n",
        "#print(practice_dones[practice_dones==1])\n",
        "\n",
        "# need to get indices,for when the dones is positive\n",
        "# weights = 1/neg\n",
        "# weights[p] = 1/pos # change the other weights to positive value\n",
        "'''\n",
        "weights[]\n",
        "p = np.random.permutation(len(args[0]))\n",
        "        shuffled = [i[p] for i in args]\n",
        "        return shuffled\n",
        "'''\n",
        "\n",
        "nx = practice_dones.numpy()\n",
        "index = np.where(nx == 1)[0]\n",
        "weights[:] = 1#1/neg\n",
        "weights[index] = neg* 1/pos\n",
        "print(weights)\n",
        "\n",
        "'''\n",
        "for i in range(n):\n",
        "    t = practice_dones[i,:].cpu().data.numpy()\n",
        "    pos = (t==1).sum()\n",
        "    neg = (t==0).sum()\n",
        "        \n",
        "    weights[i,t==1] = 1/pos\n",
        "    weights[i,t==0] = 1/neg\n",
        "\n",
        "weights = torch.Tensor(weights)\n",
        "print(weights)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.66666663]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i in range(n):\\n    t = practice_dones[i,:].cpu().data.numpy()\\n    pos = (t==1).sum()\\n    neg = (t==0).sum()\\n        \\n    weights[i,t==1] = 1/pos\\n    weights[i,t==0] = 1/neg\\n\\nweights = torch.Tensor(weights)\\nprint(weights)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}