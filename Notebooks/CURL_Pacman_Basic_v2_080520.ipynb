{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CURL_Pacman_All_Agents_080520.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_eIBf1pknbf_"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1cwYR8TdUZjtwvyV_-TbwA5_Y9mp_RC3O",
      "authorship_tag": "ABX9TyPdsS/Pjb7oerWC21VfDUXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/Model-based-RL/blob/master/CURL_Pacman_Basic_v2_080520.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hdnkq3rcJNF",
        "colab_type": "text"
      },
      "source": [
        "# Used to save Pacman video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp0jzlXIcFfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fss3NXoMcIjw",
        "colab_type": "code",
        "outputId": "39dd0c40-dac8-464b-d2fa-8f78a5bc876b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!apt-get install ffmpeg"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (46.1.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4jLAGgyjflE",
        "colab_type": "text"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "b3a5e071-d5a6-4623-bba3-2bdc82a9f74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-qmtgbgdy\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-qmtgbgdy\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=40298f3a99d68e14933f1c8f9ee373703b132619e9665912c4100deceb5850f1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h5czhi2t/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-s8n5ib4o\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-s8n5ib4o\n",
            "Collecting gym<0.16.0,>=0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/01/8771e8f914a627022296dab694092a11a7d417b6c8364f0a44a8debca734/gym-0.15.7.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines, gym\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=5bc331e6204114b94588c300cf8f91de7aa61ac10ca646162511e212dda2590f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hfx6a4qj/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-cp36-none-any.whl size=1648840 sha256=389c268e473594917f526cbe24dd1d70b5c23ce2ffc244e657a4f8a421d48f48\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/60/6a/f9c27ae133abaf5a5687ed2fa8ed19627d7fac5d843a27572b\n",
            "Successfully built baselines gym\n",
            "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gym, baselines\n",
            "  Found existing installation: gym 0.17.1\n",
            "    Uninstalling gym-0.17.1:\n",
            "      Successfully uninstalled gym-0.17.1\n",
            "Successfully installed baselines-0.1.6 gym-0.15.7\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.6MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.0MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/33/917e6fde1cad13daa7053f39b7c8af3be287314f75f1b1ea8d3fe37a8571/GitPython-3.1.2-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 10.2MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: gql, watchdog, subprocess32, graphql-core, pathtools\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=6fb6fdda7910e295a08810aa3e4ff3c4393b2d84840cea65c5bf5a4fbd9fcb04\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=1ff7c7695e1615618c9af1931031d1811b5a4a639830fa800abd5a38a5c2d221\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=a541595bb81aa0ecb6a557efebf83d79d2882e74750f66cf88c3644be5012236\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=369f5c5cde6de2aeff74385905c9e70e2cd958c8e165bd98106f7997ca26d2dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=c03a997d9c94662b0891eca88f0798ff1697715e5002b752d54e1be8e0b94d3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built gql watchdog subprocess32 graphql-core pathtools\n",
            "Installing collected packages: graphql-core, gql, shortuuid, pathtools, watchdog, docker-pycreds, sentry-sdk, smmap, gitdb, GitPython, configparser, subprocess32, wandb\n",
            "Successfully installed GitPython-3.1.2 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.8.35 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "41552604-1448-4eca-e16d-7f4b636f8c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!wandb login ################"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: wandb: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI287_Yjxn3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from benchmark.envs import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from benchmark.wrapper import AtariARIWrapper\n",
        "\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from custom.EarlyStopping import EarlyStopping_loss\n",
        "from custom.model import *\n",
        "from custom.LSTM_model import *\n",
        "from custom.custom_wrappers import DeterminsticNoopResetEnv \n",
        "from baselines.common.atari_wrappers import EpisodicLifeEnv#, WarpFrame\n",
        "\n",
        "# Required for CURL \n",
        "from collections import deque\n",
        "from skimage.util.shape import view_as_windows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YeWKrKFXuuY",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMRcLI41UQwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ReplayBuffer(Dataset):\n",
        "    \"\"\"Buffer to store environment transitions.\"\"\"\n",
        "    def __init__(self, obs_shape,action_shape, state_shape,capacity, batch_size, device,image_size=84,transform=None):\n",
        "        self.capacity = capacity\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.image_size = image_size\n",
        "        self.transform = transform\n",
        "\n",
        "        obs_dtype = np.uint8 # Need to consider the sign of this\n",
        "        self.obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n",
        "        self.actions = np.empty((capacity,*action_shape),dtype = np.float32)\n",
        "        self.states = np.empty((capacity, *state_shape), dtype = np.float32) # need to have at least int16 or float32 in order to obtain signed values\n",
        "        self.next_states = np.empty((capacity, *state_shape),dtype = np.float32) \n",
        "\n",
        "        self.idx = 0\n",
        "        self.last_save = 0\n",
        "        self.full = False\n",
        "\n",
        "    def add(self, obs,action,state, next_state): # Nawid- Add information to replay buffer\n",
        "        \n",
        "        np.copyto(self.obses[self.idx], obs)\n",
        "        np.copyto(self.actions[self.idx], action)\n",
        "        np.copyto(self.states[self.idx], state)\n",
        "        np.copyto(self.next_states[self.idx],next_state)\n",
        "\n",
        "        self.idx = (self.idx + 1) % self.capacity # This makes the data gets replaced in a recursive manner when the capacity is full\n",
        "        self.full = self.full or self.idx == 0\n",
        "    \n",
        "    def sample_cpc(self): # Nawid - samples images I believe\n",
        "\n",
        "        start = time.time()\n",
        "        idxs = np.random.randint(\n",
        "            0, self.capacity if self.full else self.idx, size=self.batch_size\n",
        "        ) # Used to randomly sample indices\n",
        "\n",
        "        obses = self.obses[idxs] # Nawid - Samples observation\n",
        "        pos = obses.copy() # Nawid -\n",
        "\n",
        "        # Nawid - Crop images randomly\n",
        "        obses = random_crop(obses, self.image_size)\n",
        "        pos = random_crop(pos, self.image_size)\n",
        "\n",
        "        obses = torch.as_tensor(obses, device=self.device).float()\n",
        "        \n",
        "\n",
        "        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n",
        "\n",
        "        states = torch.as_tensor(self.states[idxs], device=self.device)\n",
        "        next_states = torch.as_tensor(self.next_states[idxs],device=self.device)\n",
        "        y_target_position = self.next_states[idxs] - self.states[idxs]\n",
        "        y_target_position = torch.as_tensor(y_target_position, device=self.device) \n",
        "        \n",
        "        pos = torch.as_tensor(pos, device=self.device).float()\n",
        "        cpc_kwargs = dict(obs_anchor=obses, obs_pos=pos,\n",
        "                          time_anchor=None, time_pos=None) # Nawid  Postitive example is pos whilst anchor is obses\n",
        "\n",
        "        return obses, actions, y_target_position, cpc_kwargs\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, idx): # Nawid - Obtains item from replay buffer\n",
        "        idx = np.random.randint(\n",
        "            0, self.capacity if self.full else self.idx, size=1\n",
        "        )\n",
        "        idx = idx[0]\n",
        "        obs = self.obses[idx]\n",
        "        action = self.actions[idx]\n",
        "        state = self.states[idx]\n",
        "        next_state = self.next_states[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            obs = self.transform(obs)\n",
        "\n",
        "        return obs, action\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.capacity\n",
        "    \n",
        "\n",
        "class FrameStack(gym.Wrapper): # Nawid - Stacks the frames\n",
        "    def __init__(self, env, k):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self._k = k\n",
        "        self._frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape = (shp[:-1]+(shp[-1]*k,)),\n",
        "            #shape=((shp[0] * k,) + shp[1:]),\n",
        "            dtype=env.observation_space.dtype\n",
        "        )\n",
        "        self._max_episode_steps = 100000 #env._max_episode_steps - could not access max episode steps if i use the warp wrapper so I found the max episode steps manually and placed it in the code\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        for _ in range(self._k):\n",
        "            self._frames.append(obs)\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self._frames.append(obs)\n",
        "        return self._get_obs(), reward, done, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        assert len(self._frames) == self._k\n",
        "        return np.concatenate(list(self._frames), axis=2)\n",
        "    \n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env, width=84, height=84, grayscale=False, dict_space_key=None):\n",
        "        \"\"\"\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
        "        observation should be warped.\n",
        "        \"\"\"\n",
        "        super().__init__(env)\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grayscale = grayscale\n",
        "        self._key = dict_space_key\n",
        "        if self._grayscale:\n",
        "            num_colors = 1\n",
        "        else:\n",
        "            num_colors = 3\n",
        "\n",
        "        new_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(self._height, self._width, num_colors),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "        if self._key is None:\n",
        "            original_space = self.observation_space\n",
        "            self.observation_space = new_space\n",
        "        else:\n",
        "            original_space = self.observation_space.spaces[self._key]\n",
        "            self.observation_space.spaces[self._key] = new_space\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
        "\n",
        "    def observation(self, obs):\n",
        "        if self._key is None:\n",
        "            frame = obs\n",
        "        else:\n",
        "            frame = obs[self._key]\n",
        "\n",
        "        if self._grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        if self._grayscale:\n",
        "            frame = np.expand_dims(frame, -1)\n",
        "\n",
        "        if self._key is None:\n",
        "            obs = frame\n",
        "        else:\n",
        "            obs = obs.copy()\n",
        "            obs[self._key] = frame\n",
        "        return obs\n",
        "\n",
        "\n",
        "def random_crop(imgs, output_size):\n",
        "    \"\"\"\n",
        "    Vectorized way to do random crop using sliding windows\n",
        "    and picking out random ones\n",
        "\n",
        "    args:\n",
        "        imgs, batch images with shape (B,C,H,W)  -  Changed this to batch images with shape (B,H,W,C)\n",
        "    \"\"\"\n",
        "    # batch size\n",
        "    n = imgs.shape[0]\n",
        "    img_size = imgs.shape[1] # Nawid - Changed this to get image size\n",
        "    crop_max = img_size - output_size\n",
        "    #imgs = np.transpose(imgs, (0, 2, 3, 1)) #  Nawid - No longer required as I am using a different input size\n",
        "    w1 = np.random.randint(0, crop_max, n)\n",
        "    h1 = np.random.randint(0, crop_max, n)\n",
        "    # creates all sliding windows combinations of size (output_size)\n",
        "    windows = view_as_windows(\n",
        "        imgs, (1, output_size, output_size, 1))[..., 0,:,:, 0]\n",
        "    # selects a random window for each batch element\n",
        "    cropped_imgs = windows[np.arange(n), w1, h1]\n",
        "    return cropped_imgs # Nawid -Output shape is the required pytorch shape (channels in dim 1) (B,C,H,W)\n",
        "\n",
        "\n",
        "def soft_update_params(net, target_net, tau): # Nawid-update for the momentum encoder\n",
        "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
        "        target_param.data.copy_(\n",
        "            tau * param.data + (1 - tau) * target_param.data\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuPWLEVzXzPS",
        "colab_type": "text"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99AdtIC5Xs2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for 84 x 84 inputs\n",
        "OUT_DIM = {2: 39, 4: 35, 6: 31}\n",
        "# for 64 x 64 inputs\n",
        "OUT_DIM_64 = {2: 29, 4: 25, 6: 21}\n",
        "\n",
        "def tie_weights(src, trg): # Nawid - Used to copy weights which is used for conv layes\n",
        "    assert type(src) == type(trg)\n",
        "    trg.weight = src.weight\n",
        "    trg.bias = src.bias\n",
        "\n",
        "class PixelEncoder(nn.Module):\n",
        "    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n",
        "    def __init__(self, obs_shape, feature_dim, num_layers=2, num_filters=32,output_logits=False):\n",
        "        super().__init__()\n",
        "        assert len(obs_shape) == 3\n",
        "        self.obs_shape = obs_shape\n",
        "        self.feature_dim = feature_dim\n",
        "        self.num_layers = num_layers\n",
        "    \n",
        "        self.convs = nn.ModuleList( # Nawid - 3 conv layers in total with output dim 32\n",
        "            [nn.Conv2d(obs_shape[2], num_filters, 3, stride=2)] # Nawid - Changed this to take into account original numpy ordering\n",
        "        )\n",
        "        for i in range(num_layers - 1):\n",
        "            self.convs.append(nn.Conv2d(num_filters, num_filters, 3, stride=1))\n",
        "\n",
        "        out_dim = OUT_DIM_64[num_layers] if obs_shape[-2] == 64 else OUT_DIM[num_layers] # changed obs_shape[1] to -2 to take into account numpy ordering\n",
        "        self.fc = nn.Linear(num_filters * out_dim * out_dim, self.feature_dim)\n",
        "        self.ln = nn.LayerNorm(self.feature_dim) # Nawid -  Layer norm used instead of batch norm due to statistic sharing\n",
        "\n",
        "        self.output_logits = output_logits\n",
        "\n",
        "    def forward_conv(self, obs):\n",
        "        obs = obs / 255.\n",
        "        conv = torch.relu(self.convs[0](obs))\n",
        "\n",
        "        for i in range(1, self.num_layers):\n",
        "            conv = torch.relu(self.convs[i](conv))\n",
        "\n",
        "        #h = conv.view(conv.size(0), -1)\n",
        "        h = conv.reshape([conv.size(0),-1])\n",
        "        return h\n",
        "    \n",
        "    def forward(self, obs, detach=False):\n",
        "        h = self.forward_conv(obs) # Nawid - Performs conv layers\n",
        "        if detach:\n",
        "            h = h.detach()\n",
        "\n",
        "        h_fc = self.fc(h)\n",
        "        h_norm = self.ln(h_fc) # Nawid - Passes through layer normalisation \n",
        "\n",
        "        if self.output_logits:\n",
        "            out = h_norm\n",
        "        else:\n",
        "            out = torch.tanh(h_norm)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def copy_conv_weights_from(self, source):\n",
        "        \"\"\"Tie convolutional layers\"\"\"\n",
        "        # only tie conv layers\n",
        "        for i in range(self.num_layers):\n",
        "            tie_weights(src=source.convs[i], trg=self.convs[i])\n",
        "\n",
        "\n",
        "def make_encoder(obs_shape, feature_dim, num_layers, num_filters, output_logits=False):\n",
        "    return PixelEncoder(obs_shape, feature_dim, num_layers, num_filters, output_logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDaFHhxLacNp",
        "colab_type": "text"
      },
      "source": [
        "# CURL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc15PakPabSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_init(m):\n",
        "    \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n",
        "    if isinstance(m, nn.Linear): # Nawid-  Weight init for linear layers\n",
        "        nn.init.orthogonal_(m.weight.data)\n",
        "        m.bias.data.fill_(0.0)\n",
        "    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d): # Nawid- weight inti for conv\n",
        "        # delta-orthogonal init from https://arxiv.org/pdf/1806.05393.pdf\n",
        "        assert m.weight.size(2) == m.weight.size(3)\n",
        "        m.weight.data.fill_(0.0)\n",
        "        m.bias.data.fill_(0.0)\n",
        "        mid = m.weight.size(2) // 2\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        nn.init.orthogonal_(m.weight.data[:, :, mid, mid], gain)\n",
        "\n",
        "\n",
        "class CURL(nn.Module): # Nawid - Module for the contrastive loss\n",
        "    \"\"\"\n",
        "    CURL\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_shape, z_dim, encoder_feature_dim, num_layers, num_filters):\n",
        "        super(CURL, self).__init__()\n",
        "\n",
        "        # Need to fix the encoders since I do not plan to use the critics\n",
        "        self.encoder = make_encoder( # Nawid - Encoder of critic which is also used for the contrastive loss\n",
        "            obs_shape, encoder_feature_dim, num_layers,\n",
        "            num_filters, output_logits=True)\n",
        "\n",
        "        self.encoder_target = make_encoder(obs_shape, encoder_feature_dim, num_layers, \n",
        "                                           num_filters, output_logits=True)\n",
        "        \n",
        "        self.encoder_target.load_state_dict(self.encoder.state_dict()) # copies the parameters of the encoder into the target encoder which is changing slowly\n",
        "        self.W = nn.Parameter(torch.rand(z_dim, z_dim)) # Nawid - weight vector for the bilinear product\n",
        "\n",
        "    def encode(self, x, detach=False, ema=False):\n",
        "        \"\"\"\n",
        "        Encoder: z_t = e(x_t)\n",
        "        :param x: x_t, x y coordinates\n",
        "        :return: z_t, value in r2\n",
        "        \"\"\"\n",
        "        if ema:\n",
        "            with torch.no_grad():\n",
        "                z_out = self.encoder_target(x)\n",
        "        else:\n",
        "            z_out = self.encoder(x)\n",
        "\n",
        "        if detach:\n",
        "            z_out = z_out.detach()\n",
        "        return z_out\n",
        "    \n",
        "    def compute_logits(self, z_a, z_pos): # Nawid -  computes logits for contrastive loss\n",
        "        \"\"\"\n",
        "        Uses logits trick for CURL:\n",
        "        - compute (B,B) matrix z_a (W z_pos.T)\n",
        "        - positives are all diagonal elements\n",
        "        - negatives are all other elements\n",
        "        - to compute loss use multiclass cross entropy with identity matrix for labels\n",
        "        \"\"\"\n",
        "        Wz = torch.matmul(self.W, z_pos.T)  # (z_dim,B)\n",
        "        logits = torch.matmul(z_a, Wz)  # (B,B)\n",
        "        logits = logits - torch.max(logits, 1)[0][:, None]\n",
        "        return logits\n",
        "\n",
        "\n",
        "class Dynamics_model(nn.Module):\n",
        "    ''' MLP network'''\n",
        "    def __init__(self, obs_shape,hidden_dim,\n",
        "                 output_dim , encoder_feature_dim, num_layers, num_filters):\n",
        "        super(Dynamics_model,self).__init__()\n",
        "\n",
        "        self.encoder = make_encoder(obs_shape, encoder_feature_dim,num_layers,num_filters,output_logits=True)\n",
        "\n",
        "        self.trunk = nn.Sequential(\n",
        "            nn.Linear(self.encoder.feature_dim + 4 , hidden_dim), nn.ReLU(), # Size of the input is related to the encoder output as well as the concatenated one hot vector for the actions\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "        self.apply(weight_init)\n",
        "    \n",
        "    def forward(self, obs,aux, detach_encoder = False):\n",
        "        obs = self.encoder(obs, detach = detach_encoder) \n",
        "        obs = torch.cat((obs, aux), 1) # Join vectors along this dimension\n",
        "        prediction = self.trunk(obs)        \n",
        "        return prediction\n",
        "\n",
        "\n",
        "class CurlAgent(object):\n",
        "    ''' CURL representation learning'''\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_shape,\n",
        "        device,\n",
        "        hidden_dim = 256,\n",
        "        output_dim = 2,\n",
        "        encoder_feature_dim = 50,\n",
        "        encoder_lr = 1e-3,\n",
        "        encoder_tau = 0.005,\n",
        "        num_layers=4,\n",
        "        num_filters = 32,\n",
        "        cpc_update_freq=1,\n",
        "        encoder_update_freq = 2,\n",
        "        detach_encoder=False,\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.cpc_update_freq = cpc_update_freq\n",
        "        self.image_size = obs_shape[-2] # Changed this to the numpy dimension\n",
        "        self.detach_encoder = detach_encoder\n",
        "        self.encoder_update_freq = encoder_update_freq\n",
        "        self.encoder_tau = encoder_tau\n",
        "        \n",
        "        self.CURL = CURL(obs_shape, encoder_feature_dim,\n",
        "                         encoder_feature_dim, num_layers, num_filters).to(self.device)\n",
        "        \n",
        "        self.Model = Dynamics_model(obs_shape,hidden_dim,output_dim,encoder_feature_dim,num_layers,num_filters).to(self.device)\n",
        "\n",
        "        self.cpc_optimizer = torch.optim.Adam(\n",
        "                self.CURL.parameters(), lr=encoder_lr\n",
        "            )\n",
        "        \n",
        "        self.dynamics_optimizer = torch.optim.Adam(self.Model.parameters(), lr= encoder_lr)\n",
        "\n",
        "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "        self.MSE_loss = nn.MSELoss() # Nawid - Added this loss for the prediction\n",
        "        self.train()\n",
        "\n",
        "        #wandb.watch(self.CURL, log='all')\n",
        "        #wandb.watch(self.Model, log='all')\n",
        "    \n",
        "    def train(self, training = True):\n",
        "        self.training = training\n",
        "        self.CURL.train(training)\n",
        "        self.Model.train(training)\n",
        "\n",
        "    def update_cpc(self, obs_anchor, obs_pos, cpc_kwargs):\n",
        "        z_a = self.CURL.encode(obs_anchor) # Nawid -  Encode the anchor\n",
        "        z_pos = self.CURL.encode(obs_pos, ema=True) # Nawid- Encode the positive with the momentum encoder\n",
        "\n",
        "        logits = self.CURL.compute_logits(z_a, z_pos) #  Nawid- Compute the logits between them\n",
        "        labels = torch.arange(logits.shape[0]).long().to(self.device)\n",
        "        loss = self.cross_entropy_loss(logits, labels)\n",
        "        #wandb.log({'Contrastive loss':loss.cpu().detach().numpy()})\n",
        "\n",
        "        self.cpc_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        self.cpc_optimizer.step()  # Nawid - Used to update the cpc\n",
        "    \n",
        "    def update_dynamics(self, obs,actions, labels):\n",
        "        prediction = self.Model(obs,actions,detach_encoder=False) # gradient not backpropagated to the encoder\n",
        "        prediction_loss = self.MSE_loss(prediction,labels)\n",
        "        \n",
        "        print(prediction_loss.item()) #  Need to use .item otherwise the loss will still be kept which will reduce the memory on the GPU\n",
        "        \n",
        "        self.dynamics_optimizer.zero_grad()\n",
        "        prediction_loss.backward()\n",
        "        self.dynamics_optimizer.step()\n",
        "\n",
        "\n",
        "    def update(self, training_replay_buffer,step):\n",
        "        #torch.cuda.empty_cache() # Releases cache so the GPU has more memory\n",
        "        obs, actions_one_hot,predicted_change, cpc_kwargs = training_replay_buffer.sample_cpc()\n",
        "        if step % self.encoder_update_freq == 0:\n",
        "            soft_update_params(\n",
        "                self.CURL.encoder, self.CURL.encoder_target,\n",
        "                self.encoder_tau\n",
        "            )\n",
        "\n",
        "        if step % self.cpc_update_freq == 0:\n",
        "            obs_anchor, obs_pos = cpc_kwargs[\"obs_anchor\"], cpc_kwargs[\"obs_pos\"]\n",
        "            self.update_cpc(obs_anchor, obs_pos,cpc_kwargs) # Nawid -  Performs the contrastive loss I believe\n",
        "            self.update_dynamics(obs, actions_one_hot,predicted_change)\n",
        "        \n",
        "def make_agent(obs_shape, device, dict_info):\n",
        "    return CurlAgent(\n",
        "        obs_shape = obs_shape,\n",
        "        device = device,\n",
        "        encoder_update_freq =dict_info['encoder_update_freq'],\n",
        "        encoder_feature_dim = dict_info['encoder_feature_dim'],\n",
        "        encoder_lr = dict_info['encoder_lr'],\n",
        "        encoder_tau = dict_info['encoder_tau'],\n",
        "        num_layers = dict_info['num_layers'],\n",
        "        num_filters = dict_info['num_filters']\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRn5uXDhgmQs",
        "colab_type": "text"
      },
      "source": [
        "# General Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vh9bRroPrRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_wrapper(env,deterministic_reset=True,episode_life=True, warp=True, grayscale = False, frame_stack=True,ATARI_labels=True):\n",
        "    if deterministic_reset:\n",
        "        env = DeterminsticNoopResetEnv(env,66)\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)    \n",
        "    if warp:\n",
        "        env = WarpFrame(env,height = 100, width = 100, grayscale=grayscale)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env,4)\n",
        "    if ATARI_labels:\n",
        "        env = AtariARIWrapper(env) \n",
        "    return env\n",
        "\n",
        "class General_functions():\n",
        "    def __init__(self, ENV_NAME, n_actions,possible_positions):\n",
        "        self.ENV_NAME = ENV_NAME\n",
        "        self.env = gym.make(self.ENV_NAME)\n",
        "        self.env = custom_wrapper(self.env)\n",
        "        self.initial_info_labels = self.env.labels()\n",
        "        self.key_list = ['player_x', 'player_y']\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(self.device)\n",
        "\n",
        "        self.prev_action_counter = False\n",
        "        self.repeated_end = False\n",
        "        self.n_actions = n_actions\n",
        "        # possible positions is numpy array with most of the possible positions that the agent can go to\n",
        "        self.possible_positions = possible_positions\n",
        "        self.possible_positions_list = self.possible_positions.tolist()\n",
        "        \n",
        "    def one_hot(self,i):\n",
        "        a = np.zeros(self.n_actions, 'uint8')\n",
        "        a[i-1] = 1\n",
        "        return a\n",
        "    \n",
        "    def state_conversion(self,info_labels):\n",
        "        state = [info_labels[word] for word in self.key_list if word in info_labels]                \n",
        "        state = np.array(state)\n",
        "        return state\n",
        "\n",
        "    def next_position(self,state, action):\n",
        "        next_position = state[-2:].copy()\n",
        "        #print(next_position)\n",
        "        if action == 1:\n",
        "            next_position[1] = next_position[1] - 2 \n",
        "        elif action == 2:\n",
        "            next_position[0] = next_position[0] + 2\n",
        "        elif action == 3:\n",
        "            next_position[0] = next_position[0] - 2\n",
        "        elif action == 4:\n",
        "            next_position[1] = next_position[1] + 2        \n",
        "        if next_position.tolist() in self.possible_positions_list: # possible positions will be a list which is fed into the network \n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def check_all_agents(self,info_label,next_info_label):\n",
        "        repeated = np.equal(info_label,next_info_label).all()\n",
        "        if repeated:\n",
        "            self.repeated_end= True\n",
        "\n",
        "    def check_state(self,state, next_state):\n",
        "        repeated_state = np.equal(state[-2:], next_state[-2:]).all()\n",
        "        if repeated_state:\n",
        "            self.prev_action_counter = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJgI07xT7eo",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL0DwKk3XLAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data_collection(General_functions):\n",
        "    def __init__(self, ENV_NAME, n_actions, possible_positions,info_dict):\n",
        "        super(Data_collection, self).__init__(ENV_NAME, n_actions, possible_positions)\n",
        "        self.grayscale = info_dict['grayscale']\n",
        "        self.channels = 1 if self.grayscale else 3 \n",
        "        self.obs_shape = (info_dict['image_size'], info_dict['image_size'],self.channels*info_dict['frame_stack']) # Nawid - Stack together images (multiply of 3 present due to 3 channels (RGB))\n",
        "        self.pre_aug_obs_shape = (info_dict['pre_transform_image_size'],info_dict['pre_transform_image_size'],self.channels*info_dict['frame_stack'])\n",
        "        self.action_shape = (n_actions,)   \n",
        "        self.state_shape = (2,)     \n",
        "        self.replay_buffer  = ReplayBuffer(self.pre_aug_obs_shape,self.action_shape,self.state_shape,info_dict['replay_buffer_capacity'], info_dict['batch_size'],self.device, info_dict['image_size'])     #(self, obs_shape, capacity, batch_size, device,image_size=84,transform=None):\n",
        "\n",
        "\n",
        "    def random_action_selection(self, state,prev_action = None):\n",
        "        while True:\n",
        "            action = np.random.randint(1,5) \n",
        "            feasible_action = self.next_position(state,action)\n",
        "            if feasible_action:\n",
        "                self.prev_action_counter = True\n",
        "                return action, None\n",
        "            else:\n",
        "                infeasible_action_one_hot = self.one_hot(action)\n",
        "                if self.prev_action_counter and prev_action !=None:\n",
        "                    return prev_action, infeasible_action_one_hot \n",
        "    \n",
        "    def gather_random_trajectories(self,num_traj):        \n",
        "        ts = time.gmtime()\n",
        "        ts = time.strftime('%d-%m_%H:%M')\n",
        "        work_dir = '/content/drive/My Drive/MsPacman-data' + '/' + ts\n",
        "        work_dir = make_dir(work_dir)\n",
        "        buffer_dir = make_dir(os.path.join(work_dir, 'buffer'))\n",
        "        for n in range(num_traj):\n",
        "            \n",
        "            if n % 10 ==0:\n",
        "                print('trajectory number:',n)\n",
        "                # Initial set up\n",
        "            #self.env.seed(0)\n",
        "\n",
        "\n",
        "            self.env = gym.make(self.ENV_NAME) # Due to error in code, I reinstantiate the env each time\n",
        "            self.env = custom_wrapper(self.env,grayscale = self.grayscale)\n",
        "            obs = self.env.reset()\n",
        "            \n",
        "\n",
        "            self.repeated_end = False\n",
        "            info_labels = self.env.labels() # Nawid - Used to get the current state\n",
        "            state = self.state_conversion(info_labels) # Used to get the initial state\n",
        "            prev_action = None # Initialise prev action has having no action\n",
        "            \n",
        "\n",
        "            while True:\n",
        "                sampled_action, infeasible_action_one_hot = self.random_action_selection(state,prev_action)\n",
        "                sampled_action_one_hot = self.one_hot(sampled_action)\n",
        "                \n",
        "                next_obs, reward, done, next_info = self.env.step(sampled_action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                \n",
        "                next_state = self.state_conversion(next_info_labels)\n",
        "                \n",
        "                self.check_state(state,next_state)\n",
        "                self.check_all_agents(info_labels, next_info_labels) # need to use the info labels to predict the state as the info labels have all the informaiton\n",
        "                    \n",
        "                if not self.repeated_end:\n",
        "                    \n",
        "                    if infeasible_action_one_hot is not None:\n",
        "                        fake_next_state = np.zeros_like(state) #  Need to instantiate a new version each time to prevent updating a single variable which will affect all places(eg lists) where the variable is added\n",
        "                        fake_next_state[0:-2] = next_state[0:-2].copy() # the enemy position of the fake next state is the current enemy position\n",
        "                        fake_next_state[-2:] = state[-2:].copy() # The agent position for the fake next state is the state before any action was taken\n",
        "\n",
        "                        self.replay_buffer.add(obs, next_obs, infeasible_action_one_hot, state, fake_next_state)\n",
        "                        \n",
        "                    \n",
        "                    self.replay_buffer.add(obs,next_obs, sampled_action_one_hot, state, next_state)\n",
        "                else:\n",
        "                    done = True   \n",
        "                \n",
        "                obs = next_obs # do not need to copy as a new variable of obs is instantiated at each time step.\n",
        "                state = next_state.copy()\n",
        "                info_labels = next_info_labels.copy()\n",
        "                prev_action = sampled_action\n",
        "\n",
        "                if done:\n",
        "                    break    \n",
        "\n",
        "                if self.replay_buffer.full:\n",
        "                    self.replay_buffer.save(buffer_dir)\n",
        "                    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM4SvnJeJUgM",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNHK7nCaJT-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'MsPacmanDeterministic-v4'\n",
        "n_actions = 4 #9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "parse_dict= {'pre_transform_image_size':100,\n",
        "             'image_size':84,\n",
        "             'action_repeat':1,\n",
        "             'frame_stack':4,\n",
        "             'replay_buffer_capacity':10000,\n",
        "             'agent':'curl_agent',\n",
        "             'num_train_steps':10000,\n",
        "             'batch_size':256,\n",
        "             'encoder_update_freq':2,\n",
        "             'encoder_feature_dim':50,\n",
        "             'encoder_lr':1e-3,\n",
        "             'encoder_tau':0.05,\n",
        "             'num_layers':4,\n",
        "             'num_filters':32,\n",
        "             'grayscale': True\n",
        "            }\n",
        "\n",
        "\n",
        "possible_positions = np.load('/content/drive/My Drive/MsPacman-data/possible_pacman_positions.npy',allow_pickle=True)\n",
        "load_trajectories = True\n",
        "preloaded_train_data = '/content/drive/My Drive/MsPacman-data/08-05_16:21/buffer'\n",
        "preloaded_val_data = '/content/drive/My Drive/MsPacman-data/08-05_17:13/buffer'\n",
        "pretrain_model = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtD9K4s3HFw7",
        "colab_type": "text"
      },
      "source": [
        "# Main -  Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JeHblECHRgq",
        "colab_type": "text"
      },
      "source": [
        "Data - Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5StrDuHV6X74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e700b72e-0b8f-4f57-ab7e-16a82f2b0c1d"
      },
      "source": [
        "data_object = Data_collection(ENV_NAME,n_actions,possible_positions, parse_dict)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o_EJEBjJ_NX",
        "colab_type": "code",
        "outputId": "ed516044-1716-44a4-9dfa-944cf70fef4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "agent = make_agent(\n",
        "        obs_shape = data_object.obs_shape,\n",
        "        device =data_object.device,\n",
        "        dict_info = parse_dict\n",
        "    )\n",
        "\n",
        "replay_buffer = ReplayBuffer(data_object.pre_aug_obs_shape,data_object.action_shape,data_object.state_shape,parse_dict['replay_buffer_capacity'],parse_dict['batch_size'],data_object.device)\n",
        "env = gym.make(ENV_NAME)\n",
        "env = custom_wrapper(env, grayscale=True)\n",
        "obs = env.reset()\n",
        "info_labels = env.labels()\n",
        "state = data_object.state_conversion(info_labels)\n",
        "\n",
        "for step in range(parse_dict['num_train_steps']):\n",
        "    if step >= 500:\n",
        "        agent.update(replay_buffer,step)\n",
        "\n",
        "    action = np.random.randint(1,5)\n",
        "    next_obs, reward, done, next_info = env.step(action)\n",
        "    \n",
        "    next_state = data_object.state_conversion(next_info['labels'])\n",
        "    action_one_hot = data_object.one_hot(action)\n",
        "    replay_buffer.add(obs, action_one_hot, state, next_state)\n",
        "\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "    obs = next_obs\n",
        "    state = next_state"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.050731658935547\n",
            "6.680270195007324\n",
            "10.933725357055664\n",
            "11.416601181030273\n",
            "8.755731582641602\n",
            "13.456838607788086\n",
            "3.4953861236572266\n",
            "8.91973876953125\n",
            "6.711963176727295\n",
            "3.464049816131592\n",
            "3.285393238067627\n",
            "10.592052459716797\n",
            "9.064109802246094\n",
            "8.152193069458008\n",
            "4.213809013366699\n",
            "4.689882278442383\n",
            "9.351192474365234\n",
            "11.059609413146973\n",
            "2.935535430908203\n",
            "10.464079856872559\n",
            "3.1120285987854004\n",
            "6.630801677703857\n",
            "4.376309871673584\n",
            "3.0450279712677\n",
            "2.8795535564422607\n",
            "2.8126473426818848\n",
            "6.42653751373291\n",
            "2.9019877910614014\n",
            "2.8578243255615234\n",
            "5.845969200134277\n",
            "7.397816181182861\n",
            "5.976737976074219\n",
            "2.5231800079345703\n",
            "2.6288108825683594\n",
            "9.296866416931152\n",
            "2.8278236389160156\n",
            "5.741566181182861\n",
            "7.479217052459717\n",
            "7.6404314041137695\n",
            "5.935935974121094\n",
            "7.66894006729126\n",
            "7.665239334106445\n",
            "2.602530002593994\n",
            "12.21663761138916\n",
            "2.7713921070098877\n",
            "13.312751770019531\n",
            "7.726982116699219\n",
            "5.5012006759643555\n",
            "13.641345024108887\n",
            "4.035531997680664\n",
            "12.930950164794922\n",
            "16.60778045654297\n",
            "9.373372077941895\n",
            "9.928216934204102\n",
            "8.213123321533203\n",
            "3.906768798828125\n",
            "2.866389751434326\n",
            "7.602295875549316\n",
            "6.840595245361328\n",
            "9.491256713867188\n",
            "11.226770401000977\n",
            "5.6958441734313965\n",
            "6.4337005615234375\n",
            "2.8240644931793213\n",
            "8.549020767211914\n",
            "6.371088981628418\n",
            "7.30540657043457\n",
            "6.001162528991699\n",
            "7.391237258911133\n",
            "5.497194766998291\n",
            "8.10077953338623\n",
            "5.912193775177002\n",
            "8.547152519226074\n",
            "4.24087381362915\n",
            "5.334218978881836\n",
            "4.153697967529297\n",
            "2.6764183044433594\n",
            "2.7538976669311523\n",
            "5.506890296936035\n",
            "7.498848915100098\n",
            "5.466695308685303\n",
            "2.666227340698242\n",
            "9.926342010498047\n",
            "5.638126373291016\n",
            "4.245477199554443\n",
            "4.110720634460449\n",
            "2.7385001182556152\n",
            "8.325569152832031\n",
            "8.972953796386719\n",
            "2.48457670211792\n",
            "12.255638122558594\n",
            "4.326091289520264\n",
            "6.306753158569336\n",
            "4.170711517333984\n",
            "6.80403995513916\n",
            "2.7062859535217285\n",
            "6.213683128356934\n",
            "9.015158653259277\n",
            "2.3480169773101807\n",
            "12.105910301208496\n",
            "6.631793975830078\n",
            "12.056097984313965\n",
            "5.3273491859436035\n",
            "6.850063323974609\n",
            "5.352761268615723\n",
            "6.091758728027344\n",
            "11.135762214660645\n",
            "3.7674643993377686\n",
            "7.099767684936523\n",
            "2.381197452545166\n",
            "6.55882453918457\n",
            "3.910144090652466\n",
            "10.77265739440918\n",
            "5.261008262634277\n",
            "6.148106575012207\n",
            "5.231410503387451\n",
            "9.811788558959961\n",
            "2.5603160858154297\n",
            "2.2677509784698486\n",
            "3.709516763687134\n",
            "7.495628356933594\n",
            "13.247477531433105\n",
            "8.47446346282959\n",
            "7.4909987449646\n",
            "9.123230934143066\n",
            "6.881961822509766\n",
            "8.483906745910645\n",
            "5.644126892089844\n",
            "8.237176895141602\n",
            "8.896780014038086\n",
            "4.009360313415527\n",
            "8.525018692016602\n",
            "11.488689422607422\n",
            "8.598311424255371\n",
            "5.846370220184326\n",
            "6.049590110778809\n",
            "2.608454942703247\n",
            "6.921277046203613\n",
            "7.392333030700684\n",
            "8.972854614257812\n",
            "10.55636215209961\n",
            "5.163221836090088\n",
            "15.035629272460938\n",
            "2.5301260948181152\n",
            "7.10312557220459\n",
            "6.761237621307373\n",
            "5.414721488952637\n",
            "2.5036473274230957\n",
            "5.640418529510498\n",
            "2.6307151317596436\n",
            "4.109976768493652\n",
            "5.555596351623535\n",
            "8.28168773651123\n",
            "7.477694034576416\n",
            "6.468883514404297\n",
            "6.944178104400635\n",
            "10.54874038696289\n",
            "2.523879051208496\n",
            "2.5511696338653564\n",
            "9.068482398986816\n",
            "6.647315979003906\n",
            "11.367219924926758\n",
            "2.5608301162719727\n",
            "14.160445213317871\n",
            "3.8723273277282715\n",
            "9.007972717285156\n",
            "7.932194709777832\n",
            "7.288615703582764\n",
            "2.68471622467041\n",
            "3.909656524658203\n",
            "5.503878116607666\n",
            "7.536402225494385\n",
            "2.643216371536255\n",
            "4.22141695022583\n",
            "5.673794746398926\n",
            "7.132972240447998\n",
            "2.5750975608825684\n",
            "2.4554834365844727\n",
            "6.400447368621826\n",
            "6.268781661987305\n",
            "6.906603813171387\n",
            "6.488802909851074\n",
            "11.001617431640625\n",
            "6.400941371917725\n",
            "12.163384437561035\n",
            "2.7036373615264893\n",
            "5.380686283111572\n",
            "2.72062087059021\n",
            "10.643539428710938\n",
            "2.6955575942993164\n",
            "6.972970485687256\n",
            "3.9944584369659424\n",
            "2.4603803157806396\n",
            "6.606119155883789\n",
            "2.6959619522094727\n",
            "7.611381530761719\n",
            "6.951210021972656\n",
            "10.236867904663086\n",
            "6.23906946182251\n",
            "6.865321159362793\n",
            "6.278263092041016\n",
            "2.697606086730957\n",
            "10.838956832885742\n",
            "2.736734628677368\n",
            "9.17475700378418\n",
            "2.6858625411987305\n",
            "5.362325191497803\n",
            "14.162757873535156\n",
            "5.582049369812012\n",
            "5.643632888793945\n",
            "5.664028167724609\n",
            "9.70507526397705\n",
            "8.295772552490234\n",
            "2.4041476249694824\n",
            "7.701405048370361\n",
            "5.346418857574463\n",
            "8.169568061828613\n",
            "8.332204818725586\n",
            "8.88361930847168\n",
            "8.99203872680664\n",
            "3.010735511779785\n",
            "12.767786026000977\n",
            "9.245644569396973\n",
            "9.692178726196289\n",
            "9.364922523498535\n",
            "4.458324432373047\n",
            "9.31852912902832\n",
            "6.660316467285156\n",
            "5.477430820465088\n",
            "4.050664901733398\n",
            "6.026254653930664\n",
            "9.198907852172852\n",
            "6.128273963928223\n",
            "6.897587299346924\n",
            "6.480767250061035\n",
            "9.145614624023438\n",
            "7.465188980102539\n",
            "6.435733318328857\n",
            "5.634257793426514\n",
            "5.583715438842773\n",
            "2.766662120819092\n",
            "9.108359336853027\n",
            "5.122405052185059\n",
            "6.610955715179443\n",
            "5.4796528816223145\n",
            "16.02981948852539\n",
            "9.56629753112793\n",
            "11.871743202209473\n",
            "7.266721248626709\n",
            "6.890231132507324\n",
            "2.947903871536255\n",
            "10.618979454040527\n",
            "2.6865732669830322\n",
            "9.913812637329102\n",
            "8.001089096069336\n",
            "2.558661937713623\n",
            "8.261308670043945\n",
            "8.216217041015625\n",
            "3.797311544418335\n",
            "3.058448314666748\n",
            "3.8438351154327393\n",
            "14.303751945495605\n",
            "5.227338790893555\n",
            "7.6190185546875\n",
            "2.7076168060302734\n",
            "10.561992645263672\n",
            "6.346122741699219\n",
            "5.2404279708862305\n",
            "3.054764747619629\n",
            "6.557750225067139\n",
            "6.827162265777588\n",
            "4.200345039367676\n",
            "9.258926391601562\n",
            "3.014188528060913\n",
            "4.070806503295898\n",
            "12.111530303955078\n",
            "8.895248413085938\n",
            "6.309916019439697\n",
            "2.632486343383789\n",
            "9.924732208251953\n",
            "2.4361727237701416\n",
            "11.347674369812012\n",
            "2.890261173248291\n",
            "6.134944915771484\n",
            "2.851571559906006\n",
            "8.402064323425293\n",
            "2.6153359413146973\n",
            "10.563695907592773\n",
            "3.983487844467163\n",
            "9.71473217010498\n",
            "7.152441024780273\n",
            "5.540217399597168\n",
            "2.7685587406158447\n",
            "2.8295958042144775\n",
            "4.099494457244873\n",
            "11.944659233093262\n",
            "10.436397552490234\n",
            "6.530421257019043\n",
            "10.715160369873047\n",
            "6.360411643981934\n",
            "9.842240333557129\n",
            "2.7512168884277344\n",
            "6.834719657897949\n",
            "2.914583206176758\n",
            "2.564645290374756\n",
            "10.52678394317627\n",
            "8.759591102600098\n",
            "8.3291015625\n",
            "5.312757968902588\n",
            "2.3429112434387207\n",
            "10.678627014160156\n",
            "3.7901549339294434\n",
            "9.91206169128418\n",
            "10.528435707092285\n",
            "5.482879638671875\n",
            "2.625183582305908\n",
            "2.942262649536133\n",
            "5.724252223968506\n",
            "11.861896514892578\n",
            "2.742591381072998\n",
            "7.63442850112915\n",
            "3.683232307434082\n",
            "7.052826881408691\n",
            "7.441390037536621\n",
            "6.676630973815918\n",
            "5.344198703765869\n",
            "3.7963199615478516\n",
            "4.071008682250977\n",
            "11.384794235229492\n",
            "6.695209503173828\n",
            "8.992765426635742\n",
            "12.88580322265625\n",
            "16.85029411315918\n",
            "10.25251579284668\n",
            "14.740254402160645\n",
            "2.717212438583374\n",
            "5.310635566711426\n",
            "7.02669620513916\n",
            "8.12075424194336\n",
            "3.8321852684020996\n",
            "10.060089111328125\n",
            "2.4653420448303223\n",
            "10.771316528320312\n",
            "6.604355812072754\n",
            "2.2579331398010254\n",
            "3.7752132415771484\n",
            "3.9540507793426514\n",
            "10.737584114074707\n",
            "9.942537307739258\n",
            "5.92417049407959\n",
            "2.2589361667633057\n",
            "2.495213747024536\n",
            "2.5006752014160156\n",
            "6.7072954177856445\n",
            "7.685305595397949\n",
            "8.454343795776367\n",
            "3.7249338626861572\n",
            "5.06834602355957\n",
            "6.539328575134277\n",
            "2.079555034637451\n",
            "7.471276760101318\n",
            "8.800825119018555\n",
            "10.361774444580078\n",
            "4.9058122634887695\n",
            "6.656373023986816\n",
            "5.13947868347168\n",
            "6.018589019775391\n",
            "7.749495506286621\n",
            "4.04664421081543\n",
            "6.492793083190918\n",
            "11.28543472290039\n",
            "2.6666793823242188\n",
            "8.725491523742676\n",
            "7.108188629150391\n",
            "10.290483474731445\n",
            "10.126169204711914\n",
            "4.01308536529541\n",
            "12.028465270996094\n",
            "15.859474182128906\n",
            "6.810622215270996\n",
            "10.101301193237305\n",
            "2.1137635707855225\n",
            "6.0402302742004395\n",
            "2.392782211303711\n",
            "5.292367935180664\n",
            "16.266902923583984\n",
            "5.219470977783203\n",
            "13.5820951461792\n",
            "5.453826904296875\n",
            "4.972069263458252\n",
            "6.035434722900391\n",
            "7.8811726570129395\n",
            "5.511348724365234\n",
            "9.979052543640137\n",
            "6.1972455978393555\n",
            "9.987524032592773\n",
            "4.439487457275391\n",
            "2.6980347633361816\n",
            "12.243452072143555\n",
            "19.851839065551758\n",
            "5.94877815246582\n",
            "4.651932239532471\n",
            "2.6057138442993164\n",
            "2.5654687881469727\n",
            "3.362823963165283\n",
            "2.3196141719818115\n",
            "12.182367324829102\n",
            "4.943765640258789\n",
            "14.842373847961426\n",
            "3.6371872425079346\n",
            "2.5423450469970703\n",
            "7.624286651611328\n",
            "2.3072774410247803\n",
            "18.226762771606445\n",
            "7.147915363311768\n",
            "3.1698741912841797\n",
            "6.395834922790527\n",
            "2.2865803241729736\n",
            "2.0978028774261475\n",
            "7.348635673522949\n",
            "16.780874252319336\n",
            "5.718174457550049\n",
            "8.170804977416992\n",
            "6.054840564727783\n",
            "11.698286056518555\n",
            "15.131860733032227\n",
            "17.329126358032227\n",
            "5.087104320526123\n",
            "3.9667410850524902\n",
            "2.399296760559082\n",
            "2.099501609802246\n",
            "6.128566741943359\n",
            "3.5236337184906006\n",
            "6.561047077178955\n",
            "9.211017608642578\n",
            "9.75685977935791\n",
            "2.5246987342834473\n",
            "6.035091400146484\n",
            "2.330731153488159\n",
            "5.900285243988037\n",
            "5.095315933227539\n",
            "1.9945571422576904\n",
            "13.7998685836792\n",
            "2.172159194946289\n",
            "2.2542483806610107\n",
            "19.09842300415039\n",
            "14.2587308883667\n",
            "5.530423164367676\n",
            "5.2002081871032715\n",
            "6.161760330200195\n",
            "3.444761276245117\n",
            "10.841463088989258\n",
            "2.322230815887451\n",
            "16.073333740234375\n",
            "2.4195353984832764\n",
            "9.83394718170166\n",
            "2.5700366497039795\n",
            "14.475948333740234\n",
            "6.277637004852295\n",
            "9.799907684326172\n",
            "13.524662017822266\n",
            "3.4047651290893555\n",
            "8.20275592803955\n",
            "5.117752552032471\n",
            "8.233909606933594\n",
            "3.263413906097412\n",
            "7.441415309906006\n",
            "14.41645622253418\n",
            "13.694995880126953\n",
            "6.443190574645996\n",
            "14.034613609313965\n",
            "16.199668884277344\n",
            "7.258966445922852\n",
            "5.503256797790527\n",
            "8.647972106933594\n",
            "7.47532844543457\n",
            "11.039196968078613\n",
            "13.204695701599121\n",
            "12.489706039428711\n",
            "4.356227397918701\n",
            "7.483287334442139\n",
            "2.4570565223693848\n",
            "9.336555480957031\n",
            "4.579818248748779\n",
            "3.6594901084899902\n",
            "8.057421684265137\n",
            "5.8019795417785645\n",
            "7.613357067108154\n",
            "15.141326904296875\n",
            "5.228371620178223\n",
            "9.172738075256348\n",
            "5.9147233963012695\n",
            "2.9130096435546875\n",
            "17.080480575561523\n",
            "4.98362398147583\n",
            "4.832277297973633\n",
            "7.448299884796143\n",
            "4.987115859985352\n",
            "7.9765448570251465\n",
            "3.502081871032715\n",
            "4.1291422843933105\n",
            "3.2769393920898438\n",
            "4.877623081207275\n",
            "2.045894145965576\n",
            "2.441666603088379\n",
            "6.606729030609131\n",
            "6.776576042175293\n",
            "5.609550952911377\n",
            "12.064834594726562\n",
            "3.1572861671447754\n",
            "1.7995883226394653\n",
            "14.640687942504883\n",
            "5.82581090927124\n",
            "2.2481296062469482\n",
            "2.1028552055358887\n",
            "7.300997257232666\n",
            "4.705479621887207\n",
            "6.512295722961426\n",
            "2.2698514461517334\n",
            "8.51085376739502\n",
            "14.610764503479004\n",
            "7.1332597732543945\n",
            "2.1360912322998047\n",
            "5.09991455078125\n",
            "2.027560234069824\n",
            "8.123454093933105\n",
            "9.941949844360352\n",
            "7.616525650024414\n",
            "12.452461242675781\n",
            "9.70071792602539\n",
            "14.11393928527832\n",
            "5.85973596572876\n",
            "6.495744705200195\n",
            "8.164158821105957\n",
            "2.216156005859375\n",
            "6.087392807006836\n",
            "5.484382152557373\n",
            "3.2736332416534424\n",
            "8.363822937011719\n",
            "9.916982650756836\n",
            "2.698820114135742\n",
            "6.965640068054199\n",
            "6.457886219024658\n",
            "2.910616159439087\n",
            "9.052451133728027\n",
            "13.606281280517578\n",
            "6.968577861785889\n",
            "3.2994983196258545\n",
            "3.331218719482422\n",
            "2.373476028442383\n",
            "8.27621841430664\n",
            "10.41335678100586\n",
            "6.46665620803833\n",
            "2.7177257537841797\n",
            "6.61952018737793\n",
            "12.469756126403809\n",
            "3.9144818782806396\n",
            "10.088387489318848\n",
            "10.818558692932129\n",
            "10.771121978759766\n",
            "12.689802169799805\n",
            "5.762874126434326\n",
            "2.8537371158599854\n",
            "6.014351844787598\n",
            "2.8811213970184326\n",
            "11.944964408874512\n",
            "5.733555793762207\n",
            "2.927644729614258\n",
            "6.141195297241211\n",
            "8.451780319213867\n",
            "2.304107666015625\n",
            "5.385335922241211\n",
            "7.311724662780762\n",
            "2.6171791553497314\n",
            "7.180743217468262\n",
            "3.317573070526123\n",
            "4.934236526489258\n",
            "3.32706356048584\n",
            "2.016148567199707\n",
            "5.353092193603516\n",
            "2.35489559173584\n",
            "11.844347953796387\n",
            "12.542957305908203\n",
            "2.0578818321228027\n",
            "3.1889705657958984\n",
            "5.974064826965332\n",
            "2.848646879196167\n",
            "8.859668731689453\n",
            "9.391378402709961\n",
            "7.705723762512207\n",
            "10.344197273254395\n",
            "5.257617950439453\n",
            "12.946202278137207\n",
            "6.204809188842773\n",
            "9.908496856689453\n",
            "9.037686347961426\n",
            "3.686817169189453\n",
            "10.2384033203125\n",
            "3.3469791412353516\n",
            "9.513825416564941\n",
            "7.983083724975586\n",
            "11.387886047363281\n",
            "6.398036956787109\n",
            "2.2712836265563965\n",
            "3.2678444385528564\n",
            "2.6092543601989746\n",
            "1.8947923183441162\n",
            "2.3792948722839355\n",
            "6.244385242462158\n",
            "8.108064651489258\n",
            "2.1472787857055664\n",
            "2.446516990661621\n",
            "1.9925906658172607\n",
            "6.27916145324707\n",
            "6.938372611999512\n",
            "5.063833236694336\n",
            "7.8443193435668945\n",
            "2.3412094116210938\n",
            "9.241324424743652\n",
            "3.3368988037109375\n",
            "2.8562979698181152\n",
            "2.1896891593933105\n",
            "7.613324165344238\n",
            "9.57156753540039\n",
            "1.7858808040618896\n",
            "5.916225433349609\n",
            "7.4202070236206055\n",
            "20.0804443359375\n",
            "12.353120803833008\n",
            "4.569000720977783\n",
            "6.571758270263672\n",
            "4.681135654449463\n",
            "3.978346347808838\n",
            "10.287513732910156\n",
            "6.561347961425781\n",
            "4.872959136962891\n",
            "5.280767440795898\n",
            "6.961654186248779\n",
            "5.012059688568115\n",
            "5.195982933044434\n",
            "3.0028128623962402\n",
            "5.367182731628418\n",
            "6.9943366050720215\n",
            "7.841371536254883\n",
            "7.494289398193359\n",
            "4.805004596710205\n",
            "5.252015113830566\n",
            "2.593306541442871\n",
            "4.53683614730835\n",
            "6.369358062744141\n",
            "8.64071273803711\n",
            "16.742450714111328\n",
            "11.26559829711914\n",
            "7.310721397399902\n",
            "7.087821960449219\n",
            "5.896243095397949\n",
            "9.413158416748047\n",
            "2.758568286895752\n",
            "6.432284832000732\n",
            "8.787723541259766\n",
            "13.509982109069824\n",
            "5.863178730010986\n",
            "3.6483154296875\n",
            "2.5402159690856934\n",
            "10.245362281799316\n",
            "13.52224349975586\n",
            "7.048194885253906\n",
            "3.587357759475708\n",
            "12.243507385253906\n",
            "7.577788829803467\n",
            "4.144980430603027\n",
            "4.572193145751953\n",
            "7.42678165435791\n",
            "3.5975966453552246\n",
            "2.9403433799743652\n",
            "3.1813459396362305\n",
            "2.7767858505249023\n",
            "7.804527759552002\n",
            "4.823750019073486\n",
            "5.01157283782959\n",
            "11.89790153503418\n",
            "7.174827575683594\n",
            "7.878693103790283\n",
            "13.332239151000977\n",
            "15.229042053222656\n",
            "16.584930419921875\n",
            "6.400249004364014\n",
            "2.6725971698760986\n",
            "6.586653232574463\n",
            "6.273862838745117\n",
            "2.7848148345947266\n",
            "4.23148775100708\n",
            "8.221607208251953\n",
            "8.22174072265625\n",
            "4.543088436126709\n",
            "4.579156398773193\n",
            "9.230550765991211\n",
            "3.365886688232422\n",
            "6.1942524909973145\n",
            "15.569382667541504\n",
            "6.710776329040527\n",
            "10.21975326538086\n",
            "3.488903284072876\n",
            "9.572246551513672\n",
            "3.984297275543213\n",
            "5.770520210266113\n",
            "4.555530548095703\n",
            "14.558977127075195\n",
            "9.050134658813477\n",
            "5.529603481292725\n",
            "6.100226402282715\n",
            "3.4206526279449463\n",
            "3.565347909927368\n",
            "4.221428394317627\n",
            "3.6675596237182617\n",
            "9.89663028717041\n",
            "4.977288246154785\n",
            "3.9995980262756348\n",
            "8.646265029907227\n",
            "7.077247142791748\n",
            "6.857323169708252\n",
            "13.369392395019531\n",
            "2.559880495071411\n",
            "2.3899664878845215\n",
            "14.492849349975586\n",
            "2.2284669876098633\n",
            "2.7071926593780518\n",
            "4.882147312164307\n",
            "4.86407470703125\n",
            "3.4716827869415283\n",
            "10.83561897277832\n",
            "6.23408317565918\n",
            "4.788506507873535\n",
            "2.3390541076660156\n",
            "6.089041709899902\n",
            "2.0587308406829834\n",
            "3.0765678882598877\n",
            "2.3725345134735107\n",
            "3.102324962615967\n",
            "4.89021110534668\n",
            "4.773637771606445\n",
            "4.831838130950928\n",
            "3.5810725688934326\n",
            "2.352569818496704\n",
            "5.785558700561523\n",
            "4.075080871582031\n",
            "14.573014259338379\n",
            "2.357607364654541\n",
            "5.377058982849121\n",
            "4.567510604858398\n",
            "7.587604522705078\n",
            "5.382085800170898\n",
            "1.9308050870895386\n",
            "10.019511222839355\n",
            "5.398032188415527\n",
            "9.412891387939453\n",
            "9.124804496765137\n",
            "5.1621832847595215\n",
            "3.0659682750701904\n",
            "7.3907623291015625\n",
            "8.418439865112305\n",
            "3.4212541580200195\n",
            "6.851984024047852\n",
            "12.664899826049805\n",
            "9.390999794006348\n",
            "9.270624160766602\n",
            "2.396087169647217\n",
            "2.106952667236328\n",
            "4.646873474121094\n",
            "3.0412850379943848\n",
            "8.263717651367188\n",
            "4.19846773147583\n",
            "5.894654273986816\n",
            "5.3563923835754395\n",
            "4.793041229248047\n",
            "7.810896396636963\n",
            "7.325562000274658\n",
            "6.932887077331543\n",
            "8.200397491455078\n",
            "4.155353546142578\n",
            "2.0663859844207764\n",
            "7.277918815612793\n",
            "3.1892638206481934\n",
            "3.7307627201080322\n",
            "6.493751049041748\n",
            "2.3683505058288574\n",
            "9.560673713684082\n",
            "2.7197766304016113\n",
            "3.039557456970215\n",
            "2.1616721153259277\n",
            "5.845159530639648\n",
            "2.0433826446533203\n",
            "8.087018966674805\n",
            "4.4136481285095215\n",
            "3.5730819702148438\n",
            "4.81410026550293\n",
            "5.021177291870117\n",
            "6.062355041503906\n",
            "3.4822378158569336\n",
            "2.2958226203918457\n",
            "3.013960838317871\n",
            "4.757152080535889\n",
            "2.0851643085479736\n",
            "7.984147071838379\n",
            "2.8264031410217285\n",
            "4.366835594177246\n",
            "5.018862247467041\n",
            "3.125873327255249\n",
            "4.393463611602783\n",
            "5.200851917266846\n",
            "1.8248991966247559\n",
            "5.843184947967529\n",
            "2.5238425731658936\n",
            "3.560811996459961\n",
            "2.0460548400878906\n",
            "4.471578121185303\n",
            "4.140939712524414\n",
            "2.706545829772949\n",
            "6.538264274597168\n",
            "5.82951545715332\n",
            "4.648869514465332\n",
            "7.040336608886719\n",
            "6.0748090744018555\n",
            "4.745697498321533\n",
            "11.061819076538086\n",
            "3.6626458168029785\n",
            "4.56564998626709\n",
            "3.4117207527160645\n",
            "2.5566153526306152\n",
            "6.51554012298584\n",
            "4.886232376098633\n",
            "8.12224006652832\n",
            "2.543455123901367\n",
            "4.1352152824401855\n",
            "3.6974363327026367\n",
            "2.4819226264953613\n",
            "4.794229984283447\n",
            "4.205467224121094\n",
            "6.935615539550781\n",
            "6.985159873962402\n",
            "8.638021469116211\n",
            "2.032496213912964\n",
            "2.3777928352355957\n",
            "7.341999053955078\n",
            "7.5565385818481445\n",
            "5.026217937469482\n",
            "4.585986614227295\n",
            "6.370992660522461\n",
            "3.5612106323242188\n",
            "4.955206871032715\n",
            "2.2817225456237793\n",
            "4.933188438415527\n",
            "3.1270370483398438\n",
            "2.8093438148498535\n",
            "4.465588569641113\n",
            "3.042407989501953\n",
            "6.231393337249756\n",
            "2.9642372131347656\n",
            "1.8600523471832275\n",
            "2.037156820297241\n",
            "6.272208213806152\n",
            "8.167244911193848\n",
            "4.433752536773682\n",
            "2.2027547359466553\n",
            "4.618075370788574\n",
            "2.3825409412384033\n",
            "2.675161600112915\n",
            "4.492198944091797\n",
            "2.682281970977783\n",
            "4.518873691558838\n",
            "1.6880563497543335\n",
            "5.434478759765625\n",
            "2.334317684173584\n",
            "3.420809745788574\n",
            "2.758883476257324\n",
            "6.784249782562256\n",
            "2.79138445854187\n",
            "3.295304775238037\n",
            "1.921250820159912\n",
            "3.986833333969116\n",
            "2.1043410301208496\n",
            "1.93157958984375\n",
            "6.89106559753418\n",
            "2.378232955932617\n",
            "5.151639938354492\n",
            "1.683635950088501\n",
            "3.179003953933716\n",
            "2.5293941497802734\n",
            "6.987226486206055\n",
            "2.3347153663635254\n",
            "3.7094719409942627\n",
            "4.268136978149414\n",
            "2.977229356765747\n",
            "4.827767372131348\n",
            "5.29702091217041\n",
            "8.054895401000977\n",
            "2.902527332305908\n",
            "6.230738639831543\n",
            "1.8846337795257568\n",
            "1.8567386865615845\n",
            "2.024181604385376\n",
            "6.996902942657471\n",
            "5.517301082611084\n",
            "5.416293144226074\n",
            "4.731693744659424\n",
            "2.4802680015563965\n",
            "3.037748098373413\n",
            "3.6062088012695312\n",
            "4.7894158363342285\n",
            "3.284078598022461\n",
            "8.721599578857422\n",
            "4.15968132019043\n",
            "1.6174445152282715\n",
            "1.8187792301177979\n",
            "1.989701271057129\n",
            "3.0500712394714355\n",
            "3.4847450256347656\n",
            "2.2892279624938965\n",
            "2.1866414546966553\n",
            "3.4832611083984375\n",
            "1.5935317277908325\n",
            "4.009824275970459\n",
            "7.952322959899902\n",
            "1.4681082963943481\n",
            "3.063308000564575\n",
            "3.6394615173339844\n",
            "1.4925856590270996\n",
            "2.4334216117858887\n",
            "2.1896629333496094\n",
            "5.680205345153809\n",
            "6.425300598144531\n",
            "3.544705629348755\n",
            "1.8223692178726196\n",
            "1.7783437967300415\n",
            "1.6225652694702148\n",
            "4.296412467956543\n",
            "2.039645195007324\n",
            "4.739259719848633\n",
            "7.304579734802246\n",
            "3.7500391006469727\n",
            "2.0002036094665527\n",
            "4.002878189086914\n",
            "3.1733384132385254\n",
            "5.261045455932617\n",
            "1.9942240715026855\n",
            "3.5630300045013428\n",
            "3.838216781616211\n",
            "2.3922624588012695\n",
            "5.302643299102783\n",
            "4.129540920257568\n",
            "3.314584970474243\n",
            "3.2726736068725586\n",
            "3.8654704093933105\n",
            "4.990826606750488\n",
            "5.127575397491455\n",
            "2.32037091255188\n",
            "8.3707857131958\n",
            "4.563587188720703\n",
            "2.258582592010498\n",
            "7.1677350997924805\n",
            "2.737318992614746\n",
            "6.947174072265625\n",
            "6.6802215576171875\n",
            "2.046778678894043\n",
            "8.984848976135254\n",
            "8.318269729614258\n",
            "1.9150217771530151\n",
            "3.762800455093384\n",
            "1.85992431640625\n",
            "5.191781044006348\n",
            "5.413238525390625\n",
            "4.588722229003906\n",
            "7.460282325744629\n",
            "2.605485200881958\n",
            "2.376638174057007\n",
            "6.148268699645996\n",
            "9.001422882080078\n",
            "8.003371238708496\n",
            "4.289231777191162\n",
            "2.319401979446411\n",
            "8.495695114135742\n",
            "6.673807621002197\n",
            "2.385857343673706\n",
            "2.711193561553955\n",
            "2.815122365951538\n",
            "3.027949810028076\n",
            "4.15768575668335\n",
            "2.563011646270752\n",
            "7.990707874298096\n",
            "3.561446189880371\n",
            "4.142990589141846\n",
            "3.2583372592926025\n",
            "6.5383620262146\n",
            "3.467804193496704\n",
            "4.228686809539795\n",
            "1.9398653507232666\n",
            "1.5674726963043213\n",
            "7.7087602615356445\n",
            "7.154241561889648\n",
            "9.086612701416016\n",
            "2.0534772872924805\n",
            "2.0189270973205566\n",
            "6.107507705688477\n",
            "4.228272438049316\n",
            "3.2081990242004395\n",
            "4.4872236251831055\n",
            "8.832419395446777\n",
            "7.543931484222412\n",
            "4.598120212554932\n",
            "7.539055347442627\n",
            "4.269949913024902\n",
            "2.914778709411621\n",
            "1.9408624172210693\n",
            "2.33797025680542\n",
            "3.1669561862945557\n",
            "2.6309823989868164\n",
            "7.363077640533447\n",
            "5.134180068969727\n",
            "2.8119592666625977\n",
            "7.000959396362305\n",
            "5.136906623840332\n",
            "1.7089152336120605\n",
            "3.274168014526367\n",
            "3.388092041015625\n",
            "5.132270336151123\n",
            "1.9400267601013184\n",
            "3.234567403793335\n",
            "4.2368059158325195\n",
            "5.164805889129639\n",
            "12.02484130859375\n",
            "1.9581429958343506\n",
            "4.016505718231201\n",
            "1.893168568611145\n",
            "7.453752517700195\n",
            "3.276052236557007\n",
            "1.8845298290252686\n",
            "1.8851206302642822\n",
            "2.3632469177246094\n",
            "3.1095409393310547\n",
            "1.9771095514297485\n",
            "1.7706940174102783\n",
            "1.782674789428711\n",
            "6.129526615142822\n",
            "1.8030388355255127\n",
            "4.284907341003418\n",
            "6.568697929382324\n",
            "9.778460502624512\n",
            "1.8438358306884766\n",
            "6.187253475189209\n",
            "3.2929837703704834\n",
            "3.600618839263916\n",
            "1.9550236463546753\n",
            "9.68117904663086\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8d0b7a29403f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_train_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c7d4a3592683>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, training_replay_buffer, step)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mobs_anchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpc_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"obs_anchor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpc_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"obs_pos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_cpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_anchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Nawid -  Performs the contrastive loss I believe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_dynamics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_one_hot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted_change\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c7d4a3592683>\u001b[0m in \u001b[0;36mupdate_dynamics\u001b[0;34m(self, obs, actions, labels)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mprediction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSE_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#  Need to use .item otherwise the loss will still be kept which will reduce the memory on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X7_F-BU6ZFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}